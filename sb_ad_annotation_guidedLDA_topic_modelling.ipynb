{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import numpy as np\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Ad Name</th>\n",
       "      <th>Product</th>\n",
       "      <th>Key Terms  Round 1</th>\n",
       "      <th>Excitatory Potential</th>\n",
       "      <th>Emotional vs. Rational</th>\n",
       "      <th>Semantic Affinity</th>\n",
       "      <th>Valence</th>\n",
       "      <th>Keywords</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ad Number</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trailer</td>\n",
       "      <td>Fast &amp; Furious 9  Trailer</td>\n",
       "      <td>Movie Trailer</td>\n",
       "      <td>fast and the furious, fast &amp; the furious, fast...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Trailer Fast &amp; Furious 9  Trailer fast and the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quibi</td>\n",
       "      <td>Quibi  Bank Heist</td>\n",
       "      <td>Video Platform</td>\n",
       "      <td>quibi, bank heist, robbery, less than ten minu...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Quibi Quibi  Bank Heist quibi, bank heist, rob...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Brand Name                    Ad Name         Product  \\\n",
       "Ad Number                                                         \n",
       "1            Trailer  Fast & Furious 9  Trailer   Movie Trailer   \n",
       "2              Quibi          Quibi  Bank Heist  Video Platform   \n",
       "\n",
       "                                          Key Terms  Round 1  \\\n",
       "Ad Number                                                      \n",
       "1          fast and the furious, fast & the furious, fast...   \n",
       "2          quibi, bank heist, robbery, less than ten minu...   \n",
       "\n",
       "           Excitatory Potential  Emotional vs. Rational  Semantic Affinity  \\\n",
       "Ad Number                                                                    \n",
       "1                             1                       1                  2   \n",
       "2                             2                       1                  2   \n",
       "\n",
       "           Valence                                           Keywords  \n",
       "Ad Number                                                              \n",
       "1                1  Trailer Fast & Furious 9  Trailer fast and the...  \n",
       "2                2  Quibi Quibi  Bank Heist quibi, bank heist, rob...  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_data = pd.read_csv('/Users/vcroopana/Downloads/summer2020/superbowl/ip/SB_ad_annotations.csv', index_col=0) \n",
    "annotations_data['Keywords'] = annotations_data['Brand Name']\\\n",
    "                                .str.cat(annotations_data['Ad Name'], sep=\" \")\\\n",
    "                                .str.cat(annotations_data['Key Terms  Round 1'], sep=\" \")\n",
    "df = annotations_data.drop_duplicates()\n",
    "print(df.shape)\n",
    "annotations_data.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "print(len(stop))\n",
    "#stop.extend([\"best\", \"award\", \"love\", \"movie\", \"picture\", \"words\", \"actor\", \"actress\", \"red\", \"carpet\", \"oscar\", \"oscars\"])\n",
    "print(len(stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cadillacabc joinrt k\n"
     ]
    }
   ],
   "source": [
    "#clean tweets\n",
    "import re\n",
    "def removeMentions(text):\n",
    "\n",
    "    textBeforeMention = text.partition(\"@\")[0]\n",
    "    textAfterMention = text.partition(\"@\")[2]\n",
    "    textAfterMention =  re.sub(r':', '', textAfterMention) #cadillac join the 31k\n",
    "    tHandle = textAfterMention.partition(\" \")[0].lower() #cadillac    \n",
    "    text = textBeforeMention+ \" \" + textAfterMention  \n",
    "    return text\n",
    "    \n",
    "#TODO issue should've getting converted to shouldve couldn' t -> couldn\n",
    "def cleanTweet(strinp):\n",
    "    strinp = re.sub(r'RT', \"\", strinp) # Remove RT\n",
    "    strinp = strinp.lower()\n",
    "    \n",
    "    stop_removed_list = [word for word in strinp.split() if word not in (stop)]\n",
    "    stop_removed = ' '.join([str(elem) for elem in stop_removed_list])    \n",
    "    text = re.sub('https?://[A-Za-z0-9./]+', ' ', stop_removed) # Remove URLs\n",
    "    text = removeMentions(text)\n",
    "    text = re.sub('[^\\x00-\\x7F]+', ' ', text) # Remove non-ASCII chars.\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text) # remove all other than alphabet chars \n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text) # remove all single characters     \n",
    "    stop_removed_l = [word for word in text.split() if word not in (stop)]\n",
    "    stop_removed = ' '.join([str(elem) for elem in stop_removed_l]) \n",
    "    return stop_removed\n",
    "\n",
    "print(cleanTweet(\"RT @cadillacabc: Joinrt the 31K\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## GuidedLDA #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/\n",
    "<br /> topic modelling Vs classificaton: https://monkeylearn.com/topic-analysis/#papers\n",
    "<br />shorttext - https://shorttext.readthedocs.io/en/latest/tutorial_topic.html\n",
    "<br />naive Bayes - https://monkeylearn.com/topic-analysis/\n",
    "<br />guidedlda - https://guidedlda.readthedocs.io/en/latest/\n",
    "<br />guided LDA installation: https://github.com/vi3k6i5/GuidedLDA/issues/26\n",
    "<br /> guide LDA explanation: https://www.freecodecamp.org/news/how-we-changed-unsupervised-lda-to-semi-supervised-guidedlda-e36a95f3a164/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal LDA without seeding\n",
    "# model = guidedlda.GuidedLDA(n_topics=14, n_iter=100, random_state=7, refresh=20)\n",
    "# model.fit(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_data['keywords_clean'] = annotations_data['Key Terms  Round 1'].apply(lambda x: cleanTweet(x))\n",
    "annotations_data['keywords_clean'].apply(lambda x: print(str(x.split())+\",\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topic_list = [\n",
    "    ['fast', 'furious', 'fast', 'furious', 'fast', 'furious', 'ff', 'fast', 'saga', 'vin', 'diesel', 'flying', 'truck', 'stunts', 'michelle', 'rodriguez', 'fast', 'furious'],\n",
    "['quibi', 'bank', 'heist', 'robbery', 'less', 'ten', 'minutes', 'quick', 'bites', 'big', 'stories', 'chance', 'rapper', 'megan', 'thee', 'stallion', 'chancetherapper', 'meganstallion', 'quickbites'],\n",
    "['tide', 'laundry', 'laundry', 'detergent', 'schitt', 'emily', 'hampshire', 'charlie', 'day', 'walts'],\n",
    "['fox', 'show', 'daytona', 'run', 'history', 'great', 'american', 'race'],\n",
    "['donald', 'trump', 'trump', 'change', 'unemployment', 'stronger', 'safer', 'prosperous', 'trumpforpresident', 'donaldtrump', 'presidentrump', 'obama', 'gop', 'trumpsupporters'],\n",
    "['walmart', 'pickup', 'spaceship', 'toy', 'story', 'buzz', 'lightyear', 'marvin', 'martian', 'marvin', 'martians', 'arrival', 'glass', 'cleaners', 'aliens', 'men', 'black', 'groot', 'lego', 'star', 'wars', 'bill', 'bill', 'ted', 'flash', 'gordon'],\n",
    "['marvel', 'black', 'widow', 'scarlett', 'johansson'],\n",
    "['quicken', 'loans', 'aquaman', 'jason', 'momoa', 'rocket', 'mortgage', 'momoabowl', 'jasonmomoa', 'rocketmorgage'],\n",
    "['porsche', 'volkswagen', 'chase', 'fast', 'furious', 'taycan', 'turbo', 'car', 'chase', 'taycanturbo'],\n",
    "['snickers', 'snickersfixtheworld', 'fix', 'world', 'snickershole', 'fixtheworld'],\n",
    "['hulu', 'tom', 'brady', 'tombrady'],\n",
    "['fox', 'chosen', 'lego', 'masters', 'competition', 'arnett'],\n",
    "['mountain', 'dew', 'mtn', 'dew', 'soda', 'zero', 'sugar', 'shining', 'work', 'play', 'makes', 'jack', 'dull', 'boy', 'new', 'mtn', 'dew', 'zero', 'sugar', 'bryan', 'cranston', 'mtndew', 'theshining'],\n",
    "['squarespace', 'winona', 'winona', 'ryder', 'minnesota'],\n",
    "['love', 'takes', 'action', 'greek', 'words', 'love', 'agape', 'new', 'york', 'life', 'take', 'action', 'lovetakesaction', 'good', 'life', 'nyl', 'newyorklife'],\n",
    "['fox', 'super', 'monday', 'office', 'toby', 'toby', 'flenderson'],\n",
    "['chris', 'evans', 'captain', 'america', 'john', 'krasinki', 'rachel', 'dratch', 'boston', 'red', 'sox', 'david', 'ortiz', 'boston', 'accent', 'smart', 'parking', 'hyundai', 'better', 'way', 'park', 'bettahdriveus', 'chrisevans', 'smahtpark', 'bettahwaytopark'],\n",
    "['mc', 'hammer', 'cant', 'touch', 'popcorn', 'cheetos', 'cheetos', 'thing', 'cheetle', 'canttouchthis', 'mchammer'],\n",
    "['female', 'spacewalk', 'lilly', 'singh', 'busy', 'phillips', 'nicole', 'stott', 'taraji', 'henson', 'katie', 'couric', 'olay', 'make', 'space', 'women', 'makespaceforwomen', 'olay'],\n",
    "['fox', 'halftime', 'show', 'lo', 'jennifer', 'lopez', 'shakira'],\n",
    "['help', 'farmers', 'organic', 'pack', 'square', 'foot', 'make', 'impact', 'michelob', 'gold', 'michelobgold', 'pack', 'sixforsixpack'],\n",
    "['molly', 'ringwald', 'avocados', 'mexico', 'avonetwork', 'avocarrier', 'mollyringwald'],\n",
    "['hard', 'rock', 'international', 'hard', 'rock', 'stadium', 'miami', 'kobe', 'bryant', 'jennifer', 'lopez', 'jlo', 'hardrock', 'hardrockstadium', 'hardrockcasino'],\n",
    "['rick', 'morty', 'adult', 'swim', 'pringles', 'pickle', 'rick', 'rick', 'morty', 'new', 'flavor', 'summer', 'rick', 'morty', 'adultswim'],\n",
    "['turbotax', 'people', 'app', 'mobile', 'feature', 'taxes', 'tax', 'people', 'people', 'tax', 'people', 'allpeoplearetaxpeople'],\n",
    "['tide', 'laundry', 'laundry', 'detergent', 'charliey', 'day', 'bud', 'knight', 'stain', 'laundrydetergent', 'charlieday'],\n",
    "['john', 'legend', 'chrissy', 'teigen', 'genesis', 'hyundai', 'kobe', 'bryant', 'young', 'luxury', 'gv', 'sexiest', 'man', 'alive', 'johnlegend', 'chrissyteigen'],\n",
    "['coke', 'iphone', 'coca', 'cola', 'coca', 'cola', 'energy', 'showup', 'dots', 'cococola', 'cocacolaenergy', 'jonah', 'hill', 'martin', 'scorsese'],\n",
    "['planters', 'nuts', 'mrpeanut', 'matt', 'walsh', 'wesley', 'snipes', 'babynut', 'babypeanut', 'babyfuneral', 'baby', 'nut'],\n",
    "['james', 'bond', 'time', 'die', 'daniel', 'craig'],\n",
    "['fox', 'masked', 'singer', 'toads'],\n",
    "['google', 'assistant', 'virtual', 'assistant', 'googleassistant', 'virtualassistant'],\n",
    "['sabra', 'humus', 'hummus', 'wives', 'real', 'housewives', 'new', 'jersey', 'real', 'housewives', 'caroline', 'manzo', 'teresa', 'giudice', 'pain', 'mix', 'cracker', 'kim', 'chi', 'drag', 'queens', 'dragqueens', 'dragqueen', 'amanda', 'cerny', 'howimmus'],\n",
    "['weathertech', 'pets', 'golden', 'retriever', 'dog'],\n",
    "['verizon', 'phone', 'service', 'phone'],\n",
    "['general', 'motors', 'hummer', 'hummer', 'ev', 'electric', 'pickup', 'zero', 'emission', 'lebron', 'james', 'basketball', 'lebronjames'],\n",
    "['pop', 'tarts', 'queer', 'eye', 'jonathan', 'van', 'ness', 'pretzel', 'queereye', 'poptarts'],\n",
    "['minions', 'rise', 'gru'],\n",
    "['fox', 'halftime', 'show', 'lo', 'jennifer', 'lopez', 'shakira'],\n",
    "['fox', 'fox', 'nation', 'breaking', 'news'],\n",
    "['premier', 'boxing', 'champions', 'wilder', 'vs', 'fury', 'ii'],\n",
    "['homeland', 'trailer'],\n",
    "['fox', 'daytona', 'great', 'american', 'race', 'run', 'history'],\n",
    "['hunters', 'nazi', 'al', 'pacino', 'nazi', 'hunters', 'prime', 'video', 'amazon', 'barbeque', 'barbecue', 'alpacino', 'primevideo', 'hunterstv'],\n",
    "['missy', 'elliott', 'e', 'pepsi', 'zero', 'sugar', 'paint', 'black', 'pepsi', 'pepsizero', 'nocoke'],\n",
    "['find', 'goodness', 'heinz', 'ketchup', 'stranger', 'things', 'strangerthings', 'findthegoodness'],\n",
    "['premier', 'boxing', 'champions', 'wilder', 'vs', 'fury', 'ii', 'wrestling'],\n",
    "['bud', 'light', 'bud', 'light', 'seltzer', 'post', 'malone', 'anheuser', 'busch', 'inbev', 'hard', 'seltzer', 'postmalone', 'budlight', 'budweiser'],\n",
    "['office', 'rainn', 'wilson', 'delivery', 'best', 'thing', 'since', 'sliced', 'bread', 'pizza', 'pizza', 'pizza', 'littlecaesars', 'caesars'],\n",
    "['sam', 'elliott', 'lil', 'nas', 'old', 'town', 'road', 'doritos', 'cowboy', 'cool', 'ranch', 'dancer', 'billy', 'ray', 'cyrus', 'wild', 'west', 'wild', 'wild', 'west', 'make', 'move'],\n",
    "['josh', 'jacobs', 'running', 'back', 'kia', 'kia', 'seltos', 'raiders', 'give', 'everything', 'joshjacobs', 'kiaseltos'],\n",
    "['turkish', 'airlines', 'airline', 'flight', 'astronauts', 'rocket'],\n",
    "['reese', 'take', 'five', 'take', 'candybar', 'reeses'],\n",
    "['tide', 'stain', 'later', 'charlie', 'day', 'wonder', 'woman'],\n",
    "['ellen', 'degeneres', 'alexa', 'porta', 'de', 'rossi', 'maid', 'medieval', 'dragon', 'amazon', 'alexa', 'middle', 'ages', 'beforealexa'],\n",
    "['anheuser', 'busch', 'inbev', 'michelob', 'ultra', 'beer', 'jimmy', 'fallon', 'working', 'gym', 'john', 'cena', 'usain', 'bolt', 'brooks', 'koepka', 'kerri', 'walsh', 'jennings', 'worth', 'enjoy', 'low', 'carbs', 'jimmyfallon', 'usainbolt', 'workingout', 'gymbody'],\n",
    "['inspire', 'change', 'police', 'shootings', 'black', 'men', 'nfl', 'inspirechange'],\n",
    "['toyota', 'car', 'suv', 'highlander', 'toyotahighlander', 'cobiesmulders', 'cobie', 'smulders'],\n",
    "['pop', 'culture', 'annual', 'fee', 'discover', 'credit', 'card', 'chandler', 'friends', 'jack', 'black', 'mike', 'myers', 'austin', 'powers', 'noannualfee', 'discovercredit', 'yestodiscover'],\n",
    "['disney', 'plus', 'falcon', 'wanda', 'vision', 'wanda', 'loki'],\n",
    "['pop', 'culture', 'discover', 'credit', 'card', 'mean', 'girls', 'ted', 'noannualfee', 'discovercredit', 'nodiscover'],\n",
    "['football', 'withdrawal', 'system', 'science', 'doctor', 'withdrawal', 'symptoms'],\n",
    "['mobile', 'mama', 'tests', 'g', 'anthony', 'anderson'],\n",
    "['fox', 'tornado', 'lone', 'star'],\n",
    "['fox', 'masked', 'singer', 'lego', 'masters'],\n",
    "['nfl', 'building', 'better', 'game', 'amazon', 'web', 'services'],\n",
    "['kathryn', 'bigelow', 'budweiser', 'american', 'spirit', 'typical', 'american', 'kindness', 'beer', 'stereotypes', 'stereotypical', 'typicalamerican', 'americanspirit'],\n",
    "['procter', 'gamble', 'come', 'together', 'sofia', 'vergara', 'chili', 'bounty', 'old', 'spice', 'head', 'shoulders', 'olay', 'charming', 'febreze'],\n",
    "['fox', 'daytona', 'great', 'american', 'race', 'run', 'history'],\n",
    "['woman', 'coach', 'ers', 'katie', 'sowers', 'surface', 'female', 'coach', 'microsoft', 'first', 'female', 'coach', 'katiesowers', 'erscoach', 'microsoftsurface', 'femalecoach'],\n",
    "['fox', 'lone', 'star', 'storm', 'tornado'],\n",
    "['jeep', 'groundhog', 'day', 'jeep', 'gladiator', 'bill', 'murray'],\n",
    "['chris', 'rock', 'sylvester', 'stallone', 'groups', 'facebook', 'rock', 'music', 'stonehenge', 'rock', 'rocket', 'chrisrock'],\n",
    "['tide', 'tide', 'pod', 'laundry', 'detergent', 'emily', 'hampshire', 'charlie', 'day', 'finally', 'later'],\n",
    "['maisie', 'williams', 'game', 'thrones', 'let', 'go', 'frozen', 'audi', 'etron', 'sportback', 'traffic', 'letitgo', 'maisie', 'williams', 'gameofthrones']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import guidedlda as guidedlda\n",
    "import numpy as np\n",
    "\n",
    "def buildWordVector(train_data, word2id):\n",
    "    X = np.zeros((len(train_data), len(word2id)), dtype = 'int64')\n",
    "\n",
    "    for i in range(0, len(train_data)):\n",
    "        doc_words = train_data[i].split()\n",
    "        for word in doc_words:  \n",
    "            if word in word2id:\n",
    "                X[i][word2id[word]] += 1\n",
    "    return X\n",
    "\n",
    "def buildGuidedLDAModel(train_data, seed_topic_list, n_topics, seed_confidence, \n",
    "                        n_iter, n_top_words, random_state, refresh):\n",
    "    \n",
    "    ad_words = []\n",
    "    for keywrds in train_data:\n",
    "        keywordsSet = list(set(keywrds.split()))\n",
    "        ad_words.extend(keywordsSet)\n",
    "\n",
    "    ad_words = list(set(ad_words))\n",
    "    print(\"Length of ad keywords: \"+ str(len(ad_words)))\n",
    "\n",
    "    word2id = dict((v, idx) for idx, v in enumerate(ad_words)) #dict of word: id\n",
    "\n",
    "    X = buildWordVector(train_data, word2id)     #Build X - input data\n",
    "\n",
    "    seed_topics = {}     #Build Seed topics list\n",
    "    for t_id, st in enumerate(seed_topic_list):\n",
    "        for word in st:\n",
    "            if word in word2id:\n",
    "                 seed_topics[word2id[word]] = t_id\n",
    "\n",
    "    model = guidedlda.GuidedLDA(n_topics, n_iter=n_iter, random_state= random_state, refresh= refresh)\n",
    "    model.fit(X, seed_topics=seed_topics, seed_confidence= seed_confidence)\n",
    "\n",
    "    topic_vector = model.topic_word_ #topic vector [[0.00059559 0..0.00059559]..[0.00263852 ...0.00263852]]\n",
    "    topic_words = []\n",
    "\n",
    "    for i, topic_dist in enumerate(topic_vector):\n",
    "         topic_word = np.array(ad_words)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "         topic_words.append(topic_word.tolist())\n",
    "         print('Topic {}: {}'.format(i, ' '.join(topic_word)))\n",
    "    \n",
    "    return model, topic_words, word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ip required: n_topics, topic_words\n",
    "\n",
    "def buildTopicVectors(doc_topic, n_topics, topic_words):\n",
    "    \n",
    "    columns_label = ['topic {}'.format(i) for i in range(n_topics)]  # number of topics\n",
    "    topic_vector = pd.DataFrame(doc_topic, columns = columns_label)  #dataframe of doc-topics\n",
    "\n",
    "    topic_vector['max_prob'] = topic_vector.max(axis=1)\n",
    "    topic_vector['max_topic'] = topic_vector.idxmax(axis =1)\n",
    "    topic_vector['max_topic_id'] = topic_vector['max_topic'].apply(lambda x: topic_vector.columns.get_loc(x))\n",
    "    topic_vector['topic_words'] = topic_vector['max_topic_id'].apply(lambda x: topic_words[x])\n",
    "    \n",
    "    return topic_vector\n",
    "\n",
    "def get_conf_matrix(ad_manual, max_prob, thresh_prob):\n",
    "    res = \"\"\n",
    "    if(ad_manual=='none' and max_prob >= thresh_prob):\n",
    "        res = 'FP'\n",
    "    elif(ad_manual=='none' and max_prob < thresh_prob):\n",
    "        res = 'TN'\n",
    "    elif(ad_manual!='none' and max_prob >= thresh_prob):\n",
    "        res = 'TP'\n",
    "    elif(ad_manual!='none' and max_prob < thresh_prob):\n",
    "        res = 'FN'\n",
    "    return res\n",
    "\n",
    "def computeAccuracy(result):    \n",
    "    n_tp = result[result['conf_matrix'] == 'TP'].shape[0]\n",
    "    n_fp = result[result['conf_matrix'] == 'FP'].shape[0]\n",
    "    n_fn = result[result['conf_matrix'] == 'FN'].shape[0]\n",
    "    n_tn = result[result['conf_matrix'] == 'TN'].shape[0]\n",
    "    \n",
    "    print(\"n_tp:\" + str(n_tp)+ \" n_fp:\" + str(n_fp)+ \" n_fn:\" + str(n_fn) + \" n_tn:\" + str(n_tn))\n",
    "\n",
    "    precision = n_tp/(n_tp+ n_fp)\n",
    "    recall = n_tp/(n_tp+ n_fn)\n",
    "    f_measure = (2*precision*recall)/ (precision+recall)\n",
    "\n",
    "    return precision, recall, f_measure\n",
    "\n",
    "\n",
    "def removePOS(taggedText, includePOS):\n",
    "    op = ' '.join([pair[0] for pair in taggedText if pair[1] in includePOS])\n",
    "    return op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "Length of ad keywords: 338\n",
      "Topic 0: yestodiscover fee mike gop mountain jacobs life\n",
      "Topic 1: way gladiator tom hulu sox murray john\n",
      "Topic 2: trump gop alpacino trumpforpresident dog jacobs life\n",
      "Topic 3: jonah energy iphone gop mountain jacobs life\n",
      "Topic 4: rocket chris michelob stallone music stonehenge gop\n",
      "Topic 5: swim game emission gop stonehenge life unemployment\n",
      "Topic 6: bud findthegoodness busch budweiser malone anheuser post\n",
      "Topic 7: jimmy jimmyfallon walsh gym usainbolt gymbody john\n",
      "Topic 8: donaldtrump presidentrump unemployment change dog hardrock jacobs\n",
      "Topic 9: beer busch inbev cena michelob fallon gop\n",
      "Topic 10: chase taycanturbo car turbo porsche mountain jacobs\n",
      "Topic 11: day detergent charlie tax pod tide schitt\n",
      "Topic 12: life love gop mountain hardrock jacobs unemployment\n",
      "Topic 13: minnesota gop dog hardrock jacobs life unemployment\n",
      "Topic 14: winona squarespace gru ryder mountain hardrock jacobs\n",
      "Topic 15: lil town move road dancer trailer homeland\n",
      "Topic 16: chancetherapper rapper meganstallion heist chance bank gop\n",
      "Topic 17: wanda disney flight airline loki vision feature\n",
      "Topic 18: snickershole fixtheworld world medieval web hardrock jacobs\n",
      "Topic 19: suv widow car toyotahighlander scarlett johansson gop\n",
      "Topic 20: hunterstv virtualassistant google googleassistant dog jacobs life\n",
      "Topic 21: taraji henson olay stott spacewalk space mountain\n",
      "Topic 22: johnlegend teigen jennifer sexiest gop dog life\n",
      "Topic 23: coach microsoftsurface femalecoach dog jacobs life unemployment\n",
      "Topic 24: withdrawal hummus kim cracker sabra cerny giudice\n",
      "Topic 25: wesley flavor walsh adult jacobs life unemployment\n",
      "Topic 26: football withdrawal doctor system canttouchthis hammer science\n",
      "Topic 27: queer pop eye van queereye jonathan pretzel\n",
      "Topic 28: morty rick adultswim summer dog jacobs life\n",
      "Topic 29: impact cheetle cant mchammer cheetos gop mountain\n",
      "Topic 30: toby fox baby office monday super stonehenge\n",
      "Topic 31: star fox tornado lego glass storm lone\n",
      "Topic 32: tombrady amazon bryant building gop mountain hardrock\n",
      "Topic 33: coca cola coke hill gop dog life\n",
      "Topic 34: bill marvin walmart story groot buzz mountain\n",
      "Topic 35: daniel time bond die arnett craig gop\n",
      "Topic 36: cowboy goodness ranch elliott doritos bettahdriveus hardrock\n",
      "Topic 37: dragon amazon alexa mollyringwald porta boy loki\n",
      "Topic 38: rock hardrock inspirechange stadium hardrockcasino chrisrock hardrockstadium\n",
      "Topic 39: service boston bettahdriveus stonehenge jacobs life unemployment\n",
      "Topic 40: police inspire change truck volkswagen gop jacobs\n",
      "Topic 41: history race daytona retriever gop mountain jacobs\n",
      "Topic 42: mc spice thing head loki jacobs charliey\n",
      "Topic 43: pepsi pepsizero elliott paint sugar dog jacobs\n",
      "Topic 44: ii premier fury wrestling hardrock life unemployment\n",
      "Topic 45: nfl web gop dog jacobs life unemployment\n",
      "Topic 46: primevideo amazon video barbeque pacino barbecue gop\n",
      "Topic 47: woman stallion stain robbery mountain hardrock jacobs\n",
      "Topic 48: pickup dog lebron jacobs life unemployment work\n",
      "Topic 49: gop stonehenge jacobs life unemployment work bettahdriveus\n",
      "Topic 50: york flenderson gop dog jacobs life unemployment\n",
      "Topic 51: bounty head chili procter febreze vergara loki\n",
      "Topic 52: spaceship lovetakesaction dog hardrock jacobs life unemployment\n",
      "Topic 53: seltzer office wilson rainn fox postmalone thing\n",
      "Topic 54: fox show halftime shakira run competition gop\n",
      "Topic 55: assistant letitgo genesis hyundai audi gop dog\n",
      "Topic 56: sugar soda jack cranston gop mountain jacobs\n",
      "Topic 57: pizza delivery bread web jacobs life unemployment\n",
      "Topic 58: beer toyota americanspirit spirit highlander kindness unemployment\n",
      "Topic 59: kobe man miami john luxury chrissyteigen gv\n",
      "Topic 60: credit card culture discovercredit discover chandler nodiscover\n",
      "Topic 61: bettahwaytopark nut gop dog jacobs life unemployment\n",
      "Topic 62: maisie action etron traffic game gordon sportback\n",
      "Topic 63: hyundai krasinki gop stonehenge jacobs life unemployment\n",
      "Topic 64: news obama sylvester nation fox dog life\n",
      "Topic 65: kia josh seltos jacobs everything kiaseltos joshjacobs\n",
      "Topic 66: bill groundhog stonehenge jacobs life unemployment work\n",
      "Topic 67: ketchup howimmus pain heinz humus candybar mix\n",
      "Topic 68: rachel parking cyrus dratch park david sam\n",
      "Topic 69: phone verizon ev web jacobs life unemployment\n",
      "Topic 70: gop stonehenge jacobs life unemployment work bettahdriveus\n",
      "Topic 71: pack charlieday help stain sixforsixpack gold charliey\n",
      "Topic 72: dew mtn mama mtndew work mountain play\n",
      "Topic 73: rocket momoa jason rocketmorgage mortgage momoabowl jasonmomoa\n",
      "Topic 74: gop stonehenge jacobs life unemployment work bettahdriveus\n"
     ]
    }
   ],
   "source": [
    "#training data for the model\n",
    "import logging\n",
    "\n",
    "main = logging.getLogger()\n",
    "main.setLevel(logging.ERROR)\n",
    "\n",
    "train_data = annotations_data['keywords_clean'].apply(lambda x: cleanTweet(x))\n",
    "print(len(train_data))\n",
    "#Filter only adjectives and nouns\n",
    "train_data_pos = train_data.apply(lambda x: removePOS(nltk.pos_tag(x.split()), ('NN') )).to_list()\n",
    "\n",
    "seed_topic_pos = []\n",
    "for seeds in seed_topic_list:\n",
    "    tagged = nltk.pos_tag(seeds)\n",
    "    seed_topic_pos.append(removePOS(tagged, ( 'NN')))\n",
    "\n",
    "model, topic_words, word2id = buildGuidedLDAModel(\n",
    "#                                                   train_data.to_list(), \n",
    "                                                train_data_pos, \n",
    "#                                                   seed_topic_list,\n",
    "                                                seed_topic_pos,\n",
    "#     [],\n",
    "                                                  n_topics= 75, \n",
    "                                                  seed_confidence = 0.7, \n",
    "                        n_iter= 100, n_top_words = 7, random_state=7, refresh=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_ann_data = pd.read_csv(r'/Users/vcroopana/Downloads/summer2020/superbowl/mann_ann_sb.csv')    \n",
    "man_ann_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_ann_data['text_clean'] = man_ann_data['tweet_text'].apply(lambda x: cleanTweet(x))\n",
    "test_data = man_ann_data['text_clean'] \n",
    "X = buildWordVector(test_data, word2id)\n",
    "\n",
    "doc_topic = model.transform(X)\n",
    "# print(doc_topic)\n",
    "topic_vector = buildTopicVectors(doc_topic, 75, topic_words)\n",
    "# topic_vector.round(2).head(2)\n",
    "\n",
    "man_ann_data = man_ann_data.reset_index(drop=True)\n",
    "\n",
    "result = pd.concat([man_ann_data, topic_vector], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tp:629 n_fp:962 n_fn:224 n_tn:685\n",
      "precision: 0.395 recall: 0.737 f_measure: 0.515\n"
     ]
    }
   ],
   "source": [
    "result['conf_matrix'] = result.apply(lambda x: get_conf_matrix(x['ad_manual'], x['max_prob'], \n",
    "                                                               thresh_prob = 0.54), axis =1)\n",
    "precision, recall, f_measure = computeAccuracy(result)\n",
    "print(\"precision: \"+ str(np.round(precision, 3))+\" recall: \"+ str(np.round(recall,3))+\" f_measure: \"\n",
    "      + str(np.round(f_measure,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_name = '/Users/vcroopana/Downloads/summer2020/superbowl/op_mann_lda.csv'\n",
    "cols_reqd = ['tweet_text', 'text_clean', 'ad_mentioned', 'ad_keywords', 'ad_manual', 'max_prob', 'max_topic_id', 'topic_words', 'conf_matrix']\n",
    "result[cols_reqd].to_csv(op_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read random 2500 tweets from given list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sb_tweets = pd.read_csv(r'/Users/vcroopana/Downloads/summer2020/superbowl/ip/SB2020_tweets_affectivestate.csv')    \n",
    "print(sb_tweets.shape)\n",
    "sb_tweets.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def getRandomInts(nRandInts, rangeStart, rangeEnd):\n",
    "    randomlist = []\n",
    "    for i in range(0,nRandInts):\n",
    "        n = random.randint(rangeStart,rangeEnd)\n",
    "        randomlist.append(n)\n",
    "    return randomlist\n",
    "\n",
    "def retrieveRandList(randListFile):\n",
    "    randDf = pd.read_csv(randListFile) \n",
    "    randList = randDf['random'].values.tolist()\n",
    "    return randList\n",
    "\n",
    "randList = getRandomInts(2500, 0, 110735)\n",
    "randList_filename = \"/Users/vcroopana/Downloads/summer2020/superbowl/randList_2_5_K.csv\"\n",
    "pd.DataFrame(randList, columns = ['random']).to_csv(randList_filename) #Save list to file\n",
    "randTweets = sb_tweets.iloc[randList]\n",
    "randTweets.to_csv(\"/Users/vcroopana/Downloads/summer2020/superbowl/mann_ann_sb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110735, 682)\n"
     ]
    }
   ],
   "source": [
    "test_data = [cleanTweet(doc) for doc in man_ann_data['tweet_text']]\n",
    "X = buildWordVector(test_data, word2id)\n",
    "print(\"Size of test data:\"+ str(X.shape))\n",
    "\n",
    "doc_topic = model.transform(X)\n",
    "topic_vector = buildTopicVectors(doc_topic, 75, topic_words)\n",
    "\n",
    "man_ann_data = man_ann_data.reset_index(drop=True)\n",
    "result = pd.concat([man_ann_data, topic_vector], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tp:110421 n_fp:0 n_fn:314\n",
      "precision:1.0 recall:0.997 f_measure:0.999\n"
     ]
    }
   ],
   "source": [
    "result['conf_matrix'] = result.apply(lambda x: get_conf_matrix(x['ad_mentioned'], x['max_prob'], \n",
    "                                                               thresh_prob = 0.1), axis =1)\n",
    "precision, recall, f_measure = computeAccuracy(result)\n",
    "print(\"precision:\"+ str(np.round(precision,3))+\" recall:\"+ str(np.round(recall,3))\n",
    "      +\" f_measure:\"+ str(np.round(f_measure,3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#normal lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "from gensim import models\n",
    "\n",
    "def buildDict(train_data):\n",
    "    ad_words = []\n",
    "    for keywrds in train_data:\n",
    "        keywordsSet = list(set(keywrds.split()))\n",
    "        ad_words.extend(keywordsSet)\n",
    "\n",
    "    ad_words = list(set(ad_words))\n",
    "    print(\"Length of ad keywords: \"+ str(len(ad_words)))\n",
    "\n",
    "    word2id = dict((v, idx) for idx, v in enumerate(ad_words)) #dict of word: id\n",
    "    return word2id\n",
    "\n",
    "\n",
    "def displayModel(model, corpus, dictionary):\n",
    "    \n",
    "    lda_model_display = pyLDAvis.gensim.prepare(model, corpus, dictionary, sort_topics=True, mds='mmds')\n",
    "    pyLDAvis.show(lda_model_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sort_tuple(tupList):  \n",
    "    tupList.sort(reverse = True, key = lambda x: x[1])  \n",
    "    return tupList  \n",
    "\n",
    "def printTopics(model, nTopics, nWords):\n",
    "#     topics = list(set(model.show_topics(nTopics, 5)))\n",
    "#     topics.sort(key = lambda x: x[1])\n",
    "    for i in range(0, nTopics):\n",
    "        topic = model.show_topic(i, 100)  \n",
    "        sort_tuple(topic)\n",
    "        print(str(i)+\":\"+ str(topic[0:nWords]))\n",
    "        \n",
    "def get_lda_model(nTopics, ad_corpus, ad_dictionary, annotations_data):\n",
    "    \n",
    "#     annotations_data['keywords_clean'] = annotations_data['Keywords'].apply(lambda x: cleanTweet(x))\n",
    "    n_words = len(ad_dictionary) #379, 40k - for 2lakh tweets\n",
    "\n",
    "    model = models.ldamodel.LdaModel(ad_corpus, num_topics= nTopics, \n",
    "                                     id2word = ad_dictionary, \n",
    "    #                                  chunksize= 74, \n",
    "                                     alpha = 'auto', \n",
    "#                                      passes=50,\n",
    "#                                      eta = topic_word_priors,\n",
    "#                                      per_word_topics = True\n",
    "#                                      minimum_phi_value = 0.001\n",
    "    #                                  minimum_probability = 0.000001\n",
    "                                    ) \n",
    "    topics = model.print_topics(nTopics, num_words= 100)\n",
    "    print(len(ad_dictionary))\n",
    "    print(len(ad_corpus))\n",
    "    print(len(topics))\n",
    "    printTopics(model, nTopics, nWords = 5)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5309\n",
      "2500\n",
      "75\n",
      "0:[('shakira', 0.04269479), ('grow', 0.028393073), ('fact', 0.027062088), ('los', 0.027044864), ('pepsihalftime', 0.024754902)]\n",
      "1:[('ers', 0.031258497), ('tarts', 0.03098391), ('pop', 0.030983828), ('pretzel', 0.030983007), ('cry', 0.029526664)]\n",
      "2:[('announcement', 0.063551664), ('tombrady', 0.061657645), ('ad', 0.061657395), ('brady', 0.022319697), ('tom', 0.022318989)]\n",
      "3:[('nba', 0.032000337), ('petition', 0.031205203), ('kobe', 0.025993345), ('going', 0.024465458), ('bryant', 0.024464268)]\n",
      "4:[('mathieu', 0.049486637), ('th', 0.043711223), ('superbowl', 0.036069967), ('football', 0.029940888), ('star', 0.024757495)]\n",
      "5:[('nfl', 0.06172497), ('app', 0.049421523), ('fox', 0.04006005), ('sbliv', 0.02682572), ('sports', 0.025650632)]\n",
      "6:[('take', 0.04876036), ('miami', 0.04476696), ('left', 0.03801245), ('halftime', 0.03782972), ('show', 0.036236554)]\n",
      "7:[('two', 0.057215206), ('next', 0.03667879), ('mathieu', 0.03503913), ('era', 0.035037976), ('niners', 0.02101851)]\n",
      "8:[('road', 0.040979147), ('game', 0.04069465), ('chiefskingdom', 0.030055314), ('size', 0.027792064), ('ers', 0.01999878)]\n",
      "9:[('going', 0.038571056), ('battle', 0.029810853), ('left', 0.029810743), ('captmahomesii', 0.029810641), ('bruised', 0.029810613)]\n",
      "10:[('babynut', 0.05854179), ('also', 0.046139788), ('record', 0.03490172), ('superbowl', 0.027015077), ('seeing', 0.023670243)]\n",
      "11:[('ers', 0.050841447), ('brady', 0.030762851), ('quick', 0.025357079), ('tom', 0.02286367), ('drinks', 0.016641876)]\n",
      "12:[('super', 0.04243226), ('bowl', 0.04242759), ('nfl', 0.02183702), ('ad', 0.017879006), ('bud', 0.017648442)]\n",
      "13:[('text', 0.039272178), ('dad', 0.03927204), ('football', 0.039271872), ('little', 0.039271843), ('leave', 0.03927169)]\n",
      "14:[('super', 0.0422032), ('bowl', 0.041654233), ('trump', 0.023394182), ('reid', 0.022919437), ('andy', 0.022456693)]\n",
      "15:[('chiefskingdom', 0.067733385), ('champions', 0.055786356), ('world', 0.053703237), ('chiefs', 0.045069747), ('sbliv', 0.04247711)]\n",
      "16:[('chiefs', 0.026685648), ('lucky', 0.02433886), ('game', 0.01968209), ('shakira', 0.017186103), ('bryant', 0.016760452)]\n",
      "17:[('nicknames', 0.033703323), ('let', 0.020032888), ('trump', 0.019221961), ('people', 0.018456722), ('giving', 0.01687936)]\n",
      "18:[('ers', 0.10191718), ('great', 0.09541969), ('season', 0.09388663), ('thanks', 0.09282952), ('faithful', 0.09282923)]\n",
      "19:[('realdonaldtrump', 0.029894467), ('proud', 0.029893816), ('ppl', 0.02849106), ('ers', 0.017770544), ('win', 0.016371518)]\n",
      "20:[('would', 0.02881568), ('pepsihalftime', 0.028225314), ('superbowl', 0.023477037), ('baby', 0.021468272), ('j', 0.020708293)]\n",
      "21:[('hard', 0.021815484), ('everything', 0.020794213), ('paid', 0.020776518), ('know', 0.020112883), ('george', 0.019049738)]\n",
      "22:[('nas', 0.04825747), ('lil', 0.04575374), ('babynut', 0.027724689), ('elliott', 0.025357915), ('far', 0.025357287)]\n",
      "23:[('superbowl', 0.110828094), ('brady', 0.034346104), ('tom', 0.03428042), ('go', 0.028953468), ('commercial', 0.025147721)]\n",
      "24:[('america', 0.05573074), ('us', 0.03352821), ('let', 0.032759633), ('made', 0.032693494), ('among', 0.02335971)]\n",
      "25:[('baby', 0.07420399), ('groot', 0.0454352), ('yoda', 0.029473573), ('ers', 0.020965721), ('babynut', 0.019316038)]\n",
      "26:[('incredible', 0.03185004), ('exactly', 0.03056498), ('shakira', 0.029054513), ('halftimeshow', 0.027209071), ('halftime', 0.023542197)]\n",
      "27:[('super', 0.06370843), ('bowl', 0.039100874), ('wild', 0.032467514), ('sitting', 0.024000078), ('cell', 0.023966238)]\n",
      "28:[('b', 0.043739803), ('boy', 0.04197439), ('charlie', 0.031756543), ('ers', 0.028684165), ('back', 0.023718279)]\n",
      "29:[('ers', 0.03521438), ('chiefs', 0.033235855), ('fan', 0.026684217), ('know', 0.02607539), ('super', 0.02455978)]\n",
      "30:[('went', 0.039781976), ('walmart', 0.030953735), ('another', 0.028302046), ('red', 0.0269765), ('kc', 0.026975151)]\n",
      "31:[('miami', 0.039655868), ('imagine', 0.035755422), ('superbowl', 0.031137967), ('beautiful', 0.024616918), ('looks', 0.023982557)]\n",
      "32:[('commercial', 0.05176374), ('jason', 0.04836637), ('momoa', 0.02126486), ('wednesday', 0.020731525), ('far', 0.016439674)]\n",
      "33:[('superbowl', 0.062837146), ('go', 0.05299637), ('patrickmahomes', 0.040395297), ('chiefs', 0.03911811), ('chiefskingdom', 0.03074332)]\n",
      "34:[('enjoy', 0.034003444), ('excited', 0.03400129), ('thank', 0.032493737), ('fuck', 0.031401966), ('bowl', 0.02772059)]\n",
      "35:[('originalfunko', 0.04469668), ('amazon', 0.032461997), ('pop', 0.027337328), ('win', 0.022384858), ('exclusive', 0.022382054)]\n",
      "36:[('stuff', 0.036152735), ('chiefs', 0.03140544), ('winners', 0.025891386), ('superbowl', 0.025302423), ('next', 0.02385742)]\n",
      "37:[('th', 0.033441238), ('ers', 0.03325908), ('momoa', 0.030956905), ('first', 0.02520896), ('jason', 0.025037106)]\n",
      "38:[('gt', 0.090857826), ('lopez', 0.045036532), ('jennifer', 0.045029104), ('show', 0.030046849), ('watch', 0.027116282)]\n",
      "39:[('first', 0.023728525), ('touchdown', 0.022747146), ('juicecheck', 0.022746734), ('well', 0.022745056), ('ch', 0.02274452)]\n",
      "40:[('fans', 0.035424363), ('chiefskingdom', 0.03433935), ('wrong', 0.03432951), ('think', 0.022649378), ('take', 0.019729145)]\n",
      "41:[('ers', 0.054426216), ('gets', 0.036203623), ('proud', 0.025945673), ('quarter', 0.024557656), ('behind', 0.02455478)]\n",
      "42:[('snickers', 0.051153973), ('takes', 0.05115179), ('bowl', 0.02007991), ('super', 0.019267319), ('friend', 0.017845266)]\n",
      "43:[('nobody', 0.046423487), ('go', 0.044924762), ('one', 0.036971893), ('ers', 0.033095453), ('let', 0.019000683)]\n",
      "44:[('bryant', 0.056345403), ('kobe', 0.05589516), ('today', 0.05321726), ('super', 0.03168254), ('best', 0.02923906)]\n",
      "45:[('superbowl', 0.0302188), ('mathieu', 0.029275976), ('pass', 0.02408195), ('rules', 0.023304213), ('kittle', 0.023302713)]\n",
      "46:[('ers', 0.061439812), ('game', 0.027163897), ('give', 0.026333723), ('belegendary', 0.025726601), ('chiefs', 0.024384819)]\n",
      "47:[('kansas', 0.047351103), ('city', 0.040612448), ('chiefs', 0.036840457), ('daughter', 0.033408336), ('jennifer', 0.025995847)]\n",
      "48:[('doritos', 0.029848935), ('believe', 0.024615927), ('happy', 0.023311896), ('together', 0.021682862), ('feel', 0.021681199)]\n",
      "49:[('super', 0.05110951), ('bowl', 0.04921054), ('sbliv', 0.047236174), ('nfl', 0.04367986), ('chiefs', 0.028760439)]\n",
      "50:[('defense', 0.05911606), ('ers', 0.05892026), ('part', 0.03524835), ('breaking', 0.023898056), ('game', 0.018641042)]\n",
      "51:[('sherman', 0.060186416), ('mike', 0.028350735), ('bloomberg', 0.02673957), ('ers', 0.022702284), ('baby', 0.019263614)]\n",
      "52:[('sure', 0.029045962), ('brought', 0.029043088), ('ers', 0.027957378), ('superbowlliv', 0.021635517), ('money', 0.019698936)]\n",
      "53:[('ers', 0.048969343), ('left', 0.029591197), ('great', 0.02959072), ('thanks', 0.028268224), ('faithful', 0.028267466)]\n",
      "54:[('super', 0.05485681), ('bowl', 0.04885436), ('nfl', 0.044368003), ('win', 0.038843475), ('chiefs', 0.037942912)]\n",
      "55:[('shakira', 0.05373808), ('superbowl', 0.045330998), ('pepsihalftime', 0.042566266), ('come', 0.03954372), ('jlo', 0.038037796)]\n",
      "56:[('one', 0.03318838), ('thing', 0.029503496), ('therickwilson', 0.02858163), ('investment', 0.028578836), ('ad', 0.026178507)]\n",
      "57:[('https', 0.038470495), ('lmao', 0.038469672), ('nfl', 0.023239879), ('police', 0.020215876), ('jordanuhl', 0.020215815)]\n",
      "58:[('say', 0.04045254), ('star', 0.03257296), ('wars', 0.031046517), ('surprised', 0.031032182), ('super', 0.02722765)]\n",
      "59:[('ass', 0.07088362), ('shit', 0.041208252), ('beat', 0.030073762), ('could', 0.028665448), ('babynut', 0.028664414)]\n",
      "60:[('hate', 0.055378925), ('much', 0.03578042), ('trump', 0.0281643), ('donald', 0.023990309), ('ers', 0.021982098)]\n",
      "61:[('code', 0.03963363), ('makespaceforwomen', 0.028200166), ('superbowl', 0.025247516), ('trump', 0.017204419), ('white', 0.015445265)]\n",
      "62:[('ers', 0.04161718), ('nfl', 0.032989148), ('pack', 0.023943104), ('sbliv', 0.023290602), ('app', 0.022863219)]\n",
      "63:[('trump', 0.053030353), ('donald', 0.043735873), ('memes', 0.03655511), ('house', 0.036553204), ('bosa', 0.036458895)]\n",
      "64:[('baby', 0.07457948), ('drinking', 0.039598364), ('nut', 0.026858093), ('peanut', 0.026857905), ('yoda', 0.026857335)]\n",
      "65:[('bigger', 0.036622077), ('took', 0.026069451), ('ers', 0.02594975), ('hips', 0.024848297), ('gianna', 0.024839723)]\n",
      "66:[('president', 0.024913257), ('backup', 0.023774415), ('dancers', 0.02377434), ('prayer', 0.02377047), ('time', 0.023198208)]\n",
      "67:[('love', 0.05888908), ('mexico', 0.043411136), ('warner', 0.033255003), ('superbowl', 0.026692327), ('superbowlliv', 0.023669796)]\n",
      "68:[('important', 0.040740207), ('still', 0.035765015), ('ers', 0.034719486), ('help', 0.034427058), ('jimmy', 0.027385829)]\n",
      "69:[('tkelce', 0.03889796), ('superbowl', 0.022356462), ('selected', 0.020447878), ('troypolamalu', 0.020447792), ('man', 0.020447776)]\n",
      "70:[('tell', 0.0465784), ('big', 0.035191722), ('ever', 0.034591433), ('tom', 0.033379186), ('hulu', 0.03333792)]\n",
      "71:[('super', 0.055024117), ('bowl', 0.052722234), ('miami', 0.046465233), ('time', 0.038008533), ('right', 0.03717738)]\n",
      "72:[('loud', 0.039031222), ('got', 0.031913), ('hear', 0.029420212), ('ring', 0.028791433), ('get', 0.021451207)]\n",
      "73:[('brady', 0.0454464), ('tom', 0.043252382), ('zone', 0.032405812), ('superbowl', 0.028957989), ('go', 0.026383366)]\n",
      "74:[('left', 0.046705354), ('ad', 0.024615476), ('show', 0.023769097), ('nd', 0.023514943), ('amendment', 0.023514073)]\n"
     ]
    }
   ],
   "source": [
    "# displayModel(model, , word_dict)\n",
    "\n",
    "from gensim import corpora\n",
    "\n",
    "test_data = [cleanTweet(doc) for doc in man_ann_data['tweet_text']]\n",
    "\n",
    "tweet_words = [tweet.split() for tweet in test_data]\n",
    "corpora_dict = corpora.Dictionary(tweet_words)\n",
    "tweet_corpus = [corpora_dict.doc2bow(text) for text in tweet_words]\n",
    "\n",
    "lda_model = get_lda_model(75, tweet_corpus, corpora_dict, annotations_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: if you're in the IPython notebook, pyLDAvis.show() is not the best command\n",
      "      to use. Consider using pyLDAvis.display(), or pyLDAvis.enable_notebook().\n",
      "      See more information at http://pyLDAvis.github.io/quickstart.html .\n",
      "\n",
      "You must interrupt the kernel to end this command\n",
      "\n",
      "Serving to http://127.0.0.1:8891/    [Ctrl-C to exit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [23/Jun/2020 19:30:17] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Jun/2020 19:30:17] \"GET /LDAvis.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Jun/2020 19:30:17] \"GET /d3.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Jun/2020 19:30:17] \"GET /LDAvis.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Jun/2020 19:30:17] code 404, message Not Found\n",
      "127.0.0.1 - - [23/Jun/2020 19:30:17] \"GET /favicon.ico HTTP/1.1\" 404 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "stopping Server...\n"
     ]
    }
   ],
   "source": [
    "displayModel(lda_model, tweet_corpus, corpora_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
