{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding topn similar words using word2vec. However, if the word is not part of vocab, it will still fail \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK includes a pre-trained model which is part of a model that is trained on 100 billion words from the Google News Dataset. The full model is from https://code.google.com/p/word2vec/ (about 3 GB). You need to download this file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:37:46.698233Z",
     "start_time": "2020-06-01T04:37:43.610946Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim \n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:37:46.704218Z",
     "start_time": "2020-06-01T04:37:46.700227Z"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('word2vec_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:37:46.717184Z",
     "start_time": "2020-06-01T04:37:46.707210Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:37:46.727156Z",
     "start_time": "2020-06-01T04:37:46.719177Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:37:48.003353Z",
     "start_time": "2020-06-01T04:37:46.730149Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from pprint import pprint \n",
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:37:48.767329Z",
     "start_time": "2020-06-01T04:37:48.005332Z"
    }
   },
   "outputs": [],
   "source": [
    "import en_core_web_sm  # This is the default model ( vocabulary, syntax and entity)\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:37:48.774276Z",
     "start_time": "2020-06-01T04:37:48.769290Z"
    }
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "from nltk.corpus import wordnet \n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:37:48.792261Z",
     "start_time": "2020-06-01T04:37:48.778267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tagger', <spacy.pipeline.pipes.Tagger object at 0x7ff6626faf10>),\n",
      " ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x7ff665b83520>),\n",
      " ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x7ff665b834b0>)]\n"
     ]
    }
   ],
   "source": [
    "# This is the current pipeline that is used \n",
    "pprint(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:37:48.802235Z",
     "start_time": "2020-06-01T04:37:48.794256Z"
    }
   },
   "outputs": [],
   "source": [
    "#1. dataset - first three are similar and second three are dissimilar \n",
    "\n",
    "text1=[\"Trying to test how the method is for identical text.\",\n",
    "       \"I like that food.\",\n",
    "       \"Ram is very nice..\",\n",
    "      \"Pure malt whiskey.\",\n",
    "      \"It is a ferocious dog and it barks whenever anybody uknown comes near the house.\",\n",
    "      \"It is the family cow.\",\n",
    "      \"My driving license is my identity in USA as everything is linked to it.\"]\n",
    "text2=[\"Trying to test how the method is for identical text.\",\n",
    "       \"That dish is exciting\",\n",
    "       \"Is Ram very nice?\",\n",
    "       \"Fresh orange juice.\",\n",
    "       \"The painting in the art gallery is so fantastic !\",\n",
    "       \"I have been driving this sports car for last ten years and I am so satisfied! \",\n",
    "       \"The president led us to war and we lost that war! \"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:37:48.812198Z",
     "start_time": "2020-06-01T04:37:48.804197Z"
    }
   },
   "outputs": [],
   "source": [
    "# in case we are using synonym based on wordnet, there are many compound lemmas and we have to delete\n",
    "# them based on the following combinations\n",
    "chars = set('_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a pruned model from NLTK \n",
    "\n",
    "But it is better to work with the full model for any serious problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:38:02.420818Z",
     "start_time": "2020-06-01T04:37:48.814170Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.data import find\n",
    "word2vec_sample = str(find('/Users/vcroopana/nltk_data/models/word2vec_sample/pruned.word2vec.txt'))\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:38:02.437747Z",
     "start_time": "2020-06-01T04:38:02.422814Z"
    }
   },
   "outputs": [],
   "source": [
    "# this is a pruned vocabulary of the most frequent 44 K words\n",
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T03:54:00.965884Z",
     "start_time": "2020-05-30T03:54:00.962863Z"
    }
   },
   "source": [
    "## This is the full unpruned embedding ( not pruned). Better to use this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:39:22.428318Z",
     "start_time": "2020-06-01T04:38:02.440737Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load model from local\n",
    "filename = '/Users/vcroopana/gensim-data/GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load model as an object\n",
    "\n",
    "# from gensim.models.word2vec import Word2Vec\n",
    "# from gensim.models import KeyedVectors\n",
    "# import gensim.downloader as api\n",
    "\n",
    "# model = api.load(\"word2vec-google-news-300\")  # download the model and return as object ready for use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:39:22.435239Z",
     "start_time": "2020-06-01T04:39:22.430222Z"
    }
   },
   "outputs": [],
   "source": [
    "# this is a larger vocabulary\n",
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:39:36.080727Z",
     "start_time": "2020-06-01T04:39:22.437204Z"
    }
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=['university'], topn = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Doing a similarity math with this embedding\n",
    "\n",
    "king - man + woman = queen  etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:39:36.518556Z",
     "start_time": "2020-06-01T04:39:36.090700Z"
    }
   },
   "outputs": [],
   "source": [
    "#  king - man + woman = queen \n",
    "model.most_similar(positive=['woman'.lower(),'king'.lower()], negative=['man'.lower()], topn = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:39:36.845682Z",
     "start_time": "2020-06-01T04:39:36.521548Z"
    }
   },
   "outputs": [],
   "source": [
    "# Germany-Berlin+paris = france \n",
    "model.most_similar(positive=['Paris'.lower(),'Germany'.lower()], negative=['Berlin'.lower()], topn = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:39:37.222673Z",
     "start_time": "2020-06-01T04:39:36.848674Z"
    }
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=['Seoul'.lower(),'Germany'.lower()], negative=['Berlin'.lower()], topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the meaning of the two sentences by comparing the mean word2vec vectors\n",
    "\n",
    "This routine may still fail if the word is not part of vocab \n",
    "\n",
    "Before applying this routine, ensure that every word is part of vocab that you are using\n",
    "\n",
    "This function works only for a sentence but if you spply a paragraph, you need to preprocess as simple split function will not work. So to handle this, we will do preprocessing using spacy  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and lemmatization attempts to get root word (example - rain) for different word inflections (raining, rained etc). Lemma algos gives you real dictionary words, whereas stemming simply cuts off last parts of the word so its faster but less accurate. \n",
    "\n",
    "Stemming returns words which are not really dictionary words and hence you will not be able to find pretrained vectors for it in Glove, Word2Vec etc and this is a major disadvantage depending on application. You should stick to Lemmatization in this case. \n",
    "\n",
    "We will also use Spacy stopwords list and create a lemma_list and token_list per sentence.\n",
    "\n",
    "We can clean out all HTML tags by using the regex ‘<[^>]*>’; All the non word characters can be removed by ‘[\\W]+’. You should be careful though about not stripping punctuations before word contractions are handled by the lemmatizer. Note that we are doing it after lemmatization and not before. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:39:37.241623Z",
     "start_time": "2020-06-01T04:39:37.224668Z"
    }
   },
   "outputs": [],
   "source": [
    "# this is a useful function \n",
    "# It needs to be further modified so that emoticons identified are replaced by text \n",
    "# find emoticons function\n",
    "import re\n",
    "def find_emo(text):\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text)\n",
    "    return emoticons\n",
    "sample_text = \" I loved this movie :) but it was rather sad :( :]\"\n",
    "find_emo(sample_text)\n",
    "# output\n",
    "[':)', ':(']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:39:37.258577Z",
     "start_time": "2020-06-01T04:39:37.246611Z"
    }
   },
   "outputs": [],
   "source": [
    "# find a pattern text defined by regex inside a string or a list of strings\n",
    "import re\n",
    "def preprocessor(text):\n",
    "    if isinstance((text), (str)):\n",
    "        text = re.sub('<[^>]*>', '', text)\n",
    "        text = re.sub('[\\W]+', '', text.lower())\n",
    "        text = re.sub('[^\\x00-\\x7F]+', ' ', text) # removes non ascii chars\n",
    "        text = re.sub('https?://[A-Za-z0-9./]+', ' ', text) # Remove URLs\n",
    "        # remove punctuations except '_'\n",
    "        punctuation = ['(', ')', '[',']','?', ':', ':', ',', '.', '!', '/', '\"', \"'\", '@', '#', '&']\n",
    "#     text = re.sub('[^a-zA-Z]', ' ', text) # remove all other than alphabet chars \n",
    "        text = \"\".join((char for char in text if char not in punctuation))\n",
    "        text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text) # remove all single characters   \n",
    "        \n",
    "        return text\n",
    "    if isinstance((text), (list)):\n",
    "        return_list = []\n",
    "        for i in range(len(text)):\n",
    "            temp_text = re.sub('<[^>]*>', '', text[i])\n",
    "            temp_text = re.sub('[\\W]+', '', temp_text.lower())\n",
    "            return_list.append(temp_text)\n",
    "        return(return_list)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:39:37.509928Z",
     "start_time": "2020-06-01T04:39:37.263565Z"
    }
   },
   "outputs": [],
   "source": [
    "# nlp = English()  # It is already done above \n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "# using spacy tokenizer \n",
    "tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
    "\n",
    "def lemma_token(text):\n",
    "    text = text.lower()  # to take care of capital case word in glove\n",
    "    tokens = tokenizer(text)\n",
    "    token_list = []\n",
    "    lemma_list = []\n",
    "    for token in tokens:\n",
    "        if token.is_stop is False:\n",
    "            token_preprocessed = preprocessor(token.lemma_)\n",
    "#             print(token_preprocessed)\n",
    "            if token_preprocessed != '':\n",
    "                 lemma_list.append(token_preprocessed)\n",
    "                 token_list.append(token.text)   \n",
    "    #return (token_list, lemma_list)\n",
    "    return lemma_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:39:37.523889Z",
     "start_time": "2020-06-01T04:39:37.513917Z"
    }
   },
   "outputs": [],
   "source": [
    "#outputs the average word2vec for words in this sentence\n",
    "def average_vec(words, model):\n",
    "#     words = lemma_token(text)\n",
    "    #use unk word when word is not present in vocab to find a predesigned vector which is often the best vector\n",
    "    word_vecs = [model.word_vec(w) if w in model.vocab else model.word_vec('unk') for w in words ]\n",
    "    \n",
    "    op =  (np.array(word_vecs).sum(axis=0)/len(word_vecs)).reshape(1,-1)\n",
    "    return op\n",
    "\n",
    "compare = lambda a,b: cosine_similarity(average_vec(a),average_vec(b)).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:39:37.570764Z",
     "start_time": "2020-06-01T04:39:37.536859Z"
    }
   },
   "outputs": [],
   "source": [
    "compare('Pure malt whiskey'.lower(),'Fresh orange juice'.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Ad Name</th>\n",
       "      <th>Product</th>\n",
       "      <th>Key Terms  Round 1</th>\n",
       "      <th>KeyTerms_Edited</th>\n",
       "      <th>Excitatory Potential</th>\n",
       "      <th>Emotional vs. Rational</th>\n",
       "      <th>Semantic Affinity</th>\n",
       "      <th>Valence</th>\n",
       "      <th>Keywords</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ad Number</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trailer</td>\n",
       "      <td>Fast &amp; Furious 9  Trailer</td>\n",
       "      <td>Movie Trailer</td>\n",
       "      <td>fast and the furious, fast &amp; the furious, fast...</td>\n",
       "      <td>fast_and_the_furious, fast_&amp;_the_furious, fast...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Trailer Fast &amp; Furious 9  Trailer fast_and_the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quibi</td>\n",
       "      <td>Quibi  Bank Heist</td>\n",
       "      <td>Video Platform</td>\n",
       "      <td>quibi, bank heist, robbery, less than ten minu...</td>\n",
       "      <td>quibi, bank_heist, robbery, less_than_ten_minu...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Quibi Quibi  Bank Heist quibi, bank_heist, rob...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Brand Name                    Ad Name         Product  \\\n",
       "Ad Number                                                         \n",
       "1            Trailer  Fast & Furious 9  Trailer   Movie Trailer   \n",
       "2              Quibi          Quibi  Bank Heist  Video Platform   \n",
       "\n",
       "                                          Key Terms  Round 1  \\\n",
       "Ad Number                                                      \n",
       "1          fast and the furious, fast & the furious, fast...   \n",
       "2          quibi, bank heist, robbery, less than ten minu...   \n",
       "\n",
       "                                             KeyTerms_Edited  \\\n",
       "Ad Number                                                      \n",
       "1          fast_and_the_furious, fast_&_the_furious, fast...   \n",
       "2          quibi, bank_heist, robbery, less_than_ten_minu...   \n",
       "\n",
       "           Excitatory Potential  Emotional vs. Rational  Semantic Affinity  \\\n",
       "Ad Number                                                                    \n",
       "1                             1                       1                  2   \n",
       "2                             2                       1                  2   \n",
       "\n",
       "           Valence                                           Keywords  \n",
       "Ad Number                                                              \n",
       "1                1  Trailer Fast & Furious 9  Trailer fast_and_the...  \n",
       "2                2  Quibi Quibi  Bank Heist quibi, bank_heist, rob...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_data = pd.read_csv('/Users/vcroopana/Downloads/summer2020/superbowl/ip/SB_ad_annotations.csv', index_col=0) \n",
    "annotations_data['Keywords'] = annotations_data['Brand Name']\\\n",
    "                                .str.cat(annotations_data['Ad Name'], sep=\" \")\\\n",
    "                                .str.cat(annotations_data['KeyTerms_Edited'], sep=\" \")\n",
    "df = annotations_data.drop_duplicates()\n",
    "print(df.shape)\n",
    "annotations_data.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ò</th>\n",
       "      <th>user_id</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>time_of_tweet</th>\n",
       "      <th>user_location</th>\n",
       "      <th>team followed</th>\n",
       "      <th>affective_state</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>ad_keywords</th>\n",
       "      <th>ad_mentioned</th>\n",
       "      <th>ad_manual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64628</td>\n",
       "      <td>2.836102e+09</td>\n",
       "      <td>1.220000e+18</td>\n",
       "      <td>Mon Feb 03 03:53:56 +0000 2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "      <td>1</td>\n",
       "      <td>Man I wanna look at the brightside \"we thought...</td>\n",
       "      <td>John legend, Chrissy Teigen, genesis, hyundai,...</td>\n",
       "      <td>genesis  going away party</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Ò       user_id      tweet_id                   time_of_tweet  \\\n",
       "0  64628  2.836102e+09  1.220000e+18  Mon Feb 03 03:53:56 +0000 2020   \n",
       "\n",
       "  user_location team followed  affective_state  \\\n",
       "0           NaN          both                1   \n",
       "\n",
       "                                          tweet_text  \\\n",
       "0  Man I wanna look at the brightside \"we thought...   \n",
       "\n",
       "                                         ad_keywords  \\\n",
       "0  John legend, Chrissy Teigen, genesis, hyundai,...   \n",
       "\n",
       "                ad_mentioned ad_manual  \n",
       "0  genesis  going away party      none  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "man_ann_data = pd.read_csv(r'/Users/vcroopana/Downloads/summer2020/superbowl/mann_ann_sb.csv')    \n",
    "man_ann_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "cadillacabc joinrt 31k james_bond\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "print(len(stop))\n",
    "\n",
    "def removeMentions(text):\n",
    "\n",
    "    textBeforeMention = text.partition(\"@\")[0]\n",
    "    textAfterMention = text.partition(\"@\")[2]\n",
    "    textAfterMention =  re.sub(r':', '', textAfterMention) #cadillac join the 31k\n",
    "    tHandle = textAfterMention.partition(\" \")[0].lower() #cadillac    \n",
    "    text = textBeforeMention+ \" \" + textAfterMention  \n",
    "    return text\n",
    "\n",
    "def cleanTweet(strinp):\n",
    "    strinp = re.sub(r'RT', \"\", strinp) # Remove RT\n",
    "    strinp = strinp.lower()\n",
    "    \n",
    "    stop_removed_list = [word for word in strinp.split() if word not in (stop)]\n",
    "    stop_removed = ' '.join([str(elem) for elem in stop_removed_list])    \n",
    "    text = re.sub('https?://[A-Za-z0-9./]+', ' ', stop_removed) # Remove URLs\n",
    "    text = removeMentions(text)\n",
    "    text = re.sub('[^\\x00-\\x7F]+', ' ', text) # Remove non-ASCII chars.\n",
    "    \n",
    "    # remove punctuations except '_'\n",
    "    punctuation = ['(', ')', '[',']','?', ':', ':', ',', '.', '!', '/', '\"', \"'\", '@', '#', '&']\n",
    "#     text = re.sub('[^a-zA-Z]', ' ', text) # remove all other than alphabet chars \n",
    "    text = \"\".join((char for char in text if char not in punctuation))\n",
    "    \n",
    "#     text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text) # remove all single characters     \n",
    "    stop_removed_l = [word for word in text.split() if word not in (stop)]\n",
    "    stop_removed = ' '.join([str(elem) for elem in stop_removed_l]) \n",
    "    return stop_removed\n",
    "\n",
    "print(cleanTweet(\"RT @cadillacabc: Joinrt the 31K james_bond\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ad_keywords_clean = annotations_data['Keywords'].apply(lambda ad: cleanTweet(ad))\n",
    "man_ann_data['tweet_clean'] = man_ann_data['tweet_text'].apply(lambda twt: cleanTweet(twt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_avg_sim_df = pd.DataFrame(columns = annotations_data['Ad Name'])\n",
    "\n",
    "def compareTweetAndAds(tweet):\n",
    "    ad_keywords = annotations_data['Keywords']\n",
    "    ad_sim = []\n",
    "    for ad in ad_keywords:\n",
    "        compare_res = compare(tweet.lower(), ad.lower())\n",
    "        ad_sim.append(compare_res)\n",
    "    return ad_sim\n",
    "#     max_sim_index = ad_sim.index(max(ad_sim))\n",
    "#     print(max_sim_index)\n",
    "#     if(max_sim_index>0.9):\n",
    "#         return annotations_data.iloc[max_sim_index]['Ad Name']\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "man_ann_data['glove_avg'] = man_ann_data['tweet_text'].apply(lambda twt: compareTweetAndAds(twt))\n",
    "man_ann_data.to_csv(\"/Users/vcroopana/Downloads/summer2020/superbowl/mann_ann_sb_temp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_glove_similarity(annotations_data, man_ann_data, model):\n",
    "    ##### Data Init\n",
    "\n",
    "    ad_keywords_clean = annotations_data['Keywords'].apply(lambda ad: cleanTweet(ad))\n",
    "    man_ann_data['tweet_clean'] = man_ann_data['tweet_text'].apply(lambda twt: cleanTweet(twt))\n",
    "\n",
    "    ###### Preprocessing\n",
    "    tweets_lemma_tokens = man_ann_data['tweet_clean'].apply(lambda x: lemma_token(x))\n",
    "    print(\"Calculated pos tokens of tweets\")\n",
    "    ads_lemma_tokens = ad_keywords_clean.apply(lambda x: lemma_token(x))\n",
    "    print(\"Calculated pos tokens of ads\")\n",
    "\n",
    "    ###### Sim calculation\n",
    "    ad_id = 1\n",
    "    glove_sim_df = pd.DataFrame(columns = annotations_data['Ad Name'])\n",
    "\n",
    "    for ad in ads_lemma_tokens:\n",
    "        glove_sim_col =[]\n",
    "        avg_vec_ad = average_vec(ad, model)\n",
    "        print(\"computing sim for ad:\" + str(ad_id))\n",
    "        for i in range(0, len(tweets_lemma_tokens)): \n",
    "            avg_vec_twt = average_vec(tweets_lemma_tokens[i], model)     \n",
    "            if  avg_vec_twt.shape == avg_vec_ad.shape:\n",
    "                glove_sim = cosine_similarity(avg_vec_twt,avg_vec_ad).sum()\n",
    "            else:\n",
    "                glove_sim = 0\n",
    "            glove_sim_col.append(np.round(glove_sim,3))\n",
    "\n",
    "        glove_sim_df[annotations_data['Ad Name'][ad_id]] = glove_sim_col\n",
    "        ad_id = ad_id + 1\n",
    "    print(\"glove POS Similarities Calculated\")\n",
    "    return glove_sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model from local\n",
    "filename = '/Users/vcroopana/gensim-data/GoogleNews-vectors-negative300.bin'\n",
    "glove_model = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
    "print('loaded model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_sim_df = compute_glove_similarity(annotations_data, man_ann_data, glove_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top n similar ads computed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vcroopana/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Support for multi-dimensional indexing (e.g. `index[:, None]`) on an Index is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "nlargest = 5\n",
    "data = glove_sim_df\n",
    "result_con = getTopNSimAds(nlargest, data)\n",
    "## merge mann ann data and sim result\n",
    "glove_sim_df_merged = pd.concat([man_ann_data, result_con], axis =1)\n",
    "\n",
    "glove_sim_df_merged.head()\n",
    "\n",
    "print(\"Top n similar ads computed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tp:91 n_fp:364 n_fn:759 n_tn:1286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2, 0.10705882352941176, 0.13946360153256704)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "glove_sim_df_merged['conf_matrix'] = glove_sim_df_merged.apply(lambda x: get_conf_matrix(x['ad_manual'], x['top1_ad'], \n",
    "                                                          x['top1'], 0.7), axis =1)\n",
    "# man_ann_data_glove_pos['conf_matrix'] = man_ann_data_glove_pos.apply(lambda x: get_conf_matrix_2(x['ad_manual'], x['top1_ad'], \n",
    "#                             x['top2_ad'], x['top3_ad'], x['top4_ad'], x['top5_ad']), axis =1)\n",
    "computeAccuracy(glove_sim_df_merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_sim_df_merged.to_csv('/Users/vcroopana/Downloads/summer2020/superbowl/sim_glove_0.7.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Finding most similar words using Glove \n",
    "\n",
    "We will use the pre-trained word vectors from Glove. We are especially interested in Twitter dataset with 2B tweets, 27B tokens, and 200d vectors.\n",
    "\n",
    "We use Gensim to convert Glove vectors into the word2vec, then use KeyedVectors to load vectors in word2vec format.\n",
    "\n",
    "The source is this article : https://towardsdatascience.com/how-to-solve-analogies-with-word2vec-6ebaf2354009\n",
    "\n",
    "Same issue as in Word2Vec - words must exist in the vocabulary \n",
    "Also it makes sense to lower case everything otherwise may get word does NOT exist error\n",
    "\n",
    "Unlike word2Vec, while using Glove, remember to lowercase the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:40:10.269485Z",
     "start_time": "2020-06-01T04:39:37.577751Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import os\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "\n",
    "\n",
    "path_glove = os.path.abspath('/Users/vcroopana/gensim-data/glove/glove.twitter.27B.200d.txt')\n",
    "path_w2v = os.path.abspath('/Users/vcroopana/gensim-data/glove/glove.twitter.27B.200d_w2v.txt')\n",
    "\n",
    "glove_file = datapath(path_glove)\n",
    "tmp_file = get_tmpfile(path_w2v)\n",
    "\n",
    "_ = glove2word2vec(glove_file, tmp_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:07.516926Z",
     "start_time": "2020-06-01T04:40:10.276395Z"
    }
   },
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:09.697107Z",
     "start_time": "2020-06-01T04:44:07.517895Z"
    }
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=['boy'], topn = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:09.796803Z",
     "start_time": "2020-06-01T04:44:09.699066Z"
    }
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=['man'], topn = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:09.812761Z",
     "start_time": "2020-06-01T04:44:09.798798Z"
    }
   },
   "outputs": [],
   "source": [
    "compare('Quick fox jumps over dog'.lower(),'Fast fox jumps over puppy'.lower())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate similarity between text pairs ( after pre-procssing)\n",
    "\n",
    "It uses the lemma_token() function using spacy \n",
    "\n",
    "(a) tokenizes\n",
    "\n",
    "(b) picks up the lemma \n",
    "\n",
    "(c) eliminates the stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:09.844735Z",
     "start_time": "2020-06-01T04:44:09.814756Z"
    }
   },
   "outputs": [],
   "source": [
    "l1= lemma_token(text1[0])\n",
    "l2= lemma_token(text1[1])\n",
    "l3= lemma_token(text1[2])\n",
    "l4= lemma_token(text1[3])\n",
    "l5= lemma_token(text1[4])\n",
    "l6= lemma_token(text1[5])\n",
    "\n",
    "# compared with \n",
    "l1_c= lemma_token(text2[0])\n",
    "l2_c= lemma_token(text2[1])\n",
    "l3_c= lemma_token(text2[2])\n",
    "l4_c= lemma_token(text2[3])\n",
    "l5_c= lemma_token(text2[4])\n",
    "l6_c= lemma_token(text2[5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:09.871603Z",
     "start_time": "2020-06-01T04:44:09.846669Z"
    }
   },
   "outputs": [],
   "source": [
    "print(text1[0])\n",
    "print (\"\\n\")\n",
    "print(text2[0])\n",
    "print (\"\\n\")\n",
    "print(l1)\n",
    "print (\"\\n\")\n",
    "print(l1_c)\n",
    "print(\"\\n Similarity after pre_processing: {}\".format(compare(text1[0],text2[0])))\n",
    "print (\"=====================================\\n\")\n",
    "\n",
    "print(text1[1])\n",
    "print (\"\\n\")\n",
    "print(text2[1])\n",
    "print (\"\\n\")\n",
    "print(l2)\n",
    "print (\"\\n\")\n",
    "print(l2_c)\n",
    "\n",
    "print(\"\\n Similarity: {}\".format(compare(text1[1],text2[1])))\n",
    "print (\"=====================================\\n\")\n",
    "\n",
    "print(text1[2])\n",
    "print (\"\\n\")\n",
    "print(text2[2])\n",
    "print (\"\\n\")\n",
    "print(l3)\n",
    "print (\"\\n\")\n",
    "print(l3_c)\n",
    "\n",
    "print(\"\\n Similarity: {}\".format(compare(text1[2],text2[2])))\n",
    "print (\"=====================================\\n\")\n",
    "\n",
    "print(text1[3])\n",
    "print (\"\\n\")\n",
    "print(text2[3])\n",
    "print (\"\\n\")\n",
    "print(l4)\n",
    "print (\"\\n\")\n",
    "print(l4_c)\n",
    "\n",
    "print(\"\\n Similarity: {}\".format(compare(text1[3],text2[3])))\n",
    "print (\"=====================================\\n\")\n",
    "\n",
    "print(text1[4])\n",
    "print (\"\\n\")\n",
    "print(text2[4])\n",
    "print (\"\\n\")\n",
    "print(l5)\n",
    "print (\"\\n\")\n",
    "print(l5_c)\n",
    "\n",
    "print(\"\\n Similarity: {}\".format(compare(text1[4],text2[4])))\n",
    "print (\"=====================================\\n\")\n",
    "\n",
    "print(text1[5])\n",
    "print (\"\\n\")\n",
    "print(text2[5])\n",
    "print (\"\\n\")\n",
    "print(l6)\n",
    "print (\"\\n\")\n",
    "print(l6_c)\n",
    "\n",
    "print(\"\\n Similarity: {}\".format(compare(text1[5],text2[5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Modifying similarity logic further by considering POS tag types\n",
    "\n",
    "Clearly it did not work as the text has lot of implied meanings. So, just looking at average of all the words even after pre-processing  is not working. \n",
    "\n",
    "One option is to identify the nouns and develop a similarity around that.  \n",
    "\n",
    "Currently the code takes noun, adj, adv, verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:09.880580Z",
     "start_time": "2020-06-01T04:44:09.873619Z"
    }
   },
   "outputs": [],
   "source": [
    "# we will try to return lemma list of only the nouns in the text\n",
    "def lemma_token_pos(text):\n",
    "    text = text.lower()  # to take care of capital case word in glove\n",
    "    doc=nlp(text)\n",
    "    lemma_list = []\n",
    "    for token in doc:\n",
    "        if token.is_stop is False:\n",
    "            if (token.pos_ == 'NOUN' or token.pos_ == 'VERB' or token.pos_ == 'ADJ' or token.pos_ == 'adv'):\n",
    "                token_preprocessed = preprocessor(token.lemma_)\n",
    "                if token_preprocessed != '':\n",
    "                     lemma_list.append(token_preprocessed)\n",
    "                     #return (token_list, lemma_list)\n",
    "#     print(lemma_list)\n",
    "#     print(\"\\n\")\n",
    "    return lemma_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:10.045795Z",
     "start_time": "2020-06-01T04:44:09.882573Z"
    }
   },
   "outputs": [],
   "source": [
    "#testing \n",
    "lemma_token_pos(text1[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:10.053773Z",
     "start_time": "2020-06-01T04:44:10.047791Z"
    }
   },
   "outputs": [],
   "source": [
    "#outputs the average word2vec for words in this sentence\n",
    "def average_vec_pos(words):\n",
    "#     words = lemma_token_pos(text)\n",
    "    #use unk word when word is not present in vocab to find a predesigned vector which is often the best vector\n",
    "    word_vecs = [model.word_vec(w) if w in model.vocab else model.word_vec('unk') for w in words ]\n",
    "    if(len(word_vecs) == 0):\n",
    "#         print('len of word vec=0')\n",
    "        return (np.array(word_vecs).sum(axis=0)).reshape(1,-1)\n",
    "    else:                \n",
    "        return (np.array(word_vecs).sum(axis=0)/len(word_vecs)).reshape(1,-1)\n",
    "\n",
    "compare_pos = lambda a,b: cosine_similarity(average_vec_pos(a),average_vec_pos(b)).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def getTopNSimAds(nlargest, data):\n",
    "    \n",
    "    order = np.argsort(-data.values, axis=1)[:, :nlargest]\n",
    "    result = pd.DataFrame(data.columns[order], \n",
    "                          columns=['top{}_ad'.format(i) for i in range(1, nlargest+1)],\n",
    "                          index= data.index)\n",
    "\n",
    "    order_vals = np.sort(-data.values, axis=1)[:, :nlargest]\n",
    "    result_vals = pd.DataFrame(-order_vals,\n",
    "                              columns = ['top{}'.format(i) for i in range(1, nlargest+1)],\n",
    "                              index= data.index)\n",
    "    \n",
    "    result_con = pd.concat([result_vals, result], axis =1)\n",
    "    return result_con\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conf_matrix(ad_manual, ad_algo, ad_prob, thresh_prob):\n",
    "    res = \"\"\n",
    "    ad_manual = ad_manual.lower()\n",
    "    ad_algo = ad_algo.lower()\n",
    "\n",
    "    if(ad_manual == ad_algo and ad_prob >= thresh_prob):\n",
    "        res = 'TP'\n",
    "    elif(ad_manual == ad_algo and ad_prob < thresh_prob):\n",
    "        res = 'FN'\n",
    "    elif(ad_manual=='none' and ad_manual != ad_algo and ad_prob > thresh_prob):\n",
    "        res = 'FP'\n",
    "    elif(ad_manual=='none' and ad_manual != ad_algo and ad_prob < thresh_prob):\n",
    "        res = 'TN'\n",
    "    elif(ad_manual!='none' and ad_manual!= ad_algo):\n",
    "        res = 'FN'\n",
    "    return res\n",
    "def get_conf_matrix_2(ad_manual, ad_algo, ad_algo_2, ad_algo_3, ad_algo_4, ad_algo_5):\n",
    "    res = \"\"\n",
    "    ad_manual = ad_manual.lower()\n",
    "    ad_algo = ad_algo.lower()\n",
    "    ad_algo_2 = ad_algo_2.lower()\n",
    "    ad_algo_3 = ad_algo_3.lower()\n",
    "    ad_algo_4 = ad_algo_4.lower()\n",
    "    ad_algo_5 = ad_algo_5.lower()\n",
    "    \n",
    "    if(ad_manual == ad_algo or ad_manual == ad_algo_2 or ad_manual == ad_algo_3 or ad_manual == ad_algo_4\n",
    "      or ad_manual == ad_algo_5):\n",
    "        res = 'TP'\n",
    "    elif(ad_manual=='none'):\n",
    "        res = 'FP'\n",
    "    elif(ad_manual!='none' and ad_manual!= ad_algo and ad_manual!= ad_algo_2 and ad_manual!= ad_algo_3\n",
    "        and ad_manual!= ad_algo_4 and ad_manual!= ad_algo_5):\n",
    "        res = 'FN'\n",
    "    elif(ad_manual!='none' and ad_manual!= ad_algo and ad_manual!= ad_algo_2 and ad_manual!= ad_algo_3\n",
    "        and ad_manual!= ad_algo_4 and ad_manual!= ad_algo_5):\n",
    "        res = 'TN'\n",
    "    return res\n",
    "\n",
    "def computeAccuracy(result):    \n",
    "    n_tp = result[result['conf_matrix'] == 'TP'].shape[0]\n",
    "    n_fp = result[result['conf_matrix'] == 'FP'].shape[0]\n",
    "    n_fn = result[result['conf_matrix'] == 'FN'].shape[0]\n",
    "    n_tn = result[result['conf_matrix'] == 'TN'].shape[0]    \n",
    "    print(\"n_tp:\" + str(n_tp)+ \" n_fp:\" + str(n_fp)+ \" n_fn:\" + str(n_fn) + \" n_tn:\" + str(n_tn))\n",
    "    precision = n_tp/(n_tp+ n_fp)\n",
    "    recall = n_tp/(n_tp+ n_fn)\n",
    "    f_measure = (2*precision*recall)/ (precision+recall)\n",
    "\n",
    "    return precision, recall, f_measure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Data Init\n",
    "ad_keywords_clean = annotations_data['Keywords'].apply(lambda ad: cleanTweet(ad))\n",
    "man_ann_data['tweet_clean'] = man_ann_data['tweet_text'].apply(lambda twt: cleanTweet(twt))\n",
    "\n",
    "###### Preprocessing\n",
    "tweets_lemma_tokens = man_ann_data['tweet_clean'].apply(lambda x: lemma_token_pos(x))\n",
    "print(\"Calculated pos tokens of tweets\")\n",
    "ads_lemma_tokens = ad_keywords_clean.apply(lambda x: lemma_token_pos(x))\n",
    "print(\"Calculated pos tokens of ads\")\n",
    "\n",
    "###### Sim calculation\n",
    "ad_id = 1\n",
    "glove_pos_sim_df = pd.DataFrame(columns = annotations_data['Ad Name'])\n",
    "\n",
    "for ad in ads_lemma_tokens:\n",
    "    glove_pos_sim_col =[]\n",
    "    avg_vec_pos_ad = average_vec_pos(ad)\n",
    "    print(\"computing sim for ad:\" + str(ad_id))\n",
    "    for i in range(0, len(tweets_lemma_tokens)): \n",
    "        avg_vec_pos_twt = average_vec_pos(tweets_lemma_tokens[i])     \n",
    "        if  avg_vec_pos_twt.shape == avg_vec_pos_ad.shape:\n",
    "            glove_pos_sim = cosine_similarity(avg_vec_pos_twt,avg_vec_pos_ad).sum()\n",
    "        else:\n",
    "            glove_pos_sim = 0\n",
    "        glove_pos_sim_col.append(np.round(glove_pos_sim,3))\n",
    "    \n",
    "    glove_pos_sim_df[annotations_data['Ad Name'][ad_id]] = glove_pos_sim_col\n",
    "    ad_id = ad_id + 1\n",
    "glove_pos_sim_df.head(3)\n",
    "print(\"glove POS Similarities Calculated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top n similar ads computed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vcroopana/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Support for multi-dimensional indexing (e.g. `index[:, None]`) on an Index is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "nlargest = 5\n",
    "data = glove_pos_sim_df\n",
    "result_con = getTopNSimAds(nlargest, data)\n",
    "## merge mann ann data and sim result\n",
    "man_ann_data_glove_pos = pd.concat([man_ann_data, result_con], axis =1)\n",
    "\n",
    "man_ann_data_glove_pos.head()\n",
    "\n",
    "print(\"Top n similar ads computed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tp:22 n_fp:432 n_fn:828 n_tn:1218\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.048458149779735685, 0.02588235294117647, 0.033742331288343565)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "man_ann_data_glove_pos['conf_matrix'] = man_ann_data_glove_pos.apply(lambda x: get_conf_matrix(x['ad_manual'], x['top1_ad'], \n",
    "                                                          x['top1'], 0.8), axis =1)\n",
    "# man_ann_data_glove_pos['conf_matrix'] = man_ann_data_glove_pos.apply(lambda x: get_conf_matrix_2(x['ad_manual'], x['top1_ad'], \n",
    "#                             x['top2_ad'], x['top3_ad'], x['top4_ad'], x['top5_ad']), axis =1)\n",
    "computeAccuracy(man_ann_data_glove_pos)\n",
    "#man_ann_data_glove_pos.to_csv(\"/Users/vcroopana/Downloads/summer2020/superbowl/sim_glove_pos_0.8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_ann_data_glove_pos.to_csv(\"/Users/vcroopana/Downloads/summer2020/superbowl/sim_glove_pos_0.8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:10.169795Z",
     "start_time": "2020-06-01T04:44:10.055769Z"
    }
   },
   "outputs": [],
   "source": [
    "print(text1[0])\n",
    "print (\"\\n\")\n",
    "print(text2[0])\n",
    "print (\"\\n\")\n",
    "print(\"\\n Similarity after pre_processing for noun: {}\".format(compare_pos(text1[0],text2[0])))\n",
    "print (\"=====================================\\n\")\n",
    "\n",
    "\n",
    "print(text1[1])\n",
    "print (\"\\n\")\n",
    "print(text2[1])\n",
    "print (\"\\n\")\n",
    "print(\"\\n Similarity after pre_processing for noun: {}\".format(compare_pos(text1[1],text2[1])))\n",
    "print (\"=====================================\\n\")\n",
    "\n",
    "\n",
    "\n",
    "print(text1[2])\n",
    "print (\"\\n\")\n",
    "print(text2[2])\n",
    "print (\"\\n\")\n",
    "print(\"\\n Similarity after pre_processing for noun: {}\".format(compare_pos(text1[2],text2[2])))\n",
    "print (\"=====================================\\n\")\n",
    "\n",
    "\n",
    "\n",
    "print(text1[3])\n",
    "print (\"\\n\")\n",
    "print(text2[3])\n",
    "print (\"\\n\")\n",
    "print(\"\\n Similarity after pre_processing for noun: {}\".format(compare_pos(text1[3],text2[3])))\n",
    "print (\"=====================================\\n\")\n",
    "\n",
    "\n",
    "print(text1[4])\n",
    "print (\"\\n\")\n",
    "print(text2[4])\n",
    "print (\"\\n\")\n",
    "print(\"\\n Similarity after pre_processing for noun: {}\".format(compare_pos(text1[4],text2[4])))\n",
    "print (\"=====================================\\n\")\n",
    "\n",
    "print(text1[5])\n",
    "print (\"\\n\")\n",
    "print(text2[5])\n",
    "print (\"\\n\")\n",
    "print(\"\\n Similarity after pre_processing for noun: {}\".format(compare_pos(text1[5],text2[5])))\n",
    "print (\"=====================================\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things have now clearly changed.  Working only with the nouns declutters things a lot and similarity value has come down. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  We will now improve similarity by adding  top 5 similar words to each side for each word.. It is like pronouncing the difference or similarity \n",
    "\n",
    "We are still using the \"model\" initialized already by loading Glove and converting to Word2Vec \n",
    "\n",
    "You can see below that this does not work as distribution semantics is picking up all \"lemme\" kind of verbs that are mostly occuring together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:10.270499Z",
     "start_time": "2020-06-01T04:44:10.171790Z"
    }
   },
   "outputs": [],
   "source": [
    "#testing out ...\n",
    "synonym_list=[]\n",
    "tuple_list=model.most_similar(positive=['woman'], topn = 4)\n",
    "for a_tuple in tuple_list:\n",
    "    synonym_list.append(a_tuple[0])\n",
    "print(synonym_list)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So we refine our previous function with synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:10.282514Z",
     "start_time": "2020-06-01T04:44:10.272495Z"
    }
   },
   "outputs": [],
   "source": [
    "# we will try to return lemma list of only the POS in the text\n",
    "def lemma_token_pos_synonyms(text, allowed_pos, model):\n",
    "    text = text.lower()  # to take care of capital case word in glove\n",
    "    doc=nlp(text)\n",
    "    lemma_list = []\n",
    "    synonym_list=[]\n",
    "    for token in doc:\n",
    "        if token.is_stop is False:\n",
    "            if (token.pos_ in allowed_pos):\n",
    "                token_preprocessed = preprocessor(token.lemma_)\n",
    "                if token_preprocessed != '':\n",
    "                     lemma_list.append(token_preprocessed)\n",
    "    #print(lemma_list)\n",
    "    for lemma in lemma_list:\n",
    "        tuple_list=model.most_similar(positive=['lemma'], topn = 5)\n",
    "        for a_tuple in tuple_list:\n",
    "            synonym_list.append(a_tuple[0])\n",
    "    lemma_list=lemma_list+synonym_list        \n",
    "#     print(\"\\n Extended lemma list : \")\n",
    "#     print(lemma_list)\n",
    "#     print(\"\\n\")\n",
    "    return lemma_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:10.296430Z",
     "start_time": "2020-06-01T04:44:10.287457Z"
    }
   },
   "outputs": [],
   "source": [
    "#outputs the average word2vec for words in this sentence\n",
    "def average_vec_pos_syn(words, model):\n",
    "#     words = lemma_token_pos_synonyms(text)\n",
    "#     word_vecs = [model.word_vec(w) for w in words]\n",
    "#     return (np.array(word_vecs).sum(axis=0)/len(word_vecs)).reshape(1,-1)\n",
    "    word_vecs = [model.word_vec(w) if w in model.vocab else model.word_vec('unk') for w in words ]\n",
    "    if(len(word_vecs) == 0):\n",
    "        return (np.array(word_vecs).sum(axis=0)).reshape(1,-1)\n",
    "    else:                \n",
    "        return (np.array(word_vecs).sum(axis=0)/len(word_vecs)).reshape(1,-1)\n",
    "\n",
    "compare_pos_syn = lambda a,b: cosine_similarity(average_vec_pos_syn(a),average_vec_pos_syn(b)).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimilarityDf(annotations_data, ads_lemma_tokens, tweets_lemma_tokens, model):\n",
    "    ad_id = 1\n",
    "    glove_pos_sim_df = pd.DataFrame(columns = annotations_data['Ad Name'])\n",
    "        \n",
    "    for ad in ads_lemma_tokens:\n",
    "        glove_pos_sim_col =[]\n",
    "        avg_vec_pos_ad = average_vec_pos_wordnet_syn(ad, model)\n",
    "        print(\"computing sim for ad:\" + str(ad_id))\n",
    "        for i in range(0, len(tweets_lemma_tokens)): \n",
    "            avg_vec_pos_twt = average_vec_pos_wordnet_syn(tweets_lemma_tokens[i], model)     \n",
    "            if  avg_vec_pos_twt.shape == avg_vec_pos_ad.shape:\n",
    "                glove_pos_sim = cosine_similarity(avg_vec_pos_twt,avg_vec_pos_ad).sum()\n",
    "            else:\n",
    "                glove_pos_sim = 0\n",
    "            glove_pos_sim_col.append(np.round(glove_pos_sim,3))\n",
    "            \n",
    "        glove_pos_sim_df[annotations_data['Ad Name'][ad_id]] = glove_pos_sim_col\n",
    "        ad_id = ad_id + 1\n",
    "            \n",
    "    return glove_pos_sim_df\n",
    "\n",
    "def computeGlovePOSSynSimilarity(annotations_data, man_ann_data, model, allowed_pos, tweets_lemma_tokens, ads_lemma_tokens):\n",
    "    ##### Data Init\n",
    "    ad_keywords_clean = annotations_data['keywords_clean']\n",
    "        \n",
    "    ###### Preprocessing - tokenize and filter POS\n",
    "    \n",
    "#     tweets_lemma_tokens = man_ann_data['tweet_clean'].apply(\n",
    "#         lambda x: lemma_token_pos_synonyms(x, allowed_pos, model))\n",
    "#     print(\"Calculated pos tokens of tweets\")\n",
    "#     ads_lemma_tokens = ad_keywords_clean.apply(\n",
    "#         lambda x: lemma_token_pos_synonyms(x, allowed_pos, model))\n",
    "#     print(\"Calculated pos tokens of ads\")\n",
    "        \n",
    "    ###### Similarity calculation\n",
    "    glove_pos_syn_df = getSimilarityDf(annotations_data, ads_lemma_tokens, tweets_lemma_tokens, model)\n",
    "\n",
    "    print(\"Glove POS Synonymn Similarities Calculated\")\n",
    "    return glove_pos_syn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_pos = ('NOUN', 'VERB', 'ADJ', 'adv')\n",
    "tweets_lemma_syn_tokens = man_ann_data['tweet_clean'].apply(lambda x: lemma_token_pos_synonyms(x, allowed_pos, model))\n",
    "print(\"Calculated pos tokens of tweets\")\n",
    "ads_lemma_syn_tokens = ad_keywords_clean.apply(lambda x: lemma_token_pos_synonyms(x, allowed_pos, model))\n",
    "print(\"Calculated pos tokens of ads\")\n",
    "\n",
    "glove_pos_syn_sim = computeGlovePOSSynSimilarity(annotations_data, man_ann_data, model, allowed_pos,tweets_lemma_syn_tokens, ads_lemma_syn_tokens )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vcroopana/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Support for multi-dimensional indexing (e.g. `index[:, None]`) on an Index is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tp:49 n_fp:1570 n_fn:801 n_tn:80\n"
     ]
    }
   ],
   "source": [
    "nlargest = 5\n",
    "data = glove_pos_syn_sim\n",
    "result_con = getTopNSimAds(nlargest, data)\n",
    "## merge mann ann data and sim result\n",
    "glove_pos_syn_sim_merged = pd.concat([man_ann_data, result_con], axis =1)\n",
    "\n",
    "glove_pos_syn_sim_merged['conf_matrix'] = glove_pos_syn_sim_merged.apply(lambda x: get_conf_matrix(x['ad_manual'], x['top1_ad'], \n",
    "                                                          x['top1'], 0.9), axis =1)\n",
    "# man_ann_data_glove_pos['conf_matrix'] = man_ann_data_glove_pos.apply(lambda x: get_conf_matrix_2(x['ad_manual'], x['top1_ad'], \n",
    "#                             x['top2_ad'], x['top3_ad'], x['top4_ad'], x['top5_ad']), axis =1)\n",
    "computeAccuracy(glove_pos_syn_sim_merged)\n",
    "\n",
    "glove_pos_syn_sim_merged.to_csv(\"/Users/vcroopana/Downloads/summer2020/superbowl/sim_glove_pos_syn_0.9.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:11.273824Z",
     "start_time": "2020-06-01T04:44:10.299422Z"
    }
   },
   "outputs": [],
   "source": [
    "print(text1[0])\n",
    "print (\"\\n\")\n",
    "print(text2[0])\n",
    "print (\"\\n\")\n",
    "print(\"\\n Similarity after pre_processing for noun: {}\".format(compare_pos_syn(text1[0],text2[0])))\n",
    "print (\"=====================================\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will try to define some kind of cross similarity now based on Glove Embedding. Hopefully it will give us better result\n",
    "\n",
    "Given two lemma list, we do pairwise similarity and then normalize the same\n",
    "\n",
    "Hypothesis : a word in a source sentence if compared with all the words in target sentence may capure more as average all words in source or target is making us lose some context \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:11.287785Z",
     "start_time": "2020-06-01T04:44:11.275818Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_glove_pos_cross_similary(text1,text2,model,flag):\n",
    "    if flag==True:\n",
    "        words1 = lemma_token_pos(text1)\n",
    "        words2 = lemma_token_pos(text2)\n",
    "    else:\n",
    "        words1 = lemma_token(text1)\n",
    "        words2 = lemma_token(text2)        \n",
    "    print(words1)\n",
    "    print (\"\\n\")\n",
    "    print(words2)\n",
    "    print (\"\\n\")\n",
    "    if flag==True:\n",
    "        print(\"\\n POS tag is used as filter\")\n",
    "    else:\n",
    "        print(\"\\n POS tag is NOT used as filter\")\n",
    "    cross_cos_similarity_12 = 0\n",
    "    word_vecs2 = [model.word_vec(w) for w in words2]\n",
    "    pairwise_cos_similarity = 0\n",
    "    for word in words1:\n",
    "        word_vecs1=model.word_vec(word).reshape(1, -1)\n",
    "        pairwise_cos_similarity = cosine_similarity(word_vecs1,word_vecs2).sum()\n",
    "        cross_cos_similarity_12 =cross_cos_similarity_12 + pairwise_cos_similarity \n",
    "    norm_cross_cos_similarity = (cross_cos_similarity_12)/ (len(words1)*len(words2)) \n",
    "    print(\"\\n cross_cos_similarity_12 : {}\".format(cross_cos_similarity_12))\n",
    "    print(\"\\n norm_cross_cos_similarity : {}\".format(norm_cross_cos_similarity))\n",
    "    return norm_cross_cos_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_glove_pos_cross_similary(annotations_data,man_ann_data,model,flag):\n",
    "    ad_keywords_clean = annotations_data['keywords_clean']\n",
    "    tweets_lemma_tokens =[]\n",
    "    ads_lemma_tokens = []\n",
    "    if flag==True:\n",
    "        allowed_pos = ('NOUN', 'VERB', 'ADJ', 'adv')\n",
    "#         tweets_lemma_tokens = man_ann_data['tweet_clean'].apply(lambda x: self.lemma_token_pos(x, allowed_pos))\n",
    "        print(\"Calculated pos tokens of tweets\")\n",
    "#         ads_lemma_tokens = ad_keywords_clean.apply(lambda x: self.lemma_token_pos(x, allowed_pos))\n",
    "        print(\"Calculated pos tokens of ads\")\n",
    "    else:\n",
    "        tweets_lemma_tokens = man_ann_data['tweet_clean'].apply(lambda x: lemma_token(x))\n",
    "        print(\"Calculated pos tokens of tweets\")\n",
    "        ads_lemma_tokens = ad_keywords_clean.apply(lambda x: lemma_token(x))\n",
    "        print(\"Calculated pos tokens of ads\")\n",
    "\n",
    "    ad_id = 1\n",
    "    glove_pos_sim_df = pd.DataFrame(columns = annotations_data['Ad Name'])\n",
    "        \n",
    "    for ad in ads_lemma_tokens:\n",
    "        ad_token_filtd = [w for w in ad if w in model.vocab]\n",
    "        word_vecs_ad = [model.word_vec(w) for w in ad_token_filtd]\n",
    "        glove_pos_sim_col =[]\n",
    "        if(len(ad_token_filtd) == 0):\n",
    "            glove_pos_sim_col = [0] * len(tweets_lemma_tokens) \n",
    "        else:\n",
    "            print(\"computing sim for ad:\" + str(ad_id))\n",
    "            for i in range(0, len(tweets_lemma_tokens)): \n",
    "                tweets_lemma_filtd = [ w for w in tweets_lemma_tokens[i] if w in model.vocab ]\n",
    "                if(len(tweets_lemma_filtd)!=0):\n",
    "                    pairwise_cos_similarity = 0\n",
    "                    cross_cos_similarity_12 = 0\n",
    "                    for word in tweets_lemma_filtd:\n",
    "                        word_vecs_tweet = model.word_vec(word).reshape(1, -1)\n",
    "                        pairwise_cos_similarity = cosine_similarity(word_vecs_tweet ,word_vecs_ad).sum()\n",
    "                        cross_cos_similarity_12 = cross_cos_similarity_12 + pairwise_cos_similarity \n",
    "                    norm_cross_cos_similarity = (cross_cos_similarity_12)/ (len(ad_token_filtd)*len(tweets_lemma_filtd)) \n",
    "        #             print(\"\\n cross_cos_similarity_12 : {}\".format(cross_cos_similarity_12))\n",
    "#                     print(\"norm_cross_cos_similarity : {}\".format(norm_cross_cos_similarity))\n",
    "                else:\n",
    "                    norm_cross_cos_similarity =0\n",
    "                    \n",
    "                glove_pos_sim_col.append(np.round(norm_cross_cos_similarity,3))\n",
    "            \n",
    "        glove_pos_sim_df[annotations_data['Ad Name'][ad_id]] = glove_pos_sim_col\n",
    "        ad_id = ad_id + 1\n",
    "        \n",
    "    return glove_pos_sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_data['keywords_clean'] = annotations_data['Keywords'].apply(lambda ad: cleanTweet(ad))\n",
    "man_ann_data['tweet_clean'] = man_ann_data['tweet_text'].apply(lambda twt: cleanTweet(twt))\n",
    "glove_pos_cross_similary = compute_glove_pos_cross_similary(annotations_data,man_ann_data,model,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top n similar ads computed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vcroopana/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Support for multi-dimensional indexing (e.g. `index[:, None]`) on an Index is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "nlargest = 5\n",
    "data = glove_pos_cross_similary\n",
    "\n",
    "result_con = getTopNSimAds(nlargest, data)\n",
    "## merge mann ann data and sim result\n",
    "man_ann_data_glove_pos_cross_similary = pd.concat([man_ann_data, result_con], axis =1)\n",
    "\n",
    "man_ann_data_glove_pos_cross_similary.head()\n",
    "\n",
    "print(\"Top n similar ads computed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_ann_data_glove_pos_cross_similary.to_csv('/Users/vcroopana/Downloads/summer2020/superbowl/sim_glove_pos_cross_similary.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tp:24 n_fp:610 n_fn:826 n_tn:1031\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.03785488958990536, 0.02823529411764706, 0.032345013477088944)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "man_ann_data_glove_pos_cross_similary['conf_matrix'] = man_ann_data_glove_pos_cross_similary.apply(lambda x: get_conf_matrix(x['ad_manual'], x['top1_ad'], \n",
    "                                                          x['top1'], 0.4), axis =1)\n",
    "# man_ann_data_glove_pos['conf_matrix'] = man_ann_data_glove_pos.apply(lambda x: get_conf_matrix_2(x['ad_manual'], x['top1_ad'], \n",
    "#                             x['top2_ad'], x['top3_ad'], x['top4_ad'], x['top5_ad']), axis =1)\n",
    "computeAccuracy(man_ann_data_glove_pos_cross_similary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:11.303743Z",
     "start_time": "2020-06-01T04:44:11.289780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test']\n",
      "\n",
      "\n",
      "['test']\n",
      "\n",
      "\n",
      "\n",
      " POS tag is NOT used as filter\n",
      "\n",
      " cross_cos_similarity_12 : 1.0\n",
      "\n",
      " norm_cross_cos_similarity : 1.0\n"
     ]
    }
   ],
   "source": [
    "# try it out \n",
    "# cos_similarity = calculate_glove_pos_cross_similary(text1[0],text2[0],model,False)\n",
    "cos_similarity = calculate_glove_pos_cross_similary(\"this is a test\",\"loikujgh\",model,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:11.327679Z",
     "start_time": "2020-06-01T04:44:11.305739Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_pair_cross_similarity(text1,text2,model,flag):\n",
    "    print(\"\\n Cross Glove Similarity  : {}\".format(calculate_glove_pos_cross_similary(text1,text2,model,False)))\n",
    "    print (\"=====================================\\n\")\n",
    "\n",
    "for i in range(6):\n",
    "    text_pair_cross_similarity(text1[i],text2[i],model,False)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using wordnet to add synonyms to each of the text in pair of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:12.579254Z",
     "start_time": "2020-06-01T04:44:11.328677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'traveling', 'go', 'locomotion', 'trip', 'journey', 'move_around', 'move', 'jaunt', 'travelling', 'locomote', 'change_of_location', 'travel'}\n"
     ]
    }
   ],
   "source": [
    "## try out code \n",
    "from nltk.corpus import wordnet \n",
    "#Creating a list \n",
    "synonyms = []\n",
    "for syn in wordnet.synsets(\"travel\"):\n",
    "    for lm in syn.lemmas():\n",
    "             synonyms.append(lm.name())#adding into synonyms\n",
    "print (set(synonyms))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly getting the top n nearest words using distributional semantics DOES NOT work as all these \"letme\" etc are the most frequent occuring patterns !!! \n",
    "\n",
    "So, we will try out enhancing with wordnet !! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:12.586236Z",
     "start_time": "2020-06-01T04:44:12.581254Z"
    }
   },
   "outputs": [],
   "source": [
    "# we also need to take care of composite lemma in wordnet. We just delete them now\n",
    "\n",
    "def clean_lemma(lemma_list,chars):\n",
    "    new_lemma_list=[]\n",
    "    delete=0\n",
    "    for s in lemma_list:\n",
    "        if any((c in chars) for c in s):\n",
    "            delete+=1\n",
    "            #print(\"\\n one composite lemma from wordnet found .. deleting..\")\n",
    "        else:\n",
    "            new_lemma_list.append(s.lower())\n",
    "    #print(new_lemma_list)\n",
    "#     print(\"Total composite lemma deleted: {}\".format(delete))\n",
    "    return(new_lemma_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:12.598218Z",
     "start_time": "2020-06-01T04:44:12.589228Z"
    }
   },
   "outputs": [],
   "source": [
    "# we will try to return lemma list of only the nouns in the text\n",
    "# apparently it works best for noun and verb for some scenarios\n",
    "\n",
    "def lemma_token_pos_wordnet_syn(text):\n",
    "    text = text.lower()  # to take care of capital case word in glove\n",
    "    doc = nlp(text)\n",
    "    lemma_list = []\n",
    "    synonym_list=[]\n",
    "    for token in doc:\n",
    "        if token.is_stop is False:\n",
    "            #if (token.pos_ == 'NOUN' or token.pos_ == 'VERB'):\n",
    "            if (token.pos_ == 'NOUN' or token.pos_ == 'VERB' or token.pos_ == 'ADJ' or token.pos_ == 'adv'):\n",
    "                token_preprocessed = preprocessor(token.lemma_)\n",
    "                if token_preprocessed != '':\n",
    "                     lemma_list.append(token_preprocessed)\n",
    "#     print(\"\\n lemma list after 1st preprocessing: \")\n",
    "#     print(lemma_list) # test out \n",
    "    # There can be too many lemmas. So, we limit to lem_c=3\n",
    "    for lemma in lemma_list:\n",
    "        for syn in wordnet.synsets(lemma):\n",
    "            for lm in syn.lemmas():\n",
    "                synonym_list.append(lm.name())#adding into synonyms\n",
    "    # In wordnet, there are synsets where same word is used in slightly different meanings\n",
    "    # so we have to make a list of set \n",
    "    lemma_list=list(set(lemma_list+synonym_list))        \n",
    "    lemma_list=clean_lemma(lemma_list,chars)\n",
    "#     print(\"\\n lemma list after wordnet & cleanup :\")\n",
    "#     print(lemma_list)\n",
    "#     print(\"\\n\")\n",
    "    return lemma_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:12.634140Z",
     "start_time": "2020-06-01T04:44:12.600199Z"
    }
   },
   "outputs": [],
   "source": [
    "lemma_list= lemma_token_pos_wordnet_syn(\"My driving license is my identity in USA as everything is linked to it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:12.640141Z",
     "start_time": "2020-06-01T04:44:12.636134Z"
    }
   },
   "outputs": [],
   "source": [
    "#testing the routine. There will not be any more lemma to delete really \n",
    "print(clean_lemma(lemma_list,chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:12.653091Z",
     "start_time": "2020-06-01T04:44:12.642119Z"
    }
   },
   "outputs": [],
   "source": [
    "# we implement two checks \n",
    "# 1. For all composite words that wordnet gives as x_y, we delete \n",
    "# 2. Check if the words are existing in vocab. If not, skip \n",
    "\n",
    "def average_vec_pos_wordnet_syn(words,model):\n",
    "#     words = lemma_token_pos_wordnet_syn(text)\n",
    "    word_list=[]\n",
    "    omit=0\n",
    "    for word in words:\n",
    "        if word in model.vocab:\n",
    "            word_list.append(word)\n",
    "        else:\n",
    "            omit +=1\n",
    "    word_vecs = [model.word_vec(w) for w in word_list]\n",
    "#     print(\"Total no of words not found in vocab: {}\".format(omit))\n",
    "    if(len(word_vecs) ==0):\n",
    "        return (np.array(word_vecs).sum(axis=0)).reshape(1,-1)\n",
    "    \n",
    "    else:\n",
    "        return (np.array(word_vecs).sum(axis=0)/len(word_vecs)).reshape(1,-1)\n",
    "\n",
    "def compare_pos_wordnet_syn(a,b):\n",
    "    cos=cosine_similarity(average_vec_pos_wordnet_syn(a,model),average_vec_pos_wordnet_syn(b,model)).sum()\n",
    "    return cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_pos_wordnet_cross_similarity(annotations_data, man_ann_data, model):\n",
    "    ##### Data Init\n",
    "    ad_keywords_clean = annotations_data['keywords_clean']\n",
    "        \n",
    "    ###### Preprocessing - tokenize\n",
    "    tweets_lemma_tokens = man_ann_data['tweet_clean'].apply(lambda x: lemma_token_pos_wordnet_syn(x))\n",
    "    print(\"Calculated pos tokens of tweets\")\n",
    "    ads_lemma_tokens = ad_keywords_clean.apply(lambda x: lemma_token_pos_wordnet_syn(x))\n",
    "    print(\"Calculated pos tokens of ads\")\n",
    "        \n",
    "    ###### Similarity calculation\n",
    "    glove_sim_df = getSimilarityDf(annotations_data, ads_lemma_tokens, tweets_lemma_tokens, model)\n",
    "    print(\"Glove Similarities Calculated\")\n",
    "    return glove_sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_wordnet_sim = compute_pos_wordnet_cross_similarity(annotations_data, man_ann_data, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vcroopana/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Support for multi-dimensional indexing (e.g. `index[:, None]`) on an Index is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top n similar ads computed\n",
      "n_tp:25 n_fp:1318 n_fn:825 n_tn:332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.018615040953090096, 0.029411764705882353, 0.022799817601459188)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlargest = 5\n",
    "data = pos_wordnet_sim\n",
    "\n",
    "result_con = getTopNSimAds(nlargest, data)\n",
    "## merge mann ann data and sim result\n",
    "pos_wordnet_sim_merged = pd.concat([man_ann_data, result_con], axis =1)\n",
    "\n",
    "print(\"Top n similar ads computed\")\n",
    "pos_wordnet_sim_merged.to_csv('/Users/vcroopana/Downloads/summer2020/superbowl/sim_glove_pos_wordnet_similary.csv')\n",
    "\n",
    "pos_wordnet_sim_merged['conf_matrix'] = pos_wordnet_sim_merged.apply(lambda x: get_conf_matrix(x['ad_manual'], x['top1_ad'], \n",
    "                                                          x['top1'], 0.8), axis =1)\n",
    "# man_ann_data_glove_pos['conf_matrix'] = man_ann_data_glove_pos.apply(lambda x: get_conf_matrix_2(x['ad_manual'], x['top1_ad'], \n",
    "#                             x['top2_ad'], x['top3_ad'], x['top4_ad'], x['top5_ad']), axis =1)\n",
    "computeAccuracy(pos_wordnet_sim_merged)\n",
    "#Results: 0.8\n",
    "# Top n similar ads computed\n",
    "# n_tp:25 n_fp:1318 n_fn:825 n_tn:332\n",
    "# Out[111]:\n",
    "# (0.018615040953090096, 0.029411764705882353, 0.022799817601459188)\n",
    "\n",
    "# Results : 0.9\n",
    "# Top n similar ads computed\n",
    "# n_tp:12 n_fp:742 n_fn:838 n_tn:908\n",
    "# Out[110]:\n",
    "# (0.015915119363395226, 0.01411764705882353, 0.014962593516209476)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:12.690931Z",
     "start_time": "2020-06-01T04:44:12.654086Z"
    }
   },
   "outputs": [],
   "source": [
    "cos=compare_pos_wordnet_syn(text1[5],text2[5])\n",
    "print(cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:57:25.925606Z",
     "start_time": "2020-06-01T04:57:25.741592Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_pair_similarity(text1,text2):\n",
    "    print(text1)\n",
    "    print (\"\\n\")\n",
    "    print(text2)\n",
    "    print (\"\\n\")\n",
    "    print(\"\\n Similarity after pre_processing for POS and adding synomyms: {}\".format(compare_pos_wordnet_syn(text1,text2)))\n",
    "    print (\"=====================================\\n\")\n",
    "\n",
    "for i in range(7):\n",
    "    text_pair_similarity(text1[i],text2[i])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:12.865523Z",
     "start_time": "2020-06-01T04:44:12.843589Z"
    }
   },
   "outputs": [],
   "source": [
    "doc=nlp(\"Ram sings beautifully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:13.032790Z",
     "start_time": "2020-06-01T04:44:12.867518Z"
    }
   },
   "outputs": [],
   "source": [
    "lemma_list=[]\n",
    "for token in doc:\n",
    "    # Print the token and its part-of-speech tag\n",
    "    print(token.text, \"-->\", token.pos_,\"---->\",token.lemma_)\n",
    "    token_preprocessed = preprocessor(token.lemma_)\n",
    "    if token_preprocessed != '':\n",
    "        lemma_list.append(token_preprocessed)\n",
    "    for lemma in lemma_list:\n",
    "        for syn in wordnet.synsets(lemma):\n",
    "            for lm in syn.lemmas():\n",
    "                synonym_list.append(lm.name())#adding into synonyms\n",
    "    # In wordnet, there are synsets where same word is used in slightly different meanings\n",
    "    # so we have to make a list of set \n",
    "    lemma_list=list(set(lemma_list+synonym_list))        \n",
    "    lemma_list=clean_lemma(lemma_list,chars)\n",
    "print(lemma_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:13.045755Z",
     "start_time": "2020-06-01T04:44:13.034785Z"
    }
   },
   "outputs": [],
   "source": [
    "word_list=[]\n",
    "omit=0\n",
    "for word in lemma_list:\n",
    "    if word in model.vocab:\n",
    "        word_list.append(word)\n",
    "    else:\n",
    "        omit +=1\n",
    "word_vecs = [model.word_vec(w) for w in word_list]\n",
    "print(\"Total no of words not found in vocab: {}\".format(omit))\n",
    "a= (np.array(word_vecs).sum(axis=0)/len(word_vecs)).reshape(1,-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings \n",
    "\n",
    "Essentially the idea of getting all lemma for all entries in synset may not work as it can lead to hundreds and it will completely destroy the calculations. \n",
    "\n",
    "We have also seen that for very short sentence it may not work ( will give error) as you will be using POS tagging that may not identify any word ( due to shortness of sentence or error in tagger). In that case, you have no lemma identified and the code will give error !! \n",
    "\n",
    "The other thing observed is : POS tagger is also statistical and it is never 100% accurate. So, garbage in garbage out may happen \n",
    "\n",
    "Best findings so far : stick to Glove but pick up only the correct POS Tagged words so that your similarity calculation is more accurate \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wordnet based implementation of semantic cross similarity  \n",
    "\n",
    "We will do cross similarity so that we get similarity with the full target context.\n",
    "\n",
    "There are quite a few WordNet Similarities. Few have values greater than 1. Wu Palmer is between 0 and 1\n",
    "\n",
    "Also word.n.01 is the deepest level in the wordnet hierarchy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:13.055729Z",
     "start_time": "2020-06-01T04:44:13.047750Z"
    }
   },
   "outputs": [],
   "source": [
    "filtered_sent1 = []\n",
    "filtered_sent2 = []\n",
    "counter1 = 0\n",
    "counter2 = 0\n",
    "sent21_similarity = 0\n",
    "sent12_similarity = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:13.066716Z",
     "start_time": "2020-06-01T04:44:13.057724Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add synonyms to match list\n",
    "\n",
    "\n",
    "def synonymsCreator(word):\n",
    "    synonyms = []\n",
    "\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for i in syn.lemmas():\n",
    "            synonyms.append(i.name())\n",
    "\n",
    "    return synonyms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:13.075718Z",
     "start_time": "2020-06-01T04:44:13.068730Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cehck and return similarity\n",
    "\n",
    "\n",
    "def simlilarityCheck(word1, word2):\n",
    "\n",
    "    word1 = word1 + \".n.01\"\n",
    "    word2 = word2 + \".n.01\"\n",
    "    try:\n",
    "        w1 = wordnet.synset(word1)\n",
    "        w2 = wordnet.synset(word2)\n",
    "\n",
    "        return w1.wup_similarity(w2)\n",
    "\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# -----------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:13.089663Z",
     "start_time": "2020-06-01T04:44:13.077694Z"
    }
   },
   "outputs": [],
   "source": [
    "# try out the code \n",
    "sent1 = text1[0]\n",
    "sent2 = text2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:13.102632Z",
     "start_time": "2020-06-01T04:44:13.092658Z"
    }
   },
   "outputs": [],
   "source": [
    "def simpleFilter(sentence,syn_flag):\n",
    "    # Does not do wordnet synonym\n",
    "    filtered_sent = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(sentence)\n",
    "\n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            filtered_sent.append(lemmatizer.lemmatize(w))\n",
    "            if syn_flag==True:\n",
    "                for i in synonymsCreator(w):\n",
    "                    filtered_sent.append(i)\n",
    "    return filtered_sent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:13.125684Z",
     "start_time": "2020-06-01T04:44:13.104622Z"
    }
   },
   "outputs": [],
   "source": [
    "filtered_sent1 = simpleFilter(sent1,False)\n",
    "filtered_sent2 = simpleFilter(sent2, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:13.133586Z",
     "start_time": "2020-06-01T04:44:13.127594Z"
    }
   },
   "outputs": [],
   "source": [
    "filtered_sent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:13.143587Z",
     "start_time": "2020-06-01T04:44:13.136570Z"
    }
   },
   "outputs": [],
   "source": [
    "filtered_sent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:44:13.155519Z",
     "start_time": "2020-06-01T04:44:13.145547Z"
    }
   },
   "outputs": [],
   "source": [
    "#let us calculate all cross word similarity check between the two sentences\n",
    "# for i words and j words, we have ixj combinations of similarity \n",
    "#Taking the ixj kind of captures the similarity of one word of one sentence \n",
    "# with the context consttuted by all words in the other sentence \n",
    "# note that, jxi will mean the same thing from the opposite direction \n",
    "# we can probably normalize this with (i*j) assuming that is the max value \n",
    "def calculate_wordnet_similary(text1,text2,syn_flag):\n",
    "    filtered_sent1 = simpleFilter(text1,syn_flag)\n",
    "    filtered_sent2 = simpleFilter(text2,syn_flag)\n",
    "    sent1_count = len(filtered_sent1 )\n",
    "    sent2_count = len (filtered_sent2 )\n",
    "    if syn_flag==True: # in case we are taking synonyms\n",
    "        filtered_sent1=list(set(filtered_sent1))\n",
    "        filtered_sent2=list(set(filtered_sent2))\n",
    "        filtered_sent1=clean_lemma(filtered_sent1,chars)\n",
    "        filtered_sent2=clean_lemma(filtered_sent2,chars)\n",
    "    sent12_similarity=0\n",
    "    sent21_similarity=0\n",
    "    \n",
    "    for i in filtered_sent1:\n",
    "        for j in filtered_sent2:\n",
    "            sent12_similarity = sent12_similarity + simlilarityCheck(i, j)\n",
    "    normalised_similarity= (sent12_similarity+sent21_similarity ) /(sent1_count*sent2_count)       \n",
    "    print(sent12_similarity)\n",
    "    #print (sent21_similarity)\n",
    "    print(normalised_similarity )\n",
    "    return(normalised_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_word_net_semantic_similarity(annotations_data, man_ann_data, model, syn_flag):\n",
    "    ##### Data Init\n",
    "    ad_keywords_clean = annotations_data['keywords_clean']\n",
    "    tweets_clean = man_ann_data['tweet_clean']\n",
    "    \n",
    "    ad_id = 1\n",
    "    wordnet_sim_df = pd.DataFrame(columns = annotations_data['Ad Name'])\n",
    "    \n",
    "    for ad in ad_keywords_clean:\n",
    "        \n",
    "        wordnet_sim_col =[]\n",
    "        filtered_ad = simpleFilter(ad,syn_flag)      \n",
    "        sent_ad_count = len(filtered_ad )\n",
    "        \n",
    "        if syn_flag==True: # in case we are taking synonyms\n",
    "            filtered_ad=list(set(filtered_ad))           \n",
    "            filtered_ad=clean_lemma(filtered_ad,chars)            \n",
    "        \n",
    "        print(\"computing sim for ad:\" + str(ad_id))\n",
    "        \n",
    "        for i in range(0, len(tweets_clean)): \n",
    "            filtered_sent2 = simpleFilter(tweets_clean[i],syn_flag)\n",
    "            sent2_count = len(filtered_sent2)\n",
    "            if syn_flag==True:\n",
    "                filtered_sent2=list(set(filtered_sent2))\n",
    "                filtered_sent2=clean_lemma(filtered_sent2,chars)\n",
    "            sent12_similarity=0\n",
    "            sent21_similarity=0 # Where is this used?\n",
    "            \n",
    "            for p in filtered_ad:\n",
    "                for q in filtered_sent2:\n",
    "                    sent12_similarity = sent12_similarity + simlilarityCheck(p, q)\n",
    "            normalised_similarity= (sent12_similarity + sent21_similarity ) /(sent_ad_count*sent2_count)    \n",
    "            \n",
    "            wordnet_sim_col.append(normalised_similarity)\n",
    "            \n",
    "        wordnet_sim_df[annotations_data['Ad Name'][ad_id]] = wordnet_sim_col\n",
    "        ad_id = ad_id + 1\n",
    "    print(\"Wordnet Similarities Calculated\")\n",
    "    return wordnet_sim_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_sim_df = compute_word_net_semantic_similarity(annotations_data, man_ann_data, model, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vcroopana/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Support for multi-dimensional indexing (e.g. `index[:, None]`) on an Index is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top n similar ads computed\n",
      "n_tp:1 n_fp:488 n_fn:849 n_tn:1162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.002044989775051125, 0.001176470588235294, 0.0014936519790888722)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlargest = 5\n",
    "data = wordnet_sim_df\n",
    "\n",
    "result_con = getTopNSimAds(nlargest, data)\n",
    "## merge mann ann data and sim result\n",
    "wordnet_sim_df_merged = pd.concat([man_ann_data, result_con], axis =1)\n",
    "\n",
    "print(\"Top n similar ads computed\")\n",
    "\n",
    "wordnet_sim_df_merged['conf_matrix'] = wordnet_sim_df_merged.apply(lambda x: get_conf_matrix(x['ad_manual'], x['top1_ad'], \n",
    "                                                          x['top1'], 0.19), axis =1)\n",
    "# man_ann_data_glove_pos['conf_matrix'] = man_ann_data_glove_pos.apply(lambda x: get_conf_matrix_2(x['ad_manual'], x['top1_ad'], \n",
    "#                             x['top2_ad'], x['top3_ad'], x['top4_ad'], x['top5_ad']), axis =1)\n",
    "wordnet_sim_df_merged.to_csv('/Users/vcroopana/Downloads/summer2020/superbowl/sim_wordnet_cross_no_syn.csv')\n",
    "\n",
    "\n",
    "computeAccuracy(wordnet_sim_df_merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:57:51.008548Z",
     "start_time": "2020-06-01T04:57:50.958681Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_pair_similarity_run(text1,text2):\n",
    "    print(text1)\n",
    "    print (\"\\n\")\n",
    "    print(text2)\n",
    "    print (\"\\n\")\n",
    "    print(\"\\n Wordnet Similarity after pre_processing and adding synomyms: {}\".format(calculate_wordnet_similary(text1,text2,False)))\n",
    "    print (\"=====================================\\n\")\n",
    "\n",
    "for i in range(7):\n",
    "    text_pair_similarity_run(text1[i],text2[i])    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Findings including BERT similarity from another notebook on Google Colab \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th>Sentence pair</th>\n",
    "    <th>Comments</th>  \n",
    "    <th>Glove average</th>\n",
    "    <th>Glove average+POS </th>\n",
    "    <th>Glove average+top_n </th>\n",
    "    <th>Glove cross similarity </th>\n",
    "    <th>Wordnet -no Synonyms (wu palmer) </th>\n",
    "    <th>Wordnet + cross similarity </th>\n",
    "    <th>BERT based similarity </th>  \n",
    "      \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>[Trying to test how the method is for identical text.],[Trying to test how the method is for identical text.] </td>\n",
    "    <td>Similarity should be 1.00 </td>  \n",
    "    <td>1.0 </td>\n",
    "    <td> 1.0 </td>\n",
    "    <td> 1.0 </td>\n",
    "    <td> 1.0 </td>\n",
    "    <td> 1.0 </td>\n",
    "    <td> 1.0  </td>\n",
    "      <td> 1.0 </td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>[I like that food],[That dish is exciting] </td>\n",
    "    <td>Quite close as both about good dish </td>  \n",
    "    <td>0.46 </td>\n",
    "    <td> 0.59</td>\n",
    "    <td>0.97 </td>\n",
    "    <td>0.33 </td>\n",
    "    <td>0.64 </td>\n",
    "    <td>0.70 </td>\n",
    "     <td> 0.85 </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>[Ram is very nice..],[Is Ram very nice?] </td>\n",
    "    <td> Essentially same statement but put differently</td>\n",
    "    <td>0.99</td>\n",
    "    <td>0.81 </td>\n",
    "    <td>0.97 </td>\n",
    "    <td>0.64 </td>\n",
    "    <td>0.78 </td>\n",
    "    <td>0.31 </td>\n",
    "      <td>0.76  </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>[Pure malt whiskey.],[Fresh orange juice.] </td>\n",
    "    <td>Both about drinks but different drinks</td>\n",
    "    <td>0.50 </td>\n",
    "    <td>0.45 </td>\n",
    "    <td>0.97 </td>\n",
    "    <td>0.32 </td>\n",
    "    <td>0.65 </td> \n",
    "    <td>0.07 </td>\n",
    "      <td>0.77 </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>[It is a ferocious dog and it barks whenever anybody uknown comes near the house.],[The painting in the art gallery is so fantastic !] </td>\n",
    "    <td>Quite different </td>\n",
    "    <td>0.55 </td>\n",
    "    <td>0.47 </td>\n",
    "    <td>0.97 </td>\n",
    "    <td>0.24 </td>\n",
    "    <td>0.70 </td>\n",
    "    <td>0.07 </td>\n",
    "      <td>0.66 </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>[It is the family cow.],[I have been driving this sports car for last ten years and I am so satisfied!]  </td>\n",
    "   <td>Quite different </td>\n",
    "    <td>0.63 </td>\n",
    "    <td>0.64 </td>\n",
    "    <td>0.97 </td>\n",
    "    <td>0.35 </td>\n",
    "    <td>0.78 </td>\n",
    "    <td>0.13 </td>\n",
    "      <td>0.58 </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>[My driving license is my identity in USA as everything is linked to it.],[The president led us to war and we lost that war!]  </td>\n",
    "    <td>Quite different </td>\n",
    "    <td>0.61 </td>\n",
    "    <td>0.55 </td>\n",
    "    <td>0.97 </td>\n",
    "    <td>0.31 </td>\n",
    "    <td>0.88</td>\n",
    "    <td>0.08 </td>\n",
    "    <td>0.69 </td>  \n",
    "  </tr>\n",
    "    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Glove with top n similar words cannot be used . Similarly the routine for wordnet with synonyms has a flag that is set false as Wordnet can come up with a very large list of synonyms and can result in error in this function or produce very  bad similarity. What we have is pure WUP similarity \n",
    "\n",
    "2. First sentence pair is identical - so yielding 1\n",
    "\n",
    "3. Second sentence pair is quite close ( about good dish) - so BERT, Wordnet Cross similarity, Pure Wordnet  working fine \n",
    "\n",
    "4. Third sentence pair is same but one is affirmative and the other is question. So, predictably Glove avearge, Glove Average(POS) works fine as both sides are same. But BERT again is working good and Pure Wordnet WUP similarity is whereas Wordnet Cross Similarity is bad \n",
    "\n",
    "5. Fourth, fifth and sixth sentence pairs are kind of different and here it shows that because of enormity of possible synonyms, wordnet still finds some similarity. Average approach simply averages and it is not really good. But Glove cross similarity picks up the difference in meaning quite well \n",
    "\n",
    "How do we handle then similar and disparate pairs ? Looks like WUP similarity ( pure Wordnet approach) + BERT are good for similar sentences and Glove Cross similarity good for different sentences. So what may work out is 0.5(first method) + 0.5 (2nd method ) to balance it out ... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
