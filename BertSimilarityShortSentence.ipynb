{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6zoscximlAe7",
    "outputId": "a1a9a13f-69ff-421d-8e2d-2ed1cd39c6a2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install tensorflow-gpu==1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "colab_type": "code",
    "id": "6ZE5ur_e2sBg",
    "outputId": "3d406e5c-6d6c-40f7-9e9a-6cf7d0d3099e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "5DUcB8JqzgKn",
    "outputId": "4efd721d-25f6-4951-bb4d-d574f981bb7d"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import spacy.cli\n",
    "#spacy.cli.download(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uv-DrdgG4M2a"
   },
   "source": [
    "you don't have permission in Colab to load the model with normal spacy usage. \n",
    "Instead do the below steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pQ_Y42P34F-S"
   },
   "outputs": [],
   "source": [
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZnjkAsYrzgOS"
   },
   "outputs": [],
   "source": [
    "search_doc = nlp(\"The game was tense but it was a nail biting finish in cricket\")\n",
    "main_doc = nlp(\"The way Brazil played is fabulous. The scored each penalty and the freekick was a dream.\")\n",
    "\n",
    "main_doc_nouns = nlp(' '.join([str(t) for t in main_doc if t.pos_ in ['NOUN', 'PROPN']]))\n",
    "search_doc_nouns = nlp(' '.join([str(t) for t in search_doc if t.pos_ in ['NOUN', 'PROPN']]))\n",
    "\n",
    "print(main_doc_nouns)\n",
    "print(search_doc_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NZGaDc1y0enV",
    "outputId": "4f3eb420-0a49-4618-94f7-7d802256c0ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4793999618502406\n"
     ]
    }
   ],
   "source": [
    "# remove stop wrrds\n",
    "search_doc_no_stop_words = nlp(' '.join([str(t) for t in search_doc_nouns if not t.is_stop]))\n",
    "main_doc_no_stop_words = nlp(' '.join([str(t) for t in main_doc_nouns if not t.is_stop]))\n",
    "\n",
    "print(search_doc_no_stop_words.similarity(main_doc_no_stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j1Pld3Zv0yuO"
   },
   "source": [
    "Spacy's sentence embeddings are just the average of all word vector embeddings taken individually. So if you have a sentence with negating words like \"good\" and \"bad\" their vectors might cancel each other out resulting in not so good contextual embeddings. Anyway you should get rid of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow==1.15\n",
    "#run this in CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-serving-server in /Users/vcroopana/opt/anaconda3/lib/python3.7/site-packages (1.10.0)\n",
      "Requirement already satisfied: six in /Users/vcroopana/opt/anaconda3/lib/python3.7/site-packages (from bert-serving-server) (1.14.0)\n",
      "Requirement already satisfied: numpy in /Users/vcroopana/opt/anaconda3/lib/python3.7/site-packages (from bert-serving-server) (1.18.1)\n",
      "Requirement already satisfied: GPUtil>=1.3.0 in /Users/vcroopana/opt/anaconda3/lib/python3.7/site-packages (from bert-serving-server) (1.4.0)\n",
      "Requirement already satisfied: pyzmq>=17.1.0 in /Users/vcroopana/opt/anaconda3/lib/python3.7/site-packages (from bert-serving-server) (18.1.1)\n",
      "Requirement already satisfied: termcolor>=1.1 in /Users/vcroopana/opt/anaconda3/lib/python3.7/site-packages (from bert-serving-server) (1.1.0)\n",
      "Requirement already satisfied: bert-serving-client in /Users/vcroopana/opt/anaconda3/lib/python3.7/site-packages (1.10.0)\n",
      "Requirement already satisfied: numpy in /Users/vcroopana/opt/anaconda3/lib/python3.7/site-packages (from bert-serving-client) (1.18.1)\n",
      "Requirement already satisfied: pyzmq>=17.1.0 in /Users/vcroopana/opt/anaconda3/lib/python3.7/site-packages (from bert-serving-client) (18.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-serving-server  # server\n",
    "!pip install bert-serving-client  # client, independent of `bert-serving-server`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this in CLI\n",
    "# !bert-serving-start -model_dir /Users/vcroopana/bertmodels/multi_cased_L-12-H-768-A-12/ -num_worker=1 -cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b1muku4HUWgQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Nat9N5YRU_hb",
    "outputId": "dfa589d5-85ca-4b3d-8edb-fa6ab369cf81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 768)\n"
     ]
    }
   ],
   "source": [
    "embedding = bc.encode([\"The game was tense but it was a nail biting finish in cricket\"])\n",
    "# check the shape of embedding, it should be 1x768\n",
    "print(embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "U20ncOYroj72",
    "outputId": "8c6a3bd0-771e-4b66-b2ce-f61511ce27fc"
   },
   "outputs": [],
   "source": [
    "embedding1 = bc.encode([\"The way Brazil played is fabulous. The scored each penalty and the freekick was a dream.\"])\n",
    "# check the shape of embedding, it should be 1x768\n",
    "print(embedding.shape)\n",
    "print(embedding1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3UPG_SY5piuL"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_cosine_similarity(feature_vec_1, feature_vec_2):    \n",
    "    return cosine_similarity(feature_vec_1.reshape(1, -1), feature_vec_2.reshape(1, -1))[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yY7MbyD1pm0B",
    "outputId": "7f2754c6-69bc-4644-d570-8bafce2cfdb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9310798\n"
     ]
    }
   ],
   "source": [
    "print(get_cosine_similarity(embedding, embedding1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xjy4rBgjm7Fx"
   },
   "outputs": [],
   "source": [
    "#1. dataset - first three are similar and second three are dissimilar \n",
    "\n",
    "text1=[\"Trying to test how the method is for identical text.\",\n",
    "       \"I like that food.\",\n",
    "       \"Ram is very nice..\",\n",
    "      \"Pure malt whiskey.\",\n",
    "      \"It is a ferocious dog and it barks whenever anybody uknown comes near the house.\",\n",
    "      \"It is the family cow.\",\n",
    "      \"My driving license is my identity in USA as everything is linked to it.\"]\n",
    "text2=[\"Trying to test how the method is for identical text.\",\n",
    "       \"That dish is exciting\",\n",
    "       \"Is Ram very nice?\",\n",
    "       \"Fresh orange juice.\",\n",
    "       \"The painting in the art gallery is so fantastic !\",\n",
    "       \"I have been driving this sports car for last ten years and I am so satisfied! \",\n",
    "       \"The president led us to war and we lost that war! \"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5-DE-m48L19R"
   },
   "outputs": [],
   "source": [
    "# Let us define a function that calculates BERT similarity given a pair of text \n",
    "def calculate_BERT_similarity(text1, text2):\n",
    "  embedding1 = bc.encode([text1])\n",
    "  embedding2 = bc.encode([text2])\n",
    "  # use the function defined above\n",
    "  BERT_s=get_cosine_similarity(embedding1, embedding2)\n",
    "  return BERT_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nre_v9_tm7Uh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def text_pair_similarity_run_bert(text1,text2):\n",
    "#     print(text1)\n",
    "#     print (\"\\n\")\n",
    "#     print(text2)\n",
    "#     print (\"\\n\")\n",
    "    bert_sim = np.round(calculate_BERT_similarity(text1,text2),3)\n",
    "#     print(bert_sim)\n",
    "#     print(\"BERT Similarity : \"+str(bert_sim))\n",
    "#     print (\"=====================================\\n\")\n",
    "    return bert_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Ad Name</th>\n",
       "      <th>Product</th>\n",
       "      <th>Key Terms  Round 1</th>\n",
       "      <th>KeyTerms_Edited</th>\n",
       "      <th>Excitatory Potential</th>\n",
       "      <th>Emotional vs. Rational</th>\n",
       "      <th>Semantic Affinity</th>\n",
       "      <th>Valence</th>\n",
       "      <th>Keywords</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ad Number</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trailer</td>\n",
       "      <td>Fast &amp; Furious 9  Trailer</td>\n",
       "      <td>Movie Trailer</td>\n",
       "      <td>fast and the furious, fast &amp; the furious, fast...</td>\n",
       "      <td>fast_and_the_furious, fast_&amp;_the_furious, fast...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Trailer Fast &amp; Furious 9  Trailer fast and the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quibi</td>\n",
       "      <td>Quibi  Bank Heist</td>\n",
       "      <td>Video Platform</td>\n",
       "      <td>quibi, bank heist, robbery, less than ten minu...</td>\n",
       "      <td>quibi, bank_heist, robbery, less_than_ten_minu...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Quibi Quibi  Bank Heist quibi, bank heist, rob...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Brand Name                    Ad Name         Product  \\\n",
       "Ad Number                                                         \n",
       "1            Trailer  Fast & Furious 9  Trailer   Movie Trailer   \n",
       "2              Quibi          Quibi  Bank Heist  Video Platform   \n",
       "\n",
       "                                          Key Terms  Round 1  \\\n",
       "Ad Number                                                      \n",
       "1          fast and the furious, fast & the furious, fast...   \n",
       "2          quibi, bank heist, robbery, less than ten minu...   \n",
       "\n",
       "                                             KeyTerms_Edited  \\\n",
       "Ad Number                                                      \n",
       "1          fast_and_the_furious, fast_&_the_furious, fast...   \n",
       "2          quibi, bank_heist, robbery, less_than_ten_minu...   \n",
       "\n",
       "           Excitatory Potential  Emotional vs. Rational  Semantic Affinity  \\\n",
       "Ad Number                                                                    \n",
       "1                             1                       1                  2   \n",
       "2                             2                       1                  2   \n",
       "\n",
       "           Valence                                           Keywords  \n",
       "Ad Number                                                              \n",
       "1                1  Trailer Fast & Furious 9  Trailer fast and the...  \n",
       "2                2  Quibi Quibi  Bank Heist quibi, bank heist, rob...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "annotations_data = pd.read_csv('/Users/vcroopana/Downloads/summer2020/superbowl/ip/SB_ad_annotations.csv', index_col=0) \n",
    "annotations_data['Keywords'] = annotations_data['Brand Name']\\\n",
    "                                .str.cat(annotations_data['Ad Name'], sep=\" \")\\\n",
    "                                .str.cat(annotations_data['Key Terms  Round 1'], sep=\" \")\n",
    "df = annotations_data.drop_duplicates()\n",
    "print(df.shape)\n",
    "annotations_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ò</th>\n",
       "      <th>user_id</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>time_of_tweet</th>\n",
       "      <th>user_location</th>\n",
       "      <th>team followed</th>\n",
       "      <th>affective_state</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>ad_keywords</th>\n",
       "      <th>ad_mentioned</th>\n",
       "      <th>ad_manual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64628</td>\n",
       "      <td>2.836102e+09</td>\n",
       "      <td>1.220000e+18</td>\n",
       "      <td>Mon Feb 03 03:53:56 +0000 2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "      <td>1</td>\n",
       "      <td>Man I wanna look at the brightside \"we thought...</td>\n",
       "      <td>John legend, Chrissy Teigen, genesis, hyundai,...</td>\n",
       "      <td>genesis  going away party</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Ò       user_id      tweet_id                   time_of_tweet  \\\n",
       "0  64628  2.836102e+09  1.220000e+18  Mon Feb 03 03:53:56 +0000 2020   \n",
       "\n",
       "  user_location team followed  affective_state  \\\n",
       "0           NaN          both                1   \n",
       "\n",
       "                                          tweet_text  \\\n",
       "0  Man I wanna look at the brightside \"we thought...   \n",
       "\n",
       "                                         ad_keywords  \\\n",
       "0  John legend, Chrissy Teigen, genesis, hyundai,...   \n",
       "\n",
       "                ad_mentioned ad_manual  \n",
       "0  genesis  going away party      none  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "man_ann_data = pd.read_csv(r'/Users/vcroopana/Downloads/summer2020/superbowl/mann_ann_sb.csv')    \n",
    "man_ann_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "cadillacabc joinrt 31k james_bond\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "print(len(stop))\n",
    "\n",
    "def removeMentions(text):\n",
    "\n",
    "    textBeforeMention = text.partition(\"@\")[0]\n",
    "    textAfterMention = text.partition(\"@\")[2]\n",
    "    textAfterMention =  re.sub(r':', '', textAfterMention) #cadillac join the 31k\n",
    "    tHandle = textAfterMention.partition(\" \")[0].lower() #cadillac    \n",
    "    text = textBeforeMention+ \" \" + textAfterMention  \n",
    "    return text\n",
    "\n",
    "def cleanTweet(strinp):\n",
    "    strinp = re.sub(r'RT', \"\", strinp) # Remove RT\n",
    "    strinp = strinp.lower()\n",
    "    \n",
    "    stop_removed_list = [word for word in strinp.split() if word not in (stop)]\n",
    "    stop_removed = ' '.join([str(elem) for elem in stop_removed_list])    \n",
    "    text = re.sub('https?://[A-Za-z0-9./]+', ' ', stop_removed) # Remove URLs\n",
    "    text = removeMentions(text)\n",
    "    text = re.sub('[^\\x00-\\x7F]+', ' ', text) # Remove non-ASCII chars.\n",
    "    \n",
    "    # remove punctuations except '_'\n",
    "    punctuation = ['(', ')', '[',']','?', ':', ':', ',', '.', '!', '/', '\"', \"'\", '@', '#', '&']\n",
    "#     text = re.sub('[^a-zA-Z]', ' ', text) # remove all other than alphabet chars \n",
    "    text = \"\".join((char for char in text if char not in punctuation))\n",
    "    \n",
    "#     text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text) # remove all single characters     \n",
    "    stop_removed_l = [word for word in text.split() if word not in (stop)]\n",
    "    stop_removed = ' '.join([str(elem) for elem in stop_removed_l]) \n",
    "    return stop_removed\n",
    "\n",
    "print(cleanTweet(\"RT @cadillacabc: Joinrt the 31K james_bond\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings calculated\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ad_keywords_clean = annotations_data['Keywords'].apply(lambda ad: cleanTweet(ad))\n",
    "man_ann_data['tweet_clean'] = man_ann_data['tweet_text'].apply(lambda twt: cleanTweet(twt))\n",
    "embedding_tweets = man_ann_data['tweet_clean'].apply(lambda x: bc.encode([x]))\n",
    "print(\"Embeddings calculated\")\n",
    "bert_sim_df = pd.DataFrame(columns = annotations_data['Ad Name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert Similarities Calculated\n"
     ]
    }
   ],
   "source": [
    "ad_id = 1\n",
    "for ad in ad_keywords_clean:\n",
    "    embedding_ad = bc.encode([ad])\n",
    "    bert_sim_col =[]\n",
    "    print(\"computin sim for ad:\"+ str(ad_id))\n",
    "    for i in range(0, len(embedding_tweets)): \n",
    "        bert_sim = get_cosine_similarity(embedding_ad, embedding_tweets[i])\n",
    "        bert_sim_col.append(np.round(bert_sim,3))\n",
    "    \n",
    "    bert_sim_df[annotations_data['Ad Name'][ad_id]] = bert_sim_col\n",
    "    ad_id = ad_id + 1\n",
    "bert_sim_df.head(3)\n",
    "print(\"Bert Similarities Calculated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top n similar ads computed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vcroopana/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Support for multi-dimensional indexing (e.g. `index[:, None]`) on an Index is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "nlargest = 5\n",
    "order = np.argsort(-bert_sim_df.values, axis=1)[:, :nlargest]\n",
    "result = pd.DataFrame(bert_sim_df.columns[order], \n",
    "                      columns=['top{}_ad'.format(i) for i in range(1, nlargest+1)],\n",
    "                      index= bert_sim_df.index)\n",
    "\n",
    "order_vals = np.sort(-bert_sim_df.values, axis=1)[:, :nlargest]\n",
    "result_vals = pd.DataFrame(-order_vals,\n",
    "                          columns = ['top{}'.format(i) for i in range(1, nlargest+1)],\n",
    "                          index= bert_sim_df.index)\n",
    "\n",
    "result_con = pd.concat([result_vals, result], axis =1)\n",
    "man_ann_data_merged = pd.concat([man_ann_data, result_con], axis =1)\n",
    "\n",
    "man_ann_data_merged.head()\n",
    "\n",
    "print(\"Top n similar ads computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tp:103 n_fp:2396 n_fn:0 n_tn:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.041216486594637856, 1.0, 0.07916986933128363)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_conf_matrix(ad_manual, ad_algo, ad_algo_2, ad_prob, ad_prob2, thresh_prob):\n",
    "    res = \"\"\n",
    "    ad_manual = ad_manual.lower()\n",
    "    ad_algo = ad_algo.lower()\n",
    "    ad_algo_2 = ad_algo_2.lower()\n",
    "    if(ad_manual == ad_algo and ad_prob >= thresh_prob):\n",
    "        res = 'TP'\n",
    "    elif(ad_manual == ad_algo and ad_prob < thresh_prob):\n",
    "        res = 'FN'\n",
    "    elif(ad_manual=='none' and ad_prob > thresh_prob):\n",
    "        res = 'FP'\n",
    "    elif(ad_manual=='none' and ad_prob < thresh_prob):\n",
    "        res = 'TN'\n",
    "    elif(ad_manual!='none' and ad_manual!= ad_algo and ad_prob >= thresh_prob):\n",
    "        res = 'FP'\n",
    "    elif(ad_manual!='none' and ad_manual!= ad_algo and ad_prob < thresh_prob):\n",
    "        res = 'FN'\n",
    "    return res\n",
    "def get_conf_matrix_2(ad_manual, ad_algo, ad_algo_2, ad_algo_3, ad_algo_4, ad_algo_5):\n",
    "    res = \"\"\n",
    "    ad_manual = ad_manual.lower()\n",
    "    ad_algo = ad_algo.lower()\n",
    "    ad_algo_2 = ad_algo_2.lower()\n",
    "    ad_algo_3 = ad_algo_3.lower()\n",
    "    ad_algo_4 = ad_algo_4.lower()\n",
    "    ad_algo_5 = ad_algo_5.lower()\n",
    "    \n",
    "    if(ad_manual == ad_algo or ad_manual == ad_algo_2 or ad_manual == ad_algo_3 or ad_manual == ad_algo_4\n",
    "      or ad_manual == ad_algo_5):\n",
    "        res = 'TP'\n",
    "    elif(ad_manual=='none'):\n",
    "        res = 'FP'\n",
    "    elif(ad_manual!='none' and ad_manual!= ad_algo and ad_manual!= ad_algo_2 and ad_manual!= ad_algo_3\n",
    "        and ad_manual!= ad_algo_4 and ad_manual!= ad_algo_5):\n",
    "        res = 'FN'\n",
    "    elif(ad_manual!='none' and ad_manual!= ad_algo and ad_manual!= ad_algo_2 and ad_manual!= ad_algo_3\n",
    "        and ad_manual!= ad_algo_4 and ad_manual!= ad_algo_5):\n",
    "        res = 'TN'\n",
    "    return res\n",
    "\n",
    "def computeAccuracy(result):    \n",
    "    n_tp = result[result['conf_matrix'] == 'TP'].shape[0]\n",
    "    n_fp = result[result['conf_matrix'] == 'FP'].shape[0]\n",
    "    n_fn = result[result['conf_matrix'] == 'FN'].shape[0]\n",
    "    n_tn = result[result['conf_matrix'] == 'TN'].shape[0]\n",
    "    \n",
    "    print(\"n_tp:\" + str(n_tp)+ \" n_fp:\" + str(n_fp)+ \" n_fn:\" + str(n_fn) + \" n_tn:\" + str(n_tn))\n",
    "\n",
    "    precision = n_tp/(n_tp+ n_fp)\n",
    "    recall = n_tp/(n_tp+ n_fn)\n",
    "    f_measure = (2*precision*recall)/ (precision+recall)\n",
    "\n",
    "    return precision, recall, f_measure\n",
    "\n",
    "man_ann_data_merged['conf_matrix'] = man_ann_data_merged.apply(lambda x: get_conf_matrix(x['ad_manual'], x['top1_ad'], \n",
    "                                                          x['top2_ad'], x['top1'], x['top2'], 0.8), axis =1)\n",
    "# man_ann_data_merged['conf_matrix'] = man_ann_data_merged.apply(lambda x: get_conf_matrix_2(x['ad_manual'], x['top1_ad'], \n",
    "#                             x['top2_ad'], x['top3_ad'], x['top4_ad'], x['top5_ad']), axis =1)\n",
    "computeAccuracy(man_ann_data_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_ann_data_merged.to_csv(\"/Users/vcroopana/Downloads/summer2020/superbowl/sim_bert_0.8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "man_ann_data_merged = pd.read_csv(\"/Users/vcroopana/Downloads/summer2020/superbowl/sim_bert.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "N6xQ6y39m7Yn",
    "outputId": "27c5fc43-efa3-4a44-a5d4-7f4848d2850a"
   },
   "outputs": [],
   "source": [
    "for i in range(0,7):\n",
    "    text_pair_similarity_run_bert(text1[i],text2[i])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e_mj5CPwm7ce"
   },
   "outputs": [],
   "source": [
    "harphampeg lynnelanae wgravelee darngoood franklin_graham pepsi idea feel female given\n",
    "proctor gamble procter gamble come together procter gamble come together sofia vergara chili bounty old spice head shoulders olay charming febreze\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "BertSimilarityShortSentence.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
