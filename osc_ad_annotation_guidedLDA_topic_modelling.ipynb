{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Advertiser</th>\n",
       "      <th>location</th>\n",
       "      <th>Ad</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Twitter_handle</th>\n",
       "      <th>advtsr_loc</th>\n",
       "      <th>names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cadillac_Escalade</td>\n",
       "      <td>(00:02:44)</td>\n",
       "      <td>Escalade</td>\n",
       "      <td>Escalade Cadillac, driving, red carpet, escala...</td>\n",
       "      <td>cadillac</td>\n",
       "      <td>Cadillac_Escalade_(00:02:44)</td>\n",
       "      <td>Escalade cadillac Cadillac_Escalade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kohl's</td>\n",
       "      <td></td>\n",
       "      <td>Sale</td>\n",
       "      <td>Sale kohl's, home sale, kohl's cash, kohls.com...</td>\n",
       "      <td>kohls</td>\n",
       "      <td>Kohl's_</td>\n",
       "      <td>Sale kohls Kohl's</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Advertiser    location        Ad  \\\n",
       "0  Cadillac_Escalade  (00:02:44)  Escalade   \n",
       "1             Kohl's                  Sale   \n",
       "\n",
       "                                            Keywords Twitter_handle  \\\n",
       "0  Escalade Cadillac, driving, red carpet, escala...       cadillac   \n",
       "1  Sale kohl's, home sale, kohl's cash, kohls.com...          kohls   \n",
       "\n",
       "                     advtsr_loc                                names  \n",
       "0  Cadillac_Escalade_(00:02:44)  Escalade cadillac Cadillac_Escalade  \n",
       "1                       Kohl's_                    Sale kohls Kohl's  "
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_data = pd.read_csv(r'/Users/vcroopana/Downloads/summer2020/oscar_ads/ip/ad_annotations_oscars_national_edited_1.csv') \n",
    "annotations_data = annotations_data[['Advertiser', 'location', 'Ad', 'Keywords', 'Twitter_handle']]\n",
    "annotations_data['Keywords'] = annotations_data['Ad'].str.cat(annotations_data['Keywords'], sep=\" \")\\\n",
    "                                .str.cat(annotations_data['Twitter_handle'], sep=\" \")\\\n",
    "                                .str.cat(annotations_data['Advertiser'], sep=\" \")# concat the Ad, twitter handle and keywords fields \n",
    "\n",
    "annotations_data = annotations_data.fillna(\"\") #location column has NaN values\n",
    "annotations_data['advtsr_loc'] = annotations_data['Advertiser'].str.cat(annotations_data['location'], sep=\"_\")\n",
    "\n",
    "twitter_handles = annotations_data['Twitter_handle'].values.tolist()\n",
    "annotations_data['names'] = annotations_data['Ad'].str.cat(annotations_data['Twitter_handle'], sep=\" \")\\\n",
    "                                .str.cat(annotations_data['Advertiser'], sep=\" \")\n",
    "annotations_data.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "191\n"
     ]
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "print(len(stop))\n",
    "stop.extend([\"best\", \"award\", \"love\", \"movie\", \"picture\", \"words\", \"actor\", \"actress\", \"red\", \"carpet\", \"oscar\", \"oscars\"])\n",
    "print(len(stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cadillacabc joinrt k\n"
     ]
    }
   ],
   "source": [
    "#clean tweets\n",
    "import re\n",
    "def removeMentions(text):\n",
    "\n",
    "    textBeforeMention = text.partition(\"@\")[0]\n",
    "    textAfterMention = text.partition(\"@\")[2]\n",
    "    textAfterMention =  re.sub(r':', '', textAfterMention) #cadillac join the 31k\n",
    "    tHandle = textAfterMention.partition(\" \")[0].lower() #cadillac\n",
    "#     if tHandle not in twitter_handles:\n",
    "#         text = re.sub(r'@\\w+', '', text) \n",
    "#     else:\n",
    "#         text = textBeforeMention+ \" \" + textAfterMention     \n",
    "    text = textBeforeMention+ \" \" + textAfterMention  \n",
    "    return text\n",
    "    \n",
    "#TODO issue should've getting converted to shouldve couldn' t -> couldn\n",
    "def cleanTweet(strinp):\n",
    "    strinp = re.sub(r'RT', \"\", strinp) # Remove RT\n",
    "    strinp = strinp.lower()\n",
    "    \n",
    "    stop_removed_list = [word for word in strinp.split() if word not in (stop)]\n",
    "    stop_removed = ' '.join([str(elem) for elem in stop_removed_list])        \n",
    "    text = re.sub('https?://[A-Za-z0-9./]+', ' ', stop_removed) # Remove URLs\n",
    "    text = removeMentions(text)\n",
    "    text = re.sub('[^\\x00-\\x7F]+', ' ', text) # Remove non-ASCII chars.\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text) # remove all other than alphabet chars \n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text) # remove all single characters     \n",
    "    stop_removed_l = [word for word in text.split() if word not in (stop)]\n",
    "    stop_removed = ' '.join([str(elem) for elem in stop_removed_l]) \n",
    "    return stop_removed\n",
    "\n",
    "print(cleanTweet(\"RT @cadillacabc: Joinrt the 31K\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## GuidedLDA #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/\n",
    "<br /> topic modelling Vs classificaton: https://monkeylearn.com/topic-analysis/#papers\n",
    "<br /> https://www.tdktech.com/tech-talks/topic-modeling-explained-lda-to-bayesian-inference\n",
    "<br />shorttext - https://shorttext.readthedocs.io/en/latest/tutorial_topic.html\n",
    "<br />naive Bayes - https://monkeylearn.com/topic-analysis/\n",
    "<br />guidedlda - https://guidedlda.readthedocs.io/en/latest/\n",
    "<br />guided LDA installation: https://github.com/vi3k6i5/GuidedLDA/issues/26\n",
    "<br /> guide LDA explanation: https://www.freecodecamp.org/news/how-we-changed-unsupervised-lda-to-semi-supervised-guidedlda-e36a95f3a164/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal LDA without seeding\n",
    "# model = guidedlda.GuidedLDA(n_topics=14, n_iter=100, random_state=7, refresh=20)\n",
    "# model.fit(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topic_list = [\n",
    "    ['google', 'loretta'],\n",
    "    ['verizon'],\n",
    "    ['apple', 'iphone'],\n",
    "    ['postmates'],\n",
    "    ['turbotax', 'tax', 'intuit'],\n",
    "    ['disney', 'onward', 'mulan', 'memories'],\n",
    "    ['natalie', 'portman', 'dior'],\n",
    "    ['cadillac', 'escalade', 'regina', 'king'],\n",
    "    ['national', 'geographic'],\n",
    "    ['fires', 'hulu', 'little', 'everywhere', 'witherspoon','fx'],\n",
    "    ['sale',  'kohl', 'kohls'],\n",
    "    ['peroni',  'beer', 'birra', 'italia'],\n",
    "    ['dry', 'canadadry', 'canada'],\n",
    "    ['galaxy', 'fold', 'samsung', 'phone', 'flip', 'samsungmobileus']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topic_list = [\n",
    "    ['cadillac', 'king', 'driving', 'regina', 'makeyourway', 'make', 'escalade', 'way'],\n",
    "['sale', 'home', 'kohl', 'cash', 'com', 'kohls'],\n",
    "['irrisiste', 'bowls', 'applebees', 'good', 'eatin', 'applebee'],\n",
    "['fixar', 'disney', 'onward', 'march'],\n",
    "['dry', 'canadadry', 'canada', 'ale', 'real', 'task', 'ginger', 'relax', 'relaxation'],\n",
    "['necklace', 'macy', 'give', 'macys', 'gifts'],\n",
    "['marvel', 'studio', 'blackwidow', 'black', 'marvelstudios', 'widow'],\n",
    "['peroni', 'beautifully', 'peroniusa', 'motorcycle', 'beer', 'birra', 'italia'],\n",
    "['sale', 'kohl', 'cash', 'com', 'kohls', 'valentine', 'jewerly'],\n",
    "['network', 'verizon', 'reliable', 'dependable', 'america', 'proven', 'consistnet'],\n",
    "[ 'gameshow', 'johnsonville', 'sausage', 'way', 'make', 'smoked', 'retro'],\n",
    "['success', 'cadillac', 'regina', 'king', 'make', 'escalade', 'makeyourway', 'way'],\n",
    "['martin', 'scorsese', 'kathryn', 'alejandro', 'james', 'bigelow', 'directors', 'perpetual', 'rolex', 'cameron', 'org', 'irritu'],\n",
    "['say', 'host', 'congrats', 'anything', 'messages', 'mmschocolate', 'help', 'let'],\n",
    "['galaxy', 'fold', 'samsung', 'foldable', 'phone', 'flip', 'samsungmobileus', 'future', 'change', 'comic', 'strip', 'shape'],\n",
    "['slow', 'burgers', 'fast', 'postmates', 'postmate', 'subway', 'make', 'floating', 'think'],\n",
    "['room', 'verizon', 'video', 'er', 'emergency', 'chat', 'unlimited', 'family'],\n",
    "['us', 'monster', 'lion', 'frozen', 'movies', 'memories', 'bring', 'inc', 'king', 'together', 'disney'],\n",
    "['tax', 'people', 'intuit', 'turbotax'],\n",
    "['mmschocolate', 'animated', 'characters', 'blue'],\n",
    "['fires', 'hulu', 'washington', 'little', 'reese', 'everywhere', 'witherspoon', 'kerry'],\n",
    "['companion', 'cool', 'american', 'certificate', 'delta', 'express', 'americanexpress', 'grafiti', 'status', 'amex'],\n",
    "['com', 'farm', 'chsinc', 'chs', 'cooperativeownership'],\n",
    "['homosexual', 'network', 'verizon', 'people', 'chat', 'group', 'homosexuality', 'rely', 'family'],\n",
    "['cheese', 'melt', 'beef', 'perfecter', 'made', 'mcdonald', 'pounder', 'fresh', 'perfect', 'quarter', 'mcdonalds', 'sesame'],\n",
    "['ct', 'nobody', 'cadillac', 'breaking', 'barriers', 'speak', 'make', 'makeyourway', 'glass', 'escalade', 'way'],\n",
    "['quibi', 'stores', 'president', 'quick', 'big', 'com', 'less', 'minutes', 'bites'],\n",
    "[ 'chandelier', 'miss', 'rose', 'dior', 'roses'],\n",
    "['apple', 'verizon', 'iphone', 'mode'],\n",
    "['park', 'maps', 'jurassic', 'story', 'goonies', 'christmas', 'house', 'google', 'beach'],\n",
    "['cadillac', 'king', 'driving', 'regina', 'makeyourway', 'make', 'escalade', 'way'],\n",
    "['mama', 'congrats', 'support', 'emotional', 'proud', 'messages', 'mmschocolate'],\n",
    "['indeed', 'walked', 'moon', 'spaceship', 'great', 'astraunaut', 'men', 'moments'],\n",
    "['fires', 'hulu', 'washington', 'littlefireshulu', 'little', 'reese', 'everywhere', 'witherspoon', 'kerry'],\n",
    "['women', 'hulu', 'rights', 'speech', 'fxonhulu', 'mrsam', 'america', 'mrs', 'fx'],\n",
    "['cheese', 'melt', 'hulu', 'beef', 'perfecter', 'made', 'mcdonald', 'pounder', 'fresh', 'perfect', 'quarter', 'mcdonalds', 'sesame'],\n",
    "['discover', 'credit', 'fees', 'annual', 'card', 'fee'],\n",
    "['railroad', 'quibi', 'com', 'ten', 'episode', 'dessert', 'minutes', 'train'],\n",
    "['medicine', 'hiv', 'biktarvy', 'living', 'treatment'],\n",
    "['discover', 'credit', 'accepted', 'card'],\n",
    "['mulan', 'china', 'liu', 'yifei', 'chinese', 'disney', 'warrier'],\n",
    "['bar', 'kinderus', 'chocolate', 'bueno', 'kinder'],\n",
    "['harris', 'maurice', 'flowers', 'surface', 'microsoft'],\n",
    "['quibi', 'mobile', 'female', 'com', 'phone', 'astronaut'],\n",
    "['xfi', 'security', 'awesome', 'easy', 'simple', 'xfinity'],\n",
    "['martin', 'scorsese', 'kathryn', 'alejandro', 'james', 'bigelow', 'directors', 'perpetual', 'rolex', 'cameron', 'org', 'irritu'],\n",
    "[ 'follow', 'messages', 'favorite', 'mmschocolate'],\n",
    "['eno', 'wallet', 'capital', 'credit', 'one', 'capitalone', 'highway', 'card'],\n",
    "['times', 'nyt', 'nytimes', 'slavery', 'worth', 'beach', 'york', 'truth', 'new'],\n",
    "['finishline', 'cadilac', 'ct', 'nobody', 'running', 'cadillac', 'woman', 'speak'],\n",
    "['apple', 'verizon', 'iphone', 'mode'],\n",
    "['quibi', 'mobile', 'leave', 'com', 'phone', 'minutes', 'zombie'],\n",
    "['groups', 'facebook', 'rock', 'group', 'want', 'together'],\n",
    "['cynthia', 'natgeo', 'erivo', 'national', 'geographic', 'genius', 'aretha'],\n",
    "['xcel', 'us', 'energy', 'home', 'xcelenergy', 'smart', 'free', 'together', 'carbon'],\n",
    "['ct', 'nobody', 'cadillac', 'breaking', 'barriers', 'speak', 'make', 'makeyourway', 'glass', 'escalade', 'way'],\n",
    "['years', 'optic', 'party', 'colgate', 'dance', 'white', 'toothpaste'],\n",
    "['norweigian', 'line', 'ncl', 'free', 'cruisenorwegian', 'cruise', 'feel'],\n",
    "['kellog', 'delight', 'specialk', 'oatmeal', 'special', 'strawberry', 'vanila', 'almond', 'fake', 'chocolatey', 'berries', 'delicious'],\n",
    "['verizon', 'video', 'connection', 'men', 'angola', 'chat', 'family'],\n",
    "['selfie', 'say', 'thanks', 'messages', 'mmschocolate', 'let'],\n",
    "['fx', 'hulu'],\n",
    "['abc', 'american', 'idol', 'singin', 'season', 'gma', 'americanidol', 'new', 'bus'],\n",
    "['burger', 'fries', 'eatatperkins', 'pie', 'perkins', 'meal', 'dollars', 'bakery', 'restaurant'],\n",
    "['cancer', 'verizon', 'dad', 'fight'],\n",
    "['loretta', 'google', 'old'],\n",
    "['invite', 'work', 'messages', 'pretending', 'mmschocolate'],\n",
    "['galaxy', 'fold', 'samsung', 'foldable', 'phone', 'flip', 'samsungmobileus', 'future', 'change', 'comic', 'strip', 'shape'],\n",
    "['quibi', 'sand', 'quick', 'explorer', 'com', 'minutes', 'swamp'],\n",
    "['corona', 'coronaextrausa', 'enjoy', 'view', 'premier', 'lower', 'calories', 'carb'],\n",
    "['reliable', 'verizon', 'consistnet'],  \n",
    "['martin', 'scorsese', 'kathryn', 'alejandro', 'james', 'bigelow', 'directors', 'perpetual', 'rolex', 'cameron', 'org', 'irritu'],\n",
    "['flower', 'polar', 'adobe', 'creative', 'paradise', 'suite', 'creativity', 'pride', 'bear', 'astronaut']\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topic_list = [\n",
    "    ['commercial', 'cadillac', 'king', 'driving', 'regina', 'makeyourway', 'make', 'escalade', 'way'],\n",
    "['commercial', 'sale', 'home', 'kohl', 'cash', 'com', 'kohls'],\n",
    "['commercial', 'irrisiste', 'bowls', 'applebees', 'good', 'eatin', 'applebee'],\n",
    "['commercial', 'fixar', 'disney', 'onward', 'march'],\n",
    "['commercial', 'dry', 'canadadry', 'canada', 'ale', 'real', 'task', 'ginger', 'relax', 'relaxation'],\n",
    "['commercial', 'necklace', 'macy', 'give', 'macys', 'gifts'],\n",
    "['commercial', 'marvel', 'studio', 'blackwidow', 'black', 'marvelstudios', 'widow'],\n",
    "['commercial', 'peroni', 'beautifully', 'peroniusa', 'motorcycle', 'beer', 'birra', 'italia'],\n",
    "['commercial', 'sale', 'kohl', 'cash', 'com', 'kohls', 'valentine', 'jewerly'],\n",
    "['commercial', 'network', 'verizon', 'reliable', 'dependable', 'america', 'proven', 'consistnet'],\n",
    "['commercial',  'gameshow', 'johnsonville', 'sausage', 'way', 'make', 'smoked', 'retro'],\n",
    "['commercial', 'success', 'cadillac', 'regina', 'king', 'make', 'escalade', 'makeyourway', 'way'],\n",
    "['commercial', 'martin', 'scorsese', 'kathryn', 'alejandro', 'james', 'bigelow', 'directors', 'perpetual', 'rolex', 'cameron', 'org', 'irritu'],\n",
    "['commercial', 'say', 'host', 'congrats', 'anything', 'messages', 'mmschocolate', 'help', 'let'],\n",
    "['commercial', 'galaxy', 'fold', 'samsung', 'foldable', 'phone', 'flip', 'samsungmobileus', 'future', 'change', 'comic', 'strip', 'shape'],\n",
    "['commercial', 'slow', 'burgers', 'fast', 'postmates', 'postmate', 'subway', 'make', 'floating', 'think'],\n",
    "['commercial', 'room', 'verizon', 'video', 'er', 'emergency', 'chat', 'unlimited', 'family'],\n",
    "['commercial', 'us', 'monster', 'lion', 'frozen', 'movies', 'memories', 'bring', 'inc', 'king', 'together', 'disney'],\n",
    "['commercial', 'tax', 'people', 'intuit', 'turbotax'],\n",
    "['commercial', 'mmschocolate', 'animated', 'characters', 'blue'],\n",
    "['commercial', 'fires', 'hulu', 'washington', 'little', 'reese', 'everywhere', 'witherspoon', 'kerry'],\n",
    "['commercial', 'companion', 'cool', 'american', 'certificate', 'delta', 'express', 'americanexpress', 'grafiti', 'status', 'amex'],\n",
    "# ['commercial', 'com', 'farm', 'chsinc', 'chs', 'cooperativeownership'],\n",
    "['commercial', 'homosexual', 'network', 'verizon', 'people', 'chat', 'group', 'homosexuality', 'rely', 'family'],\n",
    "['commercial', 'cheese', 'melt', 'beef', 'perfecter', 'made', 'mcdonald', 'pounder', 'fresh', 'perfect', 'quarter', 'mcdonalds', 'sesame'],\n",
    "['commercial', 'ct', 'nobody', 'cadillac', 'breaking', 'barriers', 'speak', 'make', 'makeyourway', 'glass', 'escalade', 'way'],\n",
    "['commercial', 'quibi', 'stores', 'president', 'quick', 'big', 'com', 'less', 'minutes', 'bites'],\n",
    "['commercial',  'chandelier', 'miss', 'rose', 'dior', 'roses'],\n",
    "['commercial', 'apple', 'verizon', 'iphone', 'mode'],\n",
    "# ['commercial', 'park', 'maps', 'jurassic', 'story', 'goonies', 'christmas', 'house', 'google', 'beach'],\n",
    "['commercial', 'cadillac', 'king', 'driving', 'regina', 'makeyourway', 'make', 'escalade', 'way'],\n",
    "# ['commercial', 'mama', 'congrats', 'support', 'emotional', 'proud', 'messages', 'mmschocolate'],\n",
    "['commercial', 'indeed', 'walked', 'moon', 'spaceship', 'great', 'astraunaut', 'men', 'moments'],\n",
    "['commercial', 'fires', 'hulu', 'washington', 'littlefireshulu', 'little', 'reese', 'everywhere', 'witherspoon', 'kerry'],\n",
    "# ['commercial', 'women', 'hulu', 'rights', 'speech', 'fxonhulu', 'mrsam', 'america', 'mrs', 'fx'],\n",
    "['commercial', 'cheese', 'melt', 'hulu', 'beef', 'perfecter', 'made', 'mcdonald', 'pounder', 'fresh', 'perfect', 'quarter', 'mcdonalds', 'sesame'],\n",
    "['commercial', 'discover', 'credit', 'fees', 'annual', 'card', 'fee'],\n",
    "['commercial', 'railroad', 'quibi', 'com', 'ten', 'episode', 'dessert', 'minutes', 'train'],\n",
    "['commercial', 'medicine', 'hiv', 'biktarvy', 'living', 'treatment'],\n",
    "['commercial', 'discover', 'credit', 'accepted', 'card'],\n",
    "['commercial', 'mulan', 'china', 'liu', 'yifei', 'chinese', 'disney', 'warrier'],\n",
    "['commercial', 'bar', 'kinderus', 'chocolate', 'bueno', 'kinder'],\n",
    "['commercial', 'harris', 'maurice', 'flowers', 'surface', 'microsoft'],\n",
    "['commercial', 'quibi', 'mobile', 'female', 'com', 'phone', 'astronaut'],\n",
    "['commercial', 'xfi', 'security', 'awesome', 'easy', 'simple', 'xfinity'],\n",
    "# ['commercial', 'martin', 'scorsese', 'kathryn', 'alejandro', 'james', 'bigelow', 'directors', 'perpetual', 'rolex', 'cameron', 'org', 'irritu'],\n",
    "['commercial',  'follow', 'messages', 'favorite', 'mmschocolate'],\n",
    "['commercial', 'eno', 'wallet', 'capital', 'credit', 'one', 'capitalone', 'highway', 'card'],\n",
    "['commercial', 'times', 'nyt', 'nytimes', 'slavery', 'worth', 'beach', 'york', 'truth', 'new'],\n",
    "# ['commercial','finishline', 'cadilac', 'ct', 'nobody', 'running', 'cadillac', 'woman', 'speak'],\n",
    "['commercial', 'apple', 'verizon', 'iphone', 'mode'],\n",
    "['commercial', 'quibi', 'mobile', 'leave', 'com', 'phone', 'minutes', 'zombie'],\n",
    "['commercial', 'groups', 'facebook', 'rock', 'group', 'want', 'together'],\n",
    "['commercial', 'cynthia', 'natgeo', 'erivo', 'national', 'geographic', 'genius', 'aretha'],\n",
    "['commercial', 'xcel', 'us', 'energy', 'home', 'xcelenergy', 'smart', 'free', 'together', 'carbon'],\n",
    "['commercial', 'ct', 'nobody', 'cadillac', 'breaking', 'barriers', 'speak', 'make', 'makeyourway', 'glass', 'escalade', 'way'],\n",
    "['commercial', 'years', 'optic', 'party', 'colgate', 'dance', 'white', 'toothpaste'],\n",
    "['commercial', 'norweigian', 'line', 'ncl', 'free', 'cruisenorwegian', 'cruise', 'feel'],\n",
    "['commercial', 'kellog', 'delight', 'specialk', 'oatmeal', 'special', 'strawberry', 'vanila', 'almond', 'fake', 'chocolatey', 'berries', 'delicious'],\n",
    "['commercial', 'verizon', 'video', 'connection', 'men', 'angola', 'chat', 'family'],\n",
    "['commercial', 'selfie', 'say', 'thanks', 'messages', 'mmschocolate', 'let'],\n",
    "['commercial', 'fx', 'hulu'],\n",
    "['commercial', 'abc', 'american', 'idol', 'singin', 'season', 'gma', 'americanidol', 'new', 'bus'],\n",
    "['commercial', 'burger', 'fries', 'eatatperkins', 'pie', 'perkins', 'meal', 'dollars', 'bakery', 'restaurant'],\n",
    "['commercial', 'cancer', 'verizon', 'dad', 'fight'],\n",
    "['commercial', 'loretta', 'google', 'old'],\n",
    "['commercial', 'invite', 'work', 'messages', 'pretending', 'mmschocolate'],\n",
    "['commercial', 'galaxy', 'fold', 'samsung', 'foldable', 'phone', 'flip', 'samsungmobileus', 'future', 'change', 'comic', 'strip', 'shape'],\n",
    "['commercial', 'quibi', 'sand', 'quick', 'explorer', 'com', 'minutes', 'swamp'],\n",
    "['commercial', 'corona', 'coronaextrausa', 'enjoy', 'view', 'premier', 'lower', 'calories', 'carb'],\n",
    "# ['commercial', 'reliable', 'verizon', 'consistnet'],  \n",
    "['commercial', 'martin', 'scorsese', 'kathryn', 'alejandro', 'james', 'bigelow', 'directors', 'perpetual', 'rolex', 'cameron', 'org', 'irritu'],\n",
    "['commercial', 'flower', 'polar', 'adobe', 'creative', 'paradise', 'suite', 'creativity', 'pride', 'bear', 'astronaut']\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topic_list = [\n",
    "    ['commercial', 'cadillac', 'escalade'],\n",
    "#     ['commercial', 'success', 'cadillac', 'regina', 'king', 'make', 'escalade', 'makeyourway', 'way'],\n",
    "# ['commercial', 'ct', 'nobody', 'cadillac', 'breaking', 'barriers', 'speak', 'make', 'makeyourway', 'glass', 'escalade', 'way'],\n",
    "# ['commercial', 'cadillac', 'king', 'driving', 'regina', 'makeyourway', 'make', 'escalade', 'way'],\n",
    "# ['commercial', 'ct', 'nobody', 'cadillac', 'breaking', 'barriers', 'speak', 'make', 'makeyourway', 'glass', 'escalade', 'way'],\n",
    "\n",
    "['commercial', 'sale', 'kohl','kohls'],\n",
    "# ['commercial', 'sale', 'kohl', 'cash', 'com', 'kohls', 'valentine', 'jewerly'],\n",
    "\n",
    "['commercial',  'applebees',  'applebee'],\n",
    "['commercial', 'fixar', 'disney', 'onward'],\n",
    "['commercial', 'dry', 'canadadry', 'canada', 'ale', 'ginger', 'relax', 'relaxation'],\n",
    "['commercial', 'necklace', 'macy','macys', 'gifts'],\n",
    "['commercial', 'marvel', 'studio', 'blackwidow', 'black', 'marvelstudios', 'widow'],\n",
    "['commercial', 'peroni','peroniusa', 'motorcycle', 'beer', 'birra', 'italia'],\n",
    "['commercial', 'network', 'verizon', 'reliable', 'dependable', 'america', 'proven', 'consistent'],\n",
    "# ['commercial',  'gameshow', 'johnsonville', 'sausage', 'way', 'make', 'smoked', 'retro'],\n",
    "# ['commercial', 'martin', 'scorsese', 'kathryn', 'alejandro', 'james', 'bigelow', 'directors', 'perpetual', 'rolex', 'cameron', 'org', 'irritu'],\n",
    "['commercial',  'mmschocolate'],\n",
    "# ['commercial', 'mmschocolate', 'animated', 'characters', 'blue'],\n",
    "#         ['commercial',  'follow', 'messages', 'favorite', 'mmschocolate'],\n",
    "# ['commercial', 'selfie', 'say', 'thanks', 'messages', 'mmschocolate', 'let'],\n",
    "#['commercial', 'invite', 'work', 'messages', 'pretending', 'mmschocolate'],\n",
    "\n",
    "['commercial', 'galaxy', 'fold', 'samsung', 'foldable', 'phone', 'flip', 'samsungmobileus'],\n",
    "# ['commercial', 'galaxy', 'fold', 'samsung', 'foldable', 'phone', 'flip', 'samsungmobileus', 'future', 'change', 'comic', 'strip', 'shape'],\n",
    "\n",
    "['commercial', 'burgers', 'postmates', 'postmate', 'subway'],\n",
    "['commercial',  'verizon', 'video', 'emergency', 'chat', 'unlimited', 'family'],\n",
    "# ['commercial', 'homosexual', 'network', 'verizon', 'people', 'chat', 'group', 'homosexuality', 'rely', 'family'],\n",
    "# ['commercial', 'verizon', 'video', 'connection', 'men', 'angola', 'chat', 'family'],\n",
    "# ['commercial', 'cancer', 'verizon', 'dad', 'fight'],\n",
    "# ['commercial', 'reliable', 'verizon', 'consistnet'],  \n",
    "\n",
    "['commercial', 'apple', 'iphone', 'mode'],\n",
    "# ['commercial', 'apple', 'verizon', 'iphone', 'mode'],\n",
    "\n",
    "['commercial', 'lion', 'frozen', 'memories',  'disney'],\n",
    "['commercial', 'tax', 'people', 'intuit', 'turbotax'],\n",
    "\n",
    "['commercial', 'fires', 'hulu', 'little', 'reese', 'everywhere'],\n",
    "# ['commercial', 'fires', 'hulu', 'washington', 'littlefireshulu', 'little', 'reese', 'everywhere', 'witherspoon', 'kerry'],\n",
    "['commercial', 'fx', 'hulu'],\n",
    "\n",
    "['commercial', 'companion', 'cool', 'american', 'delta', 'express', 'americanexpress', 'amex'],\n",
    "# ['commercial', 'com', 'farm', 'chsinc', 'chs', 'cooperativeownership'],\n",
    "['commercial', 'cheese', 'melt', 'beef', 'mcdonald', 'pounder', 'fresh', 'perfect', 'quarter', 'mcdonalds', 'sesame'],\n",
    "# ['commercial', 'cheese', 'melt', 'hulu', 'beef', 'perfecter', 'made', 'mcdonald', 'pounder', 'fresh', 'perfect', 'quarter', 'mcdonalds', 'sesame'],\n",
    "\n",
    "['commercial', 'quibi', 'stores', 'president', 'bites'],\n",
    "['commercial', 'railroad', 'quibi', 'com', 'ten', 'episode', 'dessert', 'train'],\n",
    "# ['commercial', 'quibi', 'mobile', 'female', 'com', 'phone', 'astronaut'],\n",
    "# ['commercial', 'quibi', 'mobile', 'leave', 'com', 'phone', 'minutes', 'zombie'],\n",
    "# ['commercial', 'quibi', 'sand', 'quick', 'explorer', 'com', 'minutes', 'swamp'],\n",
    "\n",
    "['commercial',  'chandelier', 'miss', 'rose', 'dior', 'roses'],\n",
    "    \n",
    "# ['commercial', 'park', 'maps', 'jurassic', 'story', 'goonies', 'christmas', 'house', 'google', 'beach'],\n",
    "# ['commercial', 'mama', 'congrats', 'support', 'emotional', 'proud', 'messages', 'mmschocolate'],\n",
    "['commercial', 'indeed', 'moon', 'spaceship', 'astraunaut'],\n",
    "# ['commercial', 'women', 'hulu', 'rights', 'speech', 'fxonhulu', 'mrsam', 'america', 'mrs', 'fx'],\n",
    "\n",
    "['commercial', 'discover', 'credit', 'fees', 'card', 'fee'],\n",
    "#     ['commercial', 'discover', 'credit', 'accepted', 'card'],\n",
    "['commercial', 'medicine', 'hiv', 'biktarvy', 'treatment'],\n",
    "\n",
    "['commercial', 'mulan', 'china', 'liu', 'yifei', 'chinese', 'disney', 'warrier'],\n",
    "['commercial', 'bar', 'kinderus', 'chocolate', 'bueno', 'kinder'],\n",
    "['commercial', 'harris', 'maurice', 'flowers', 'surface', 'microsoft'],\n",
    "['commercial', 'xfi', 'security', 'awesome', 'easy', 'simple', 'xfinity'],\n",
    "# ['commercial', 'martin', 'scorsese', 'kathryn', 'alejandro', 'james', 'bigelow', 'directors', 'perpetual', 'rolex', 'cameron', 'org', 'irritu'],\n",
    "\n",
    "['commercial', 'eno', 'wallet', 'capital', 'capitalone', 'highway'],\n",
    "['commercial', 'times', 'nyt', 'nytimes', 'slavery', 'worth', 'beach', 'york', 'truth', 'new'],\n",
    "# ['commercial','finishline', 'cadilac', 'ct', 'nobody', 'running', 'cadillac', 'woman', 'speak'],\n",
    "\n",
    "\n",
    "['commercial', 'groups', 'facebook', 'rock', 'group'],\n",
    "['commercial', 'cynthia', 'natgeo', 'erivo', 'national', 'geographic'],\n",
    "['commercial', 'xcel', 'us', 'energy', 'home', 'xcelenergy', 'smart', 'free', 'together', 'carbon'],\n",
    "\n",
    "['commercial', 'colgate', 'white', 'toothpaste'],\n",
    "['commercial', 'norweigian',  'cruisenorwegian', 'cruise'],\n",
    "    \n",
    "['commercial', 'kellog',  'specialk', 'oatmeal','strawberry', 'vanila', 'almond', 'fake', 'chocolatey', 'berries', 'delicious'],\n",
    "\n",
    "['commercial', 'abc', 'american', 'idol', 'gma', 'americanidol', 'bus'],\n",
    "['commercial', 'burger', 'fries', 'eatatperkins', 'pie', 'perkins', 'meal', 'dollars', 'bakery', 'restaurant'],\n",
    "\n",
    "['commercial', 'loretta', 'google', 'old'],\n",
    "\n",
    "['commercial', 'corona', 'coronaextrausa', 'enjoy', 'view', 'premier', 'lower', 'calories', 'carb'],\n",
    "\n",
    "\n",
    "['commercial', 'directors',  'rolex'],\n",
    "['commercial', 'flower', 'polar', 'adobe', 'creative', 'paradise', 'suite', 'creativity', 'pride', 'bear']\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topic_list = [\n",
    "    [ 'cadillac', 'escalade'],\n",
    "#     [ 'success', 'cadillac', 'regina', 'king', 'make', 'escalade', 'makeyourway', 'way'],\n",
    "# [ 'ct', 'nobody', 'cadillac', 'breaking', 'barriers', 'speak', 'make', 'makeyourway', 'glass', 'escalade', 'way'],\n",
    "# [ 'cadillac', 'king', 'driving', 'regina', 'makeyourway', 'make', 'escalade', 'way'],\n",
    "# [ 'ct', 'nobody', 'cadillac', 'breaking', 'barriers', 'speak', 'make', 'makeyourway', 'glass', 'escalade', 'way'],\n",
    "\n",
    "['sale', 'kohl','kohls'],\n",
    "# [ 'sale', 'kohl', 'cash', 'com', 'kohls', 'valentine', 'jewerly'],\n",
    "\n",
    "[  'applebees',  'applebee'],\n",
    "['fixar', 'disney', 'onward'],\n",
    "[ 'dry', 'canadadry', 'canada', 'ale', 'ginger', 'relax', 'relaxation'],\n",
    "[ 'necklace', 'macy','macys', 'gifts'],\n",
    "[ 'marvel', 'studio', 'blackwidow', 'black', 'marvelstudios', 'widow'],\n",
    "[ 'peroni','peroniusa', 'motorcycle', 'beer', 'birra', 'italia'],\n",
    "[ 'network', 'verizon', 'reliable', 'dependable', 'america', 'proven', 'consistent'],\n",
    "# [  'gameshow', 'johnsonville', 'sausage', 'way', 'make', 'smoked', 'retro'],\n",
    "# [ 'martin', 'scorsese', 'kathryn', 'alejandro', 'james', 'bigelow', 'directors', 'perpetual', 'rolex', 'cameron', 'org', 'irritu'],\n",
    "[  'mmschocolate'],\n",
    "# [ 'mmschocolate', 'animated', 'characters', 'blue'],\n",
    "#         [  'follow', 'messages', 'favorite', 'mmschocolate'],\n",
    "# [ 'selfie', 'say', 'thanks', 'messages', 'mmschocolate', 'let'],\n",
    "#[ 'invite', 'work', 'messages', 'pretending', 'mmschocolate'],\n",
    "\n",
    "[ 'galaxy', 'fold', 'samsung', 'foldable', 'phone', 'flip', 'samsungmobileus'],\n",
    "# [ 'galaxy', 'fold', 'samsung', 'foldable', 'phone', 'flip', 'samsungmobileus', 'future', 'change', 'comic', 'strip', 'shape'],\n",
    "\n",
    "[ 'burgers', 'postmates', 'postmate', 'subway'],\n",
    "[  'verizon', 'video', 'emergency', 'chat', 'unlimited', 'family'],\n",
    "# [ 'homosexual', 'network', 'verizon', 'people', 'chat', 'group', 'homosexuality', 'rely', 'family'],\n",
    "# [ 'verizon', 'video', 'connection', 'men', 'angola', 'chat', 'family'],\n",
    "# [ 'cancer', 'verizon', 'dad', 'fight'],\n",
    "# [ 'reliable', 'verizon', 'consistnet'],  \n",
    "\n",
    "[ 'apple', 'iphone', 'mode'],\n",
    "# [ 'apple', 'verizon', 'iphone', 'mode'],\n",
    "\n",
    "[ 'lion', 'frozen', 'memories',  'disney'],\n",
    "[ 'tax', 'people', 'intuit', 'turbotax'],\n",
    "\n",
    "[ 'fires', 'hulu', 'little', 'reese', 'everywhere'],\n",
    "# [ 'fires', 'hulu', 'washington', 'littlefireshulu', 'little', 'reese', 'everywhere', 'witherspoon', 'kerry'],\n",
    "[ 'fx', 'hulu'],\n",
    "\n",
    "[ 'companion', 'cool', 'american', 'delta', 'express', 'americanexpress', 'amex'],\n",
    "# [ 'com', 'farm', 'chsinc', 'chs', 'cooperativeownership'],\n",
    "[ 'cheese', 'melt', 'beef', 'mcdonald', 'pounder', 'fresh', 'perfect', 'quarter', 'mcdonalds', 'sesame'],\n",
    "# [ 'cheese', 'melt', 'hulu', 'beef', 'perfecter', 'made', 'mcdonald', 'pounder', 'fresh', 'perfect', 'quarter', 'mcdonalds', 'sesame'],\n",
    "\n",
    "[ 'quibi', 'stores', 'president', 'bites'],\n",
    "[ 'railroad', 'quibi', 'com', 'ten', 'episode', 'dessert', 'train'],\n",
    "# [ 'quibi', 'mobile', 'female', 'com', 'phone', 'astronaut'],\n",
    "# [ 'quibi', 'mobile', 'leave', 'com', 'phone', 'minutes', 'zombie'],\n",
    "# [ 'quibi', 'sand', 'quick', 'explorer', 'com', 'minutes', 'swamp'],\n",
    "\n",
    "[  'chandelier', 'miss', 'rose', 'dior', 'roses'],\n",
    "    \n",
    "# [ 'park', 'maps', 'jurassic', 'story', 'goonies', 'christmas', 'house', 'google', 'beach'],\n",
    "# [ 'mama', 'congrats', 'support', 'emotional', 'proud', 'messages', 'mmschocolate'],\n",
    "[ 'indeed', 'moon', 'spaceship', 'astraunaut'],\n",
    "# [ 'women', 'hulu', 'rights', 'speech', 'fxonhulu', 'mrsam', 'america', 'mrs', 'fx'],\n",
    "\n",
    "[ 'discover', 'credit', 'fees', 'card', 'fee'],\n",
    "#     [ 'discover', 'credit', 'accepted', 'card'],\n",
    "[ 'medicine', 'hiv', 'biktarvy', 'treatment'],\n",
    "\n",
    "[ 'mulan', 'china', 'liu', 'yifei', 'chinese', 'disney', 'warrier'],\n",
    "[ 'bar', 'kinderus', 'chocolate', 'bueno', 'kinder'],\n",
    "[ 'harris', 'maurice', 'flowers', 'surface', 'microsoft'],\n",
    "[ 'xfi', 'security', 'awesome', 'easy', 'simple', 'xfinity'],\n",
    "# [ 'martin', 'scorsese', 'kathryn', 'alejandro', 'james', 'bigelow', 'directors', 'perpetual', 'rolex', 'cameron', 'org', 'irritu'],\n",
    "\n",
    "[ 'eno', 'wallet', 'capital', 'capitalone', 'highway'],\n",
    "[ 'times', 'nyt', 'nytimes', 'slavery', 'worth', 'beach', 'york', 'truth', 'new'],\n",
    "# ['commercial','finishline', 'cadilac', 'ct', 'nobody', 'running', 'cadillac', 'woman', 'speak'],\n",
    "\n",
    "\n",
    "[ 'groups', 'facebook', 'rock', 'group'],\n",
    "[ 'cynthia', 'natgeo', 'erivo', 'national', 'geographic'],\n",
    "[ 'xcel', 'us', 'energy', 'home', 'xcelenergy', 'smart', 'free', 'together', 'carbon'],\n",
    "\n",
    "[ 'colgate', 'white', 'toothpaste'],\n",
    "[ 'norweigian',  'cruisenorwegian', 'cruise'],\n",
    "    \n",
    "[ 'kellog',  'specialk', 'oatmeal','strawberry', 'vanila', 'almond', 'fake', 'chocolatey', 'berries', 'delicious'],\n",
    "\n",
    "[ 'abc', 'american', 'idol', 'gma', 'americanidol', 'bus'],\n",
    "[ 'burger', 'fries', 'eatatperkins', 'pie', 'perkins', 'meal', 'dollars', 'bakery', 'restaurant'],\n",
    "\n",
    "[ 'loretta', 'google', 'old'],\n",
    "\n",
    "[ 'corona', 'coronaextrausa', 'enjoy', 'view', 'premier', 'lower', 'calories', 'carb'],\n",
    "\n",
    "\n",
    "[ 'directors',  'rolex'],\n",
    "[ 'flower', 'polar', 'adobe', 'creative', 'paradise', 'suite', 'creativity', 'pride', 'bear']\n",
    "    \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "import guidedlda as guidedlda\n",
    "import numpy as np\n",
    "\n",
    "def buildWordVector(train_data, word2id):\n",
    "    X = np.zeros((len(train_data), len(word2id)), dtype = 'int64')\n",
    "\n",
    "    for i in range(0, len(train_data)):\n",
    "        doc_words = train_data[i].split()\n",
    "        for word in doc_words:  \n",
    "            if word in word2id:\n",
    "                X[i][word2id[word]] += 1\n",
    "    return X\n",
    "\n",
    "def buildGuidedLDAModel(train_data, seed_topic_list, n_topics, seed_confidence, \n",
    "                        n_iter, n_top_words, random_state=7, refresh=1):\n",
    "    \n",
    "    ad_words = []\n",
    "    for keywrds in train_data:\n",
    "        keywordsSet = list(set(keywrds.split()))\n",
    "        ad_words.extend(keywordsSet)\n",
    "\n",
    "    ad_words = list(set(ad_words))\n",
    "    print(\"Length of ad keywords: \"+ str(len(ad_words)))\n",
    "\n",
    "    word2id = dict((v, idx) for idx, v in enumerate(ad_words)) #dict of word: id\n",
    "\n",
    "    X = buildWordVector(train_data, word2id)     #Build X - input data\n",
    "\n",
    "    seed_topics = {}     #Build Seed topics list\n",
    "    for t_id, st in enumerate(seed_topic_list):\n",
    "        for word in st:\n",
    "            if word in word2id:\n",
    "                 seed_topics[word2id[word]] = t_id\n",
    "\n",
    "    model = guidedlda.GuidedLDA(n_topics, n_iter, random_state= random_state, refresh= refresh)\n",
    "    model.fit(X, seed_topics=seed_topics, seed_confidence= seed_confidence)\n",
    "\n",
    "    topic_vector = model.topic_word_ #topic vector [[0.00059559 0..0.00059559]..[0.00263852 ...0.00263852]]\n",
    "    topic_words = []\n",
    "\n",
    "    for i, topic_dist in enumerate(topic_vector):\n",
    "         topic_word = np.array(ad_words)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "         topic_words.append(topic_word.tolist())\n",
    "         print('Topic {}: {}'.format(i, ' '.join(topic_word)))\n",
    "    \n",
    "    return model, topic_words, word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ip required: n_topics, topic_words\n",
    "def get_max_topic_id(topic_vector, x):\n",
    "    if x['max_prob']<=1:\n",
    "        return topic_vector.columns.get_loc(x['max_topic']) \n",
    "    else:\n",
    "        return None\n",
    "def get_max_topic_words(topic_words, topic_id):\n",
    "    if topic_id is not None:\n",
    "        return topic_words[topic_id] \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def buildTopicVectors(doc_topic, n_topics, topic_words):\n",
    "    \n",
    "    columns_label = ['topic {}'.format(i) for i in range(n_topics)]  # number of topics\n",
    "    topic_vector = pd.DataFrame(doc_topic, columns = columns_label)  #dataframe of doc-topics\n",
    "\n",
    "    topic_vector['max_prob'] = topic_vector.max(axis=1)\n",
    "    topic_vector['max_topic'] = topic_vector.idxmax(axis =1)\n",
    "#     topic_vector['max_topic_id'] = topic_vector['max_topic'].apply(lambda x: topic_vector.columns.get_loc(x))\n",
    "    topic_vector['max_topic_id'] = topic_vector.apply(lambda x: get_max_topic_id(topic_vector, x), axis = 1)\n",
    "#     topic_vector['topic_words'] = topic_vector['max_topic_id'].apply(lambda x: topic_words[x] if x is not None \n",
    "#                                                                     else None)\n",
    "    topic_vector['topic_words'] = topic_vector['max_topic_id'].apply(lambda x: get_max_topic_words(topic_words, x))\n",
    "    \n",
    "    return topic_vector\n",
    "\n",
    "def get_conf_matrix(ad_manual, max_prob, thresh_prob):\n",
    "    res = \"\"\n",
    "    if(ad_manual=='none' and max_prob >= thresh_prob):\n",
    "        res = 'FP'\n",
    "    elif(ad_manual=='none' and max_prob < thresh_prob):\n",
    "        res = 'TN'\n",
    "    elif(ad_manual!='none' and max_prob >= thresh_prob):\n",
    "        res = 'TP'\n",
    "    elif(ad_manual!='none' and max_prob < thresh_prob):\n",
    "        res = 'FN'\n",
    "    return res\n",
    "\n",
    "def computeAccuracy(result):    \n",
    "    n_tp = result[result['conf_matrix'] == 'TP'].shape[0]\n",
    "    n_fp = result[result['conf_matrix'] == 'FP'].shape[0]\n",
    "    n_fn = result[result['conf_matrix'] == 'FN'].shape[0]\n",
    "    n_tn = result[result['conf_matrix'] == 'TN'].shape[0]\n",
    "    print(\"n_tp:\"+ str(n_tp)+\" n_fp:\"+ str(n_fp)+\" n_fn:\"+ str(n_fn)+\" n_tn:\"+ str(n_tn))\n",
    "\n",
    "    precision = n_tp/(n_tp+ n_fp)\n",
    "    recall = n_tp/(n_tp+ n_fn)\n",
    "    f_measure = (2*precision*recall)/ (precision+recall)\n",
    "\n",
    "    return precision, recall, f_measure\n",
    "\n",
    "\n",
    "def removePOS(taggedText, includePOS):\n",
    "    op = ' '.join([pair[0] for pair in taggedText if pair[1] in includePOS])\n",
    "    return op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of ad keywords: 377\n",
      "Topic 0: cadillac escalade makeyourway way make regina king glass driving studio\n",
      "Topic 1: kohl kohls com sale cash valentine jewerly home astraunaut studio\n",
      "Topic 2: jurassic samsungmobileus idol studio escalade emotional mobile ten inc quick\n",
      "Topic 3: angola coronaextrausa americanidol season samsungmobileus studio escalade emotional mobile ten\n",
      "Topic 4: canada dry ale real escalade emotional mobile ten inc quick\n",
      "Topic 5: gifts give macy necklace macys jurassic astraunaut escalade emotional mobile\n",
      "Topic 6: widow black marvel studio blackwidow rely marvelstudios jurassic escalade emotional\n",
      "Topic 7: peroni italia peroniusa beautifully birra beer sand motorcycle inc one\n",
      "Topic 8: er unlimited emergency room proud emotional inc one idol studio\n",
      "Topic 9: chs chsinc cooperativeownership farm escalade emotional mobile ten inc quick\n",
      "Topic 10: phone foldable samsung fold samsungmobileus galaxy flip comic shape strip\n",
      "Topic 11: aretha geographic national cynthia natgeo genius erivo kinderus quick idol\n",
      "Topic 12: verizon family chat video network homosexuality homosexual group restaurant samsungmobileus\n",
      "Topic 13: iphone night mode apple come verizon success jurassic escalade emotional\n",
      "Topic 14: microsoft surface harris maurice flowers jurassic ten bigelow one idol\n",
      "Topic 15: gameshow johnsonille johonsonville smoked retro messages episode escalade emotional mobile\n",
      "Topic 16: hulu everywhere little fires kerry reese witherspoon washington littlefireshulu specialk\n",
      "Topic 17: america hulu fx mrs rights fxonhulu speech women mrsam samsungmobileus\n",
      "Topic 18: kinder bueno kinderus chocolate bar jurassic quick idol studio escalade\n",
      "Topic 19: pounder mcdonald quarter mcdonalds sesame fresh made melt cheese perfecter\n",
      "Topic 20: delta companion express american status grafiti americanexpress certificate amex cool\n",
      "Topic 21: quibi com minutes phone quick astronaut mobile train big zombie\n",
      "Topic 22: dior miss roses chandelier chs escalade emotional mobile ten inc\n",
      "Topic 23: perkins bakery pie burger fries eatatperkins meal idol studio escalade\n",
      "Topic 24: discover card credit annual accepted fee yes fees strawberry creativity\n",
      "Topic 25: hiv biktarvy medicine living astraunaut one idol studio escalade emotional\n",
      "Topic 26: verizon cancer fight connection dad jurassic quick idol studio escalade\n",
      "Topic 27: good jurassic bigelow idol studio escalade emotional mobile ten inc\n",
      "Topic 28: bring monster inc memories king samsungmobileus studio escalade emotional mobile\n",
      "Topic 29: ginger relax canadadry relaxation astraunaut idol studio escalade emotional mobile\n",
      "Topic 30: party jurassic bigelow idol studio escalade emotional mobile ten inc\n",
      "Topic 31: times new york truth sausage nytimes beach slavery nyt specialk\n",
      "Topic 32: spaceship astraunaut jurassic samsungmobileus idol studio escalade emotional mobile ten\n",
      "Topic 33: vanila chocolatey bites jurassic idol studio escalade emotional mobile ten\n",
      "Topic 34: energy xcel free carbon xcelenergy home smart jurassic astraunaut studio\n",
      "Topic 35: colgate optic toothpaste white years dance quick bigelow one idol\n",
      "Topic 36: cruise norweigian line feel ncl dollars cruisenorwegian astraunaut studio escalade\n",
      "Topic 37: facebook groups group want together rock astraunaut idol studio escalade\n",
      "Topic 38: task view jurassic bigelow idol studio escalade emotional mobile ten\n",
      "Topic 39: mmschocolate let say messages congrats favorite help support characters follow\n",
      "Topic 40: google maps loretta jurassic beach story christmas goonies park old\n",
      "Topic 41: corona premier lower enjoy carb calories jurassic quick studio escalade\n",
      "Topic 42: rolex directors perpetual alejandro rritu martin scorsese cameron org kathryn\n",
      "Topic 43: adobe flower thanks suite creativity bear quick idol studio escalade\n",
      "Topic 44: ct speak cadillac nobody barriers breaking episode pretending running woman\n",
      "Topic 45: verizon reliable proven consistnet dependable network railroad studio escalade emotional\n",
      "Topic 46: one capital eno wallet highway capitalone quick idol studio escalade\n",
      "Topic 47: burgers postmates make think postmate floating subway slow fast quick\n",
      "Topic 48: selfie treatment chs idol studio escalade emotional mobile ten inc\n",
      "Topic 49: people turbotax tax intuit men jurassic astraunaut studio escalade emotional\n",
      "Topic 50: worth jurassic samsungmobileus idol studio escalade emotional mobile ten inc\n",
      "Topic 51: jurassic samsungmobileus idol studio escalade emotional mobile ten inc quick\n",
      "Topic 52: animated jurassic walked idol studio escalade emotional mobile ten inc\n",
      "Topic 53: indeed female walked moon moments great men astraunaut studio escalade\n",
      "Topic 54: irrisiste creative astronaut applebees paradise pride polar inc bigelow one\n",
      "Topic 55: disney mulan yifei china chinese warrier liu inc one idol\n",
      "Topic 56: disney together us lion frozen samsungmobileus studio escalade emotional mobile\n",
      "Topic 57: special almond oatmeal fake berries jurassic astraunaut idol studio escalade\n",
      "Topic 58: blue invite jurassic astraunaut idol studio escalade emotional mobile ten\n",
      "Topic 59: johnsonville make way jurassic astraunaut studio escalade emotional mobile ten\n",
      "Topic 60: applebee bowls eatin ten astraunaut idol studio escalade emotional mobile\n",
      "Topic 61: jurassic samsungmobileus idol studio escalade emotional mobile ten inc quick\n",
      "Topic 62: rose swamp explorer anything host jurassic astraunaut studio escalade emotional\n",
      "Topic 63: abc gma singin idol bus new dessert american jurassic studio\n",
      "Topic 64: onward disney strawberry march fixar kinderus astraunaut studio escalade emotional\n",
      "Topic 65: delicious delight specialk kellog samsungmobileus idol studio escalade emotional mobile\n",
      "Topic 66: xfinity xfi easy awesome simple movies security quick idol studio\n"
     ]
    }
   ],
   "source": [
    "#training data for the model\n",
    "import logging\n",
    "\n",
    "main = logging.getLogger()\n",
    "main.setLevel(logging.ERROR)\n",
    "\n",
    "# annotations_data['Keywords'] = annotations_data['Keywords'].apply(lambda x: x+\" commercial\")\n",
    "train_data = annotations_data['Keywords'].apply(lambda x: cleanTweet(x))\n",
    "\n",
    "# annotations_data['names_edited'] = annotations_data['names'].apply(lambda x: x+\" commercial\")\n",
    "# train_data = annotations_data['names_edited'].apply(lambda x: cleanTweet(x))\n",
    "\n",
    "train_data_pos = train_data.apply(lambda x: removePOS(nltk.pos_tag(x.split()), ('NN') ))\n",
    "\n",
    "seed_topic_pos = []\n",
    "for seeds in seed_topic_list:\n",
    "    tagged = nltk.pos_tag(seeds)\n",
    "    seed_topic_pos.append(removePOS(tagged, ('NN')))\n",
    "    \n",
    "# print(seed_topic_pos)   \n",
    "\n",
    "model, topic_words, word2id = buildGuidedLDAModel(\n",
    "                                                  train_data, \n",
    "#                                                 train_data_pos, \n",
    "                                                  seed_topic_list,\n",
    "#                                                 seed_topic_pos,\n",
    "                                                  n_topics= 67, \n",
    "                                                  seed_confidence = 1, \n",
    "                        n_iter= 500, n_top_words = 10, random_state=7, refresh=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tweet_id_str</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>retweeted_status</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>ad_mentioned</th>\n",
       "      <th>ad_manual</th>\n",
       "      <th>ad_keywords</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>gender_firstname</th>\n",
       "      <th>description</th>\n",
       "      <th>gender_text_200_tweets</th>\n",
       "      <th>comm</th>\n",
       "      <th>ad_clean</th>\n",
       "      <th>ad_gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>373</td>\n",
       "      <td>1.226680e+18</td>\n",
       "      <td>Mon Feb 10 01:23:17 +0000 2020</td>\n",
       "      <td>RT @frontlinepbs: The #Oscars start in just a ...</td>\n",
       "      <td>{'created_at': 'Mon Feb 10 00:54:11 +0000 2020...</td>\n",
       "      <td>oscars start minutes sending luck wishes team ...</td>\n",
       "      <td>quibi.com_(1:16:30)</td>\n",
       "      <td>none</td>\n",
       "      <td>['bites', 'stores', 'quibi', 'minutes', 'big',...</td>\n",
       "      <td>20006234</td>\n",
       "      <td>WETA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Public television channels TV 26, WETA HD, WET...</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  tweet_id_str                      created_at  \\\n",
       "2499    373  1.226680e+18  Mon Feb 10 01:23:17 +0000 2020   \n",
       "\n",
       "                                                   text  \\\n",
       "2499  RT @frontlinepbs: The #Oscars start in just a ...   \n",
       "\n",
       "                                       retweeted_status  \\\n",
       "2499  {'created_at': 'Mon Feb 10 00:54:11 +0000 2020...   \n",
       "\n",
       "                                             text_clean         ad_mentioned  \\\n",
       "2499  oscars start minutes sending luck wishes team ...  quibi.com_(1:16:30)   \n",
       "\n",
       "     ad_manual                                        ad_keywords   user_id  \\\n",
       "2499      none  ['bites', 'stores', 'quibi', 'minutes', 'big',...  20006234   \n",
       "\n",
       "     user_name gender_firstname  \\\n",
       "2499      WETA              NaN   \n",
       "\n",
       "                                            description  \\\n",
       "2499  Public television channels TV 26, WETA HD, WET...   \n",
       "\n",
       "     gender_text_200_tweets  comm ad_clean ad_gender  \n",
       "2499                 female     0     none       NaN  "
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "man_ann_data_raw = pd.read_csv(r'/Users/vcroopana/Downloads/summer2020/oscar_ads/op/jaccard/op_jacc_5K_gender_sampled.csv')    \n",
    "man_ann_data = man_ann_data_raw.dropna(subset= ['ad_manual'])\n",
    "man_ann_data.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [cleanTweet(doc) for doc in man_ann_data['text_clean']]\n",
    "X = buildWordVector(test_data, word2id)\n",
    "\n",
    "\n",
    "doc_topic = model.transform(X)\n",
    "# print(doc_topic)\n",
    "topic_vector = buildTopicVectors(doc_topic, 67, topic_words)\n",
    "# topic_vector.round(2).head(2)\n",
    "\n",
    "man_ann_data = man_ann_data.reset_index(drop=True)\n",
    "\n",
    "result = pd.concat([man_ann_data, topic_vector], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tp:308 n_fp:189 n_fn:381 n_tn:1623\n",
      "precision: 0.62 recall: 0.447 f_measure: 0.519\n"
     ]
    }
   ],
   "source": [
    "result['conf_matrix'] = result.apply(lambda x: get_conf_matrix(x['ad_manual'], x['max_prob'], \n",
    "                                                               thresh_prob = 0.84), axis =1)\n",
    "precision, recall, f_measure = computeAccuracy(result)\n",
    "print(\"precision: \"+ str(np.round(precision, 3))+\" recall: \"+ str(np.round(recall,3))+\" f_measure: \"\n",
    "      + str(np.round(f_measure,3)))\n",
    "\n",
    "op_name = '/Users/vcroopana/Downloads/summer2020/oscar_ads/op/topicmodelling/op_mann_lda_67_topics.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_reqd = ['text', 'text_clean', 'ad_manual', 'ad_keywords', 'max_prob', 'max_topic_id', 'topic_words', 'conf_matrix']\n",
    "result[cols_reqd].to_csv(op_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tweet_id_str</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>retweeted_status</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>ad_mentioned</th>\n",
       "      <th>ad_manual</th>\n",
       "      <th>ad_keywords</th>\n",
       "      <th>user_id</th>\n",
       "      <th>...</th>\n",
       "      <th>topic 62</th>\n",
       "      <th>topic 63</th>\n",
       "      <th>topic 64</th>\n",
       "      <th>topic 65</th>\n",
       "      <th>topic 66</th>\n",
       "      <th>max_prob</th>\n",
       "      <th>max_topic</th>\n",
       "      <th>max_topic_id</th>\n",
       "      <th>topic_words</th>\n",
       "      <th>conf_matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>373</td>\n",
       "      <td>1.226680e+18</td>\n",
       "      <td>Mon Feb 10 01:23:17 +0000 2020</td>\n",
       "      <td>RT @frontlinepbs: The #Oscars start in just a ...</td>\n",
       "      <td>{'created_at': 'Mon Feb 10 00:54:11 +0000 2020...</td>\n",
       "      <td>oscars start minutes sending luck wishes team ...</td>\n",
       "      <td>quibi.com_(1:16:30)</td>\n",
       "      <td>none</td>\n",
       "      <td>['bites', 'stores', 'quibi', 'minutes', 'big',...</td>\n",
       "      <td>20006234</td>\n",
       "      <td>...</td>\n",
       "      <td>6.946954e-310</td>\n",
       "      <td>6.946954e-310</td>\n",
       "      <td>6.946954e-310</td>\n",
       "      <td>6.946954e-310</td>\n",
       "      <td>6.946954e-310</td>\n",
       "      <td>6.946988e-310</td>\n",
       "      <td>topic 1</td>\n",
       "      <td>1</td>\n",
       "      <td>[mrs, fxonhulu, speech, work, hulu, mrsam, fol...</td>\n",
       "      <td>TN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  tweet_id_str                      created_at  \\\n",
       "0    373  1.226680e+18  Mon Feb 10 01:23:17 +0000 2020   \n",
       "\n",
       "                                                text  \\\n",
       "0  RT @frontlinepbs: The #Oscars start in just a ...   \n",
       "\n",
       "                                    retweeted_status  \\\n",
       "0  {'created_at': 'Mon Feb 10 00:54:11 +0000 2020...   \n",
       "\n",
       "                                          text_clean         ad_mentioned  \\\n",
       "0  oscars start minutes sending luck wishes team ...  quibi.com_(1:16:30)   \n",
       "\n",
       "  ad_manual                                        ad_keywords   user_id  ...  \\\n",
       "0      none  ['bites', 'stores', 'quibi', 'minutes', 'big',...  20006234  ...   \n",
       "\n",
       "        topic 62       topic 63       topic 64       topic 65       topic 66  \\\n",
       "0  6.946954e-310  6.946954e-310  6.946954e-310  6.946954e-310  6.946954e-310   \n",
       "\n",
       "        max_prob max_topic  max_topic_id  \\\n",
       "0  6.946988e-310   topic 1             1   \n",
       "\n",
       "                                         topic_words  conf_matrix  \n",
       "0  [mrs, fxonhulu, speech, work, hulu, mrsam, fol...           TN  \n",
       "\n",
       "[1 rows x 89 columns]"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Approaches tried:\n",
    "#Trained on test data without using ad keywords\n",
    "# Trained adding commerical to keywwords i.e training data and see topic list \n",
    "# trained without commercial in keywords i.e training data and seed topic list : f1 score: 0.53 with 0.84 thresh prob\n",
    "# n_top_words = 3, fscore = 0.544 at 0.9 thresh prob\n",
    "# 35 topics, 3 words, 900 iters, 0.91 prob 0.551 f score; precision: 0.601 recall: 0.509 f_measure: 0.551\n",
    "# 35 topics, 3 words, 200 iters, 0.92 prob 0.562 f score; precision: 0.627 recall: 0.509 f_measure: 0.562\n",
    "\n",
    "\n",
    "# google (145/209), disney() - too many false positives\n",
    "# False Negatives ?\n",
    "\n",
    "# ntopics = 20, 3 words, 200 iters, 0.97 prob, \n",
    "# n_tp:354 n_fp:191 n_fn:335\n",
    "# precision: 0.65 recall: 0.514 f_measure: 0.574\n",
    "\n",
    "# google (140/191) - too many false positives\n",
    "\n",
    "#67 topics:\n",
    "# n_tp:313 n_fp:177 n_fn:376 n_tn:1635\n",
    "# precision: 0.639 recall: 0.454 f_measure: 0.531\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "\n",
    "def displayModel(model, corpus, dictionary):\n",
    "    \n",
    "    lda_model_display = pyLDAvis.gensim.prepare(model, corpus, dictionary, sort_topics=True, mds='mmds')\n",
    "    pyLDAvis.show(lda_model_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayModel(model, , word2id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
