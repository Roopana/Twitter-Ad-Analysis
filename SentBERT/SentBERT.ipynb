{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentBERT.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "niAmM0-gYQv4",
        "T_d32URbUc-P"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMfECRFV4Sq5"
      },
      "source": [
        "Ref : https://www.sbert.net/docs/training/overview.html#creating-networks-from-scratch\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezL-7kHR159P"
      },
      "source": [
        "pip install -U sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUaMZl7l1l2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "077da23a-7f2c-4e52-856e-6b61051f66e1"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "model = SentenceTransformer('distilbert-base-nli-mean-tokens')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-386182866b3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInputExample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'distilbert-base-nli-mean-tokens'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niAmM0-gYQv4"
      },
      "source": [
        "\n",
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gsl3ppgi52kB"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_2 = pd.read_csv(\"./2_man_ann_sb.csv\",  index_col= None)\n",
        "df_2 = df_2.dropna(subset = ['tweet_text'])\n",
        "\n",
        "df_1 = pd.read_csv(\"./mann_ann_sb.csv\", index_col= None)\n",
        "\n",
        "df_3 = pd.read_csv(\"./3_man_ann_sb_full_1.csv\", index_col = None)\n",
        "df_3 = df_3.dropna(subset = ['tweet_text'])\n",
        "\n",
        "df_raw = df_1.append(df_2).append(df_3) # using batch 1 and batch 2 for training\n",
        "\n",
        "print(df_raw.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSIOy26R5-FF"
      },
      "source": [
        "# Remove ads marked as below because they are not available in ad annotations file although tweets mention them\n",
        "# commercials, joe biden, pizzahut, joe bieden, michael bloomberg, mike bloomberg, scientology...\n",
        "ads_remove = ['commercials', 'joe biden', 'pizzahut', 'joe bieden', 'michael bloomberg', 'mike bloomberg', \n",
        "              'scientology','papa johns',  'bakari',  'secret', 'dashlane', 'bernie the peoples perfume',\n",
        "              'ram trucks', 'golden gronks', \"bush's best\", 'ragged old flag', 'patience', 'guitar hero',\n",
        "              'disney mulan']\n",
        "\n",
        "# ads_rename = ['nfl100', 'tide']\n",
        "# rename ads with spelling faults while manually adding the annotation \n",
        "\n",
        "df_raw['ad_manual_adjusted'] = df_raw['ad_manual_adjusted'].apply(lambda x: x.lower())\n",
        "df_raw.loc[df_raw.ad_manual_adjusted == \"discover card  no we don‚äôt charge annual fees\", \n",
        "       \"ad_manual_adjusted\"] = \"discover card  no we don’t charge annual fees\"\n",
        "df_raw.loc[df_raw.ad_manual_adjusted == \"doritos the cool ranch\", \n",
        "       \"ad_manual_adjusted\"] = \"doritos  the cool ranch\"\n",
        "df_raw.loc[df_raw.ad_manual_adjusted == \"discover card yes we're accepted\", \n",
        "       \"ad_manual_adjusted\"] =  \"discover card  yes we’re accepted\"\n",
        "df_raw.loc[df_raw.ad_manual_adjusted == \"discover card yes we’re accepted\", \n",
        "       \"ad_manual_adjusted\"] =  \"discover card  yes we’re accepted\"\n",
        "df_raw.loc[df_raw.ad_manual_adjusted == \"discover card  yes we're accepted\", \n",
        "       \"ad_manual_adjusted\"] =  \"discover card  yes we’re accepted\"\n",
        "df_raw.loc[df_raw.ad_manual_adjusted == \"budweiser typical american\", \n",
        "       \"ad_manual_adjusted\"] = \"budweiser  typical american\"\n",
        "df_raw.loc[df_raw.ad_manual_adjusted == 'fox  halftime show  teaser_3',\n",
        "            \"ad_manual_adjusted\"] = \"fox  halftime show  teaser_1\"\n",
        "df_raw.loc[df_raw.ad_manual_adjusted == 'fox  halftime show  teaser_2',\n",
        "            \"ad_manual_adjusted\"] = \"fox  halftime show  teaser_1\"\n",
        "            \n",
        "print(df_raw.shape)\n",
        "df = pd.DataFrame()\n",
        "removed_Data = pd.DataFrame()\n",
        "\n",
        "for i,row  in df_raw.iterrows():\n",
        "  if row['ad_manual_adjusted'] not in ads_remove:\n",
        "    df = df.append(row)\n",
        "  else:\n",
        "    removed_Data = removed_Data.append(row)\n",
        "print(df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jUwMr-P7jo_"
      },
      "source": [
        "classType = 'sent_exploded' # binary or multi-class or sent-exploded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5etga0l7kJ8"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "# stop.append('superbowl')\n",
        "# stop.append('super') \n",
        "# stop.append('bowl')\n",
        "\n",
        "# remove for multi class since almost all ads have these words\n",
        "if classType is not 'binary':\n",
        "  stop.append('commercial')\n",
        "  stop.append('ad')\n",
        "  stop.append('commercials')\n",
        "  stop.append('ads')\n",
        "print(len(stop))\n",
        "\n",
        "def removeMentions(text):\n",
        "\n",
        "    textBeforeMention = text.partition(\"@\")[0]\n",
        "    textAfterMention = text.partition(\"@\")[2]\n",
        "    textAfterMention =  re.sub(r':', '', textAfterMention) #cadillac join the 31k\n",
        "    tHandle = textAfterMention.partition(\" \")[0].lower() #cadillac    \n",
        "    text = textBeforeMention+ \" \" + textAfterMention  \n",
        "    return text\n",
        "\n",
        "def cleanTweet(strinp):\n",
        "    strinp = re.sub(r'RT', \"\", strinp) # Remove RT\n",
        "    strinp = strinp.lower()\n",
        "    \n",
        "    stop_removed_list = [word for word in strinp.split() if word not in (stop)]\n",
        "    stop_removed = ' '.join([str(elem) for elem in stop_removed_list])    \n",
        "    text = re.sub('https?://[A-Za-z0-9./]+', ' ', stop_removed) # Remove URLs\n",
        "    text = removeMentions(text)\n",
        "    text = re.sub('[^\\x00-\\x7F]+', ' ', text) # Remove non-ASCII chars.\n",
        "    \n",
        "    # remove punctuations except '-'\n",
        "    punctuation = ['(', ')', '[',']','?', ':', ':', ',', '.', '!', '/', '\"', \"'\", '@', '#', '&', '-', '_']\n",
        "    text = \"\".join((char for char in text if char not in punctuation))\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text) # remove all other than alphabet chars \n",
        "\n",
        "#     text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text) # remove all single characters     \n",
        "    stop_removed_l = [word for word in text.split() if word not in (stop)]\n",
        "    stop_removed = ' '.join([str(elem) for elem in stop_removed_l]) \n",
        "    return stop_removed\n",
        "\n",
        "print(cleanTweet(\"RT @cadillacabc: Joinrt the 31K james_bond\") )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOBfatuk7znG"
      },
      "source": [
        "df['text_clean'] = df['tweet_text'].apply(lambda x: cleanTweet(x))\n",
        "df['ad_manual_adjusted'] = df['ad_manual_adjusted'].apply(lambda x: x.lower())\n",
        "df['ad_related'] = df['ad_manual_adjusted'].apply(lambda ad: 0 if ad == 'none' else 1)\n",
        "\n",
        "comma_filter = ~df['ad_manual_adjusted'].str.contains(',')\n",
        "df = df[comma_filter]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUKBKwW2738L"
      },
      "source": [
        "df_unique = df.drop_duplicates(subset = ['text_clean'])\n",
        "df_with_dupes = df\n",
        "df = df_unique\n",
        "\n",
        "print(df_with_dupes.shape)\n",
        "print(df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MYnXpVK8HYJ"
      },
      "source": [
        "ad_product_df = pd.read_csv('./SB_ad_annotations_product_category_modified.csv')\n",
        "ad_product_df = ad_product_df.rename(columns = {'Ad Name': 'Ad_Name'}) # rename the column to remove space\n",
        "ad_product_df = ad_product_df.dropna() # because the file has trailing empty rows, remove them\n",
        "# remove fox half time show teaser_2 ad because its keywords is same as fox half time show teaser_2\n",
        "print(ad_product_df.shape)\n",
        "ad_product_df.drop(ad_product_df[ad_product_df['Ad_Name'] == 'FOX  Halftime Show  Teaser_2'].index, inplace = True) \n",
        "print(ad_product_df.shape)\n",
        "ad_product_dict = dict()\n",
        "\n",
        "ad_product_df['Product_modified'] = ad_product_df['Product_modified'].apply(lambda x: x.lower())\n",
        "for i, row in ad_product_df.iterrows():\n",
        "  ad_product_dict[row['Ad_Name'].lower()] = row['Product_modified'].lower()\n",
        "\n",
        "ad_product_dict['none'] = 'none'\n",
        "print(ad_product_dict)\n",
        "\n",
        "df['product_modified'] = df['ad_manual_adjusted'].apply(lambda ad: ad_product_dict[ad])\n",
        "df['product_modified'] = df['product_modified'].apply(lambda x: x.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpiDhITtjPou"
      },
      "source": [
        "# group the keywords of ads part of a product bucket and append these to the training sentences\n",
        "ad_product_keywords_dict = ad_product_df.groupby('Product_modified')['Key Terms  Round 2'].agg(lambda x : x.sum() if x.dtype=='float64' else ' '.join(x))\n",
        "# clean the ad key words - not removing duplicate words here - TODO\n",
        "for ad_bucket in ad_product_keywords_dict.keys():\n",
        "  ad_product_keywords_dict[ad_bucket] = cleanTweet(ad_product_keywords_dict[ad_bucket])\n",
        "ad_product_df['product_modified_keywords'] = ad_product_df['Product_modified'].apply(lambda x: ad_product_keywords_dict[x])\n",
        "ad_product_df.head(2)\n",
        "\n",
        "# generate adname, ad keywords dict to use in sent exploding\n",
        "ad_name_keywords_dict = dict()\n",
        "for i, row in ad_product_df.iterrows():\n",
        "  ad_name_keywords_dict[row['Ad_Name'].lower()] = cleanTweet(row['Key Terms  Round 2'])\n",
        "ad_name_keywords_dict['none'] = 'none'\n",
        "print(ad_name_keywords_dict)\n",
        "\n",
        "ad_keywords_name_dict = dict()\n",
        "for ad_name in ad_name_keywords_dict:\n",
        "  keywords_temp = ad_name_keywords_dict[ad_name]\n",
        "  ad_keywords_name_dict[keywords_temp] = ad_name\n",
        "print(ad_keywords_name_dict)\n",
        "\n",
        "ad_product_df['ad_name_keywords'] = ad_product_df['Ad_Name'].apply(lambda x: ad_name_keywords_dict[x.lower()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eG1duwodju1c"
      },
      "source": [
        "# use column name 'ad_manual_adjusted' of df to get ad_name\n",
        "def get_ad_related_twts(df, removeCommas = True):\n",
        "  df['ad_manual_adjusted'] = df['ad_manual_adjusted'].apply(lambda x: x.lower())\n",
        "  ad_filter = df['ad_manual_adjusted']!= 'none'\n",
        "  ad_related_twts = df[ad_filter]\n",
        "  if removeCommas:\n",
        "    ad_filter_1 = ~ad_related_twts['ad_manual_adjusted'].str.contains(',')\n",
        "    ad_related_twts = ad_related_twts[ad_filter_1]\n",
        "  return ad_related_twts\n",
        "\n",
        "def getAdTweets(ad_related_twts, ad):\n",
        "  return ad_related_twts[ad_related_twts.ad_manual_adjusted == ad].shape[0]\n",
        "\n",
        "def get_ad_id_dict(ad_related_twts): \n",
        "  n_ad_related = ad_related_twts.shape[0]\n",
        "  print(\"# ad related tweets: \"+ str(n_ad_related))\n",
        "  ads_annotated = ad_related_twts.ad_manual_adjusted.values\n",
        "  adset = set(ads_annotated)\n",
        "  print(\"unique ads:\"+ str(len(adset)))\n",
        "  ad_id_dict = {}\n",
        "  i = 0\n",
        "  for ad in adset : \n",
        "    if(getAdTweets(ad_related_twts, ad) >=2):\n",
        "      ad_id_dict[ad] = i\n",
        "      i = i+1\n",
        "    else:\n",
        "      print('ad with <2 samples: '+ str(ad))\n",
        "  print(\" No of ads with >=2 samples:\"+ str(len(ad_id_dict)))\n",
        "  ad_id_dict['none'] = len(ad_id_dict)\n",
        "  print(ad_id_dict)\n",
        "  return ad_id_dict\n",
        "\n",
        "def convertAdNameToAdId(ad_id_dict, ad_name):\n",
        "  if ad_name in ad_id_dict:\n",
        "    return ad_id_dict[ad_name]\n",
        "  else:\n",
        "    return ad_id_dict['none']\n",
        "\n",
        "ad_related_twts = get_ad_related_twts(df)\n",
        "ad_id_dict = get_ad_id_dict(get_ad_related_twts(df))\n",
        "n_unique_ads = len(ad_id_dict) # ad_id_dict has none as well, so minus 1 when using embeddings\n",
        "df['ad_manual_adjusted_id'] = df['ad_manual_adjusted'].apply(lambda x: convertAdNameToAdId(ad_id_dict,x))\n",
        "print(n_unique_ads)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_d32URbUc-P"
      },
      "source": [
        "#Cosine sim or embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQvTArLZhCUO"
      },
      "source": [
        "# from sentence_transformers import util\n",
        "\n",
        "# tweets = df['text_clean'].values\n",
        "# ad_names = df['ad_manual_adjusted'].values\n",
        "# ad_keywords = list(ad_name_keywords_dict.values())\n",
        "\n",
        "# twt_embeddings = model.encode(tweets, convert_to_tensor = True)\n",
        "# ad_embeddings = model.encode(ad_keywords, convert_to_tensor = True)\n",
        "\n",
        "# #Compute cosine-similarities for each sentence with each other sentence\n",
        "# cosine_scores = util.pytorch_cos_sim(twt_embeddings, ad_embeddings)\n",
        "\n",
        "# #Find the pairs with the highest cosine similarity scores\n",
        "# pairs = []\n",
        "# for i in range(len(cosine_scores)-1):\n",
        "#     max_ad_keywords = ''\n",
        "#     max_score = 0\n",
        "#     ad_sentBERT = ''\n",
        "#     max_score_ad_id = 0\n",
        "#     for j in range(0, len(ad_embeddings)):\n",
        "#         if(cosine_scores[i][j] > max_score):\n",
        "#           max_score = cosine_scores[i][j]\n",
        "#           max_ad_keywords = ad_keywords[j]\n",
        "#           ad_sentBERT = ad_keywords_name_dict[max_ad_keywords]\n",
        "#           max_score_ad_id = j\n",
        "#         # pairs.append({'index': [i, j], 'score': cosine_scores[i][j],'tweet': tweets[i],  'ad_sentBERT_keywords': curr_ad_keywords,'ad_sentBERT': ad_keywords_name_dict[curr_ad_keywords] })\n",
        "#     pairs.append({'index': [i, max_score_ad_id], 'score': max_score,'tweet': tweets[i],  'ad_sentBERT_keywords': max_ad_keywords,'ad_sentBERT': ad_sentBERT })\n",
        "# #Sort scores in decreasing order\n",
        "# #pairs = sorted(pairs, key= lambda x: x['score'], reverse=True)\n",
        "\n",
        "# for pair in pairs[0:10]:\n",
        "#     i, j = pair['index']\n",
        "#     print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(tweets[i], ad_keywords[j], pair['score']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0tjZSyVnuBb"
      },
      "source": [
        "# df_pairs = pd.DataFrame(pairs, columns=['index', 'score', 'tweet', 'ad_sentBERT_keywords', 'ad_sentBERT'])\n",
        "\n",
        "# df['ad_sentBERT'] = df_pairs['ad_sentBERT']\n",
        "# df['score'] = df_pairs['score']\n",
        "# df['match'] = df.apply(lambda row: 1 if row['ad_sentBERT'] == row['ad_manual_adjusted'] else 0, axis = 1)\n",
        "\n",
        "# print(df.shape)\n",
        "# print(df['match'].sum())\n",
        "# print(df['match'].sum() * 100/ df.shape[0])\n",
        "# df.to_csv('./sent_bert_results.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Os1XcxU4KWc"
      },
      "source": [
        "**Train the model**\n",
        "\n",
        "Loss Function - CosineSimilarityLoss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-tWqokgjKtr"
      },
      "source": [
        "# train test split for multi class classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "n = df.shape[0]\n",
        "print(n)\n",
        "# sentences, test_sentences, labels, test_labels = train_test_split(df.text_clean.values, \n",
        "#                   df.ad_manual_adjusted.values, \n",
        "#                   random_state = 2018, \n",
        "#                    test_size = 0.2, stratify = df.ad_manual_adjusted.values)\n",
        "sentences, test_sentences, labels, test_labels = train_test_split(df.text_clean.values, \n",
        "                  df.ad_manual_adjusted.values, \n",
        "                  random_state = 2018, \n",
        "                   test_size = 0.2)\n",
        "train_size = len(sentences)\n",
        "test_size = len(test_sentences)\n",
        "print( \"Train size: \"+ str(train_size)+\" test size:\" + str(test_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x97BgUcj3J0N"
      },
      "source": [
        "ad_names = list(ad_id_dict.keys())\n",
        "\n",
        "def build_train_data(twt, label_true):\n",
        "  examples = []\n",
        "  for ad in ad_names:\n",
        "      if label_true == ad:\n",
        "        examples.append(InputExample(texts=[twt, ad_name_keywords_dict[label_true] ], label= 1.0))\n",
        "      else:\n",
        "        examples.append(InputExample(texts=[twt, ad_name_keywords_dict[label_true] ], label= 0.0))\n",
        "  return examples\n",
        "\n",
        "train_ex_nested = [ build_train_data(sentences[i], labels[i]) for i in range(0, len(sentences)) ]\n",
        "\n",
        "train_examples = []\n",
        "for row in train_ex_nested:\n",
        "  train_examples.extend(row)\n",
        "\n",
        "print(len(train_examples))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZ6IAa_AcRRj"
      },
      "source": [
        "# Sentence Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-k5k-gXHS8o"
      },
      "source": [
        "# train_dataloader = DataLoader(train_examples, shuffle=True, batch_size = 200)\n",
        "# train_loss = losses.CosineSimilarityLoss(model)\n",
        "model = SentenceTransformer('nq-distilbert-base-v1')\n",
        "\n",
        "# # model.fit(train_objectives=[(train_dataloader, train_loss)], epochs = 3, warmup_steps=100, evaluation_steps=500)\n",
        "# model.fit(train_objectives=[(train_dataloader, train_loss)], epochs = 1)\n",
        "# #           ,output_path=model_save_path)\n",
        "# #start: 11:34 am\n",
        "# # takes 3 hrs for 1000 input training samples i.e 61K exploded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dInuvhbucmrl"
      },
      "source": [
        "# Cross Encoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXANWEyOZEQh"
      },
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "model = CrossEncoder('cross-encoder/stsb-TinyBERT-L-4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyvNAMWHZRJA"
      },
      "source": [
        "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size = 200)\n",
        "# train_loss = losses.CosineSimilarityLoss(model)\n",
        "model.fit(train_dataloader)\n",
        "# 23 sec for 1000 samples "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CVlYof8AFGD"
      },
      "source": [
        "model.save('./cross_enc_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn7pbI5oQ38m"
      },
      "source": [
        "from sklearn. preprocessing import normalize\n",
        "import numpy as np\n",
        "\n",
        "ad_keywords = list(ad_name_keywords_dict.values())\n",
        "ad_names = list(ad_name_keywords_dict.keys())\n",
        "\n",
        "def get_most_sim_ad(test_twt):\n",
        "  sims = []\n",
        "  for ad_keyword in ad_keywords:\n",
        "    sims.append(model.predict([test_twt, ad_keyword]))\n",
        "\n",
        "  norm_sims = normalize(np.array(sims).reshape(1, -1))\n",
        "  max_sim = max(norm_sims)\n",
        "  max_ad_ind = np.argmax(norm_sims)\n",
        "  max_ad = ad_names[max_ad_ind]\n",
        "  # print(test_twt+\":\"+ max_ad)\n",
        "  return max_ad\n",
        "\n",
        "max_ad = get_most_sim_ad('nyxipuff alexa like tweet')\n",
        "\n",
        "# pred_cross_enc_train = [get_most_sim_ad(train_twt) for train_twt in sentences[0: 500]]\n",
        "\n",
        "pred_cross_enc = [get_most_sim_ad(test_twt) for test_twt in test_sentences]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHaTcOWSX2OG"
      },
      "source": [
        "# Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfA7Nu0AguGv",
        "outputId": "14d60997-8938-4814-d7b5-39f1ebc1c5a0"
      },
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "def getMetrics(true_labels_flat, pred_labels, averageType):\n",
        "  print(\"Evaluating metrics as per '\"+averageType+\"' average type\")\n",
        "  f1score = f1_score(true_labels_flat, pred_labels, average= averageType) \n",
        "  print('f1_score:'+ str(f1score))\n",
        "\n",
        "  prec = precision_score(true_labels_flat, pred_labels, average=averageType)\n",
        "  print('Precision:'+ str(prec))\n",
        "\n",
        "  acc = accuracy_score(true_labels_flat, pred_labels)\n",
        "  print(\"Accuracy: \"+ str(acc))\n",
        "\n",
        "  recall = recall_score(true_labels_flat, pred_labels, average=averageType)\n",
        "  print(\"recall: \"+ str(recall))\n",
        "\n",
        "  class_report = classification_report(true_labels_flat, pred_labels, output_dict = True)\n",
        "  class_report_df = pd.DataFrame(class_report)\n",
        "  print(class_report)\n",
        "  class_report_df.to_csv(\"./class_report_cross_enc.csv\")\n",
        "\n",
        "  confusionmatrix = confusion_matrix(true_labels_flat, pred_labels)\n",
        "  #print(confusionmatrix)\n",
        "  # if averageType == 'binary':\n",
        "    # tn, fp, fn, tp = confusionmatrix.ravel()\n",
        "    # print(\"tp: \"+ str(tp)+\" tn: \"+ str(tn)+\" fp: \"+ str(fp)+\" fn: \"+ str(fn))\n",
        "\n",
        "if classType == 'binary':\n",
        "  averageType = 'binary'\n",
        "elif classType == 'sent_exploded':\n",
        "  averageType = 'weighted' # no weighting for imbalance\n",
        "else:\n",
        "  averageType = 'weighted'\n",
        "  # micro: Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
        "  # macro: Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
        "  # weighted : Calculate metrics for each label, and find their average weighted by support\n",
        "  #  (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; \n",
        "  # it can result in an F-score that is not between precision and recall.\n",
        "classType = 'sent_exploded'\n",
        "getMetrics(test_labels, pred_cross_enc, averageType)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluating metrics as per 'weighted' average type\n",
            "f1_score:0.010613120784794606\n",
            "Precision:0.02832618025751073\n",
            "Accuracy: 0.0068669527896995704\n",
            "recall: 0.0068669527896995704\n",
            "{'amazon echo  before alexa': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 12}, 'amazon prime video  hunters': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'audi  let it go [t1]': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 9}, 'avocados from mexico  the avocados from mexico shopping network': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 5}, 'bud light seltzer  posty store  inside post s brain': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 10}, 'budweiser  typical american': {'precision': 1.0, 'recall': 0.14285714285714285, 'f1-score': 0.25, 'support': 7}, 'cheetos  can t touch this': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 7}, 'coca-cola energy  show up': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 5}, 'discover card  yes we’re accepted': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'disney+  it s time': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'donald j. trump for president  criminal justice reform': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 84}, 'doritos  the cool ranch': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 29}, 'facebook  ready to rock?': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'fast & furious 9  trailer': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0}, 'fox  chosen  lego masters': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1}, 'fox  halftime show  teaser_1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 63}, 'fox  super monday': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 3}, 'fox  toads  the masked singer': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 4}, 'fox nation  breaking news': {'precision': 1.0, 'recall': 0.16666666666666666, 'f1-score': 0.2857142857142857, 'support': 6}, 'genesis  going away party': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 3}, 'google assistant  loretta': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 14}, 'hard rock hotels & casinos  bling cup': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 3}, 'heinz ketchup  find the goodness  four at once': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'hulu  tom brady s big announcement': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 32}, 'hummer  gmc  quiet revolution': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 5}, 'hyundai  smaht pahk': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 27}, 'jeep  groundhog day [t1]': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 11}, 'kia  tough never quits': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 4}, 'little caesars pizza  best thing since sliced bread': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'marvel  black widow trailer': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'michelob  6 for 6-pack': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'michelob  jimmy works it out': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 5}, 'microsoft surface  be the one': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 4}, 'minions  the rise of gru  trailer': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0}, 'mountain dew': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 5}, 'new york life  love takes action': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'nfl  building a better game': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 7}, 'nfl  inspire change  anquan boldin': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'no time to die  trailer': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'none': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 629}, 'olay  make space for women': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 19}, 'pepsi zero sugar  zero sugar. done right.': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 7}, 'planters  baby funeral': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 20}, 'pop-tarts  pop-tarts fixed the pretzel commercial': {'precision': 1.0, 'recall': 0.3333333333333333, 'f1-score': 0.5, 'support': 3}, 'porsche  the heist': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'premier boxing champions  wilder vs. fury ii_2': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0}, 'pringles  the infinite dimensions of rick and morty': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 12}, 'procter & gamble  when we come together': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'reese s  rock': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'rocket mortgage  home': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 32}, 'sabra  how do you  mmus?': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 6}, 'snickers  fix the world': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 4}, 't-mobile  mama tests 5g': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 12}, 'tide  bud knight': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 4}, 'tide  finally later': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 7}, 'tide  when is later  masked singer': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0}, 'tide  ww': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0}, 'toyota  heroes': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'turbotax  turbotax  all people are tax people remix': {'precision': 1.0, 'recall': 0.25, 'f1-score': 0.4, 'support': 12}, 'verizon  the amazing things 5g won t do': {'precision': 1.0, 'recall': 0.25, 'f1-score': 0.4, 'support': 4}, 'walmart  famous visitors': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 7}, 'weathertech  lucky dog': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 5}, 'accuracy': 0.0068669527896995704, 'macro avg': {'precision': 0.0967741935483871, 'recall': 0.034562211981566816, 'f1-score': 0.04573732718894009, 'support': 1165}, 'weighted avg': {'precision': 0.02832618025751073, 'recall': 0.0068669527896995704, 'f1-score': 0.010613120784794606, 'support': 1165}}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Loovr8e2xY4"
      },
      "source": [
        "from sentence_transformers import evaluation\n",
        "sentences1 = ['This list contains the first column. With your sentences', 'With your sentences', 'You want your model to evaluate on']\n",
        "sentences2 = ['Sentences contains the other column', 'The evaluator matches sentences1[i] with sentences2[i]', 'Compute the cosine similarity and compares it to scores[i]']\n",
        "scores = [0.3, 0.6, 0.2]\n",
        "\n",
        "evaluator = evaluation.EmbeddingSimilarityEvaluator(sentences1, sentences2, scores)\n",
        "#evaluator = EmbeddingSimilarityEvaluator.from_input_examples(sts_reader.get_examples('sts-dev.csv'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quFiqB42bbjG"
      },
      "source": [
        "# ax+by+cz \n",
        "\n",
        "# a1x+b1y+c1z \n",
        "\n",
        "# Actual similarity = 0.3\n",
        "\n",
        "# BERT - 3000 words\n",
        "\n",
        "# If similarity is computed directly, the ad vector might not be in the vicinty of ad key word vector, \n",
        "# hence giving less similarity score\n",
        "\n",
        "# Issues:\n",
        "# RHS - Not a full sentence: \n",
        "\n",
        "# todo:\n",
        "# For fine Tuning have similarity scores as 0 if not ad related , 1 if ad related\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8t7yOy4Vbeoe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}