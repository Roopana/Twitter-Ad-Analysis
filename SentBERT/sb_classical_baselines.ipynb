{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sb_classical_baselines.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHJ_HDnoCsy9"
      },
      "source": [
        "# https://www.kdnuggets.com/2018/11/multi-class-text-classification-model-comparison-selection.html/2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJFmeCBOPtKV"
      },
      "source": [
        "# Code to perform classification using classical ML models\n",
        "# Naive Bayes\n",
        "# SGDClassifier\n",
        "# Logistic Regression\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRziP-lnT0J7",
        "outputId": "dc30b504-3434-4ea6-c73e-187cfc358261"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "import gensim\n",
        "import nltk\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print('done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Q6N186FUTaW",
        "outputId": "46cf2359-612b-4e8c-9083-c173e5ea8891"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df_2 = pd.read_csv(\"./2_man_ann_sb.csv\",  index_col= None)\n",
        "df_2 = df_2.dropna(subset = ['tweet_text'])\n",
        "\n",
        "df_1 = pd.read_csv(\"./mann_ann_sb.csv\", index_col= None)\n",
        "\n",
        "df_3 = pd.read_csv(\"./3_man_ann_sb_full_1.csv\", index_col = None)\n",
        "df_3 = df_3.dropna(subset = ['tweet_text'])\n",
        "\n",
        "df_raw = df_1.append(df_2).append(df_3) # using batch 1 and batch 2 for training\n",
        "df = df_raw\n",
        "df_raw.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7500, 19)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef_TXhb_bCbU"
      },
      "source": [
        "Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkJbRGL8Ubca",
        "outputId": "21c64544-e99f-4ce5-c87c-20bd08ffa893"
      },
      "source": [
        "# Remove ads marked as below because they are not available in ad annotations file although tweets mention them\n",
        "# commercials, joe biden, pizzahut, joe bieden, michael bloomberg, mike bloomberg, scientology...\n",
        "ads_remove = ['commercials', 'joe biden', 'pizzahut', 'joe bieden', 'michael bloomberg', 'mike bloomberg', \n",
        "              'scientology','papa johns',  'bakari',  'secret', 'dashlane', 'bernie the peoples perfume',\n",
        "              'ram trucks', 'golden gronks', \"bush's best\", 'ragged old flag', 'patience', 'guitar hero',\n",
        "              'disney mulan']\n",
        "\n",
        "# ads_rename = ['nfl100', 'tide']\n",
        "\n",
        "# rename ads with spelling faults while manually adding the annotation \n",
        "\n",
        "df_raw['ad_manual_adjusted'] = df_raw['ad_manual_adjusted'].apply(lambda x: x.lower())\n",
        "df_raw.loc[df_raw.ad_manual_adjusted == \"discover card  no we don‚äôt charge annual fees\", \n",
        "       \"ad_manual_adjusted\"] = \"discover card  no we don’t charge annual fees\"\n",
        "df_raw.loc[df_raw.ad_manual_adjusted == \"doritos the cool ranch\", \n",
        "       \"ad_manual_adjusted\"] = \"doritos  the cool ranch\"\n",
        "df_raw.loc[df_raw.ad_manual_adjusted == \"discover card yes we're accepted\", \n",
        "       \"ad_manual_adjusted\"] =  \"discover card  yes we’re accepted\"\n",
        "df_raw.loc[df_raw.ad_manual_adjusted == \"discover card yes we’re accepted\", \n",
        "       \"ad_manual_adjusted\"] =  \"discover card  yes we’re accepted\"\n",
        "df_raw.loc[df_raw.ad_manual_adjusted == \"discover card  yes we're accepted\", \n",
        "       \"ad_manual_adjusted\"] =  \"discover card  yes we’re accepted\"\n",
        "df_raw.loc[df_raw.ad_manual_adjusted == \"budweiser typical american\", \n",
        "       \"ad_manual_adjusted\"] = \"budweiser  typical american\"\n",
        "df_raw.loc[df_raw.ad_manual_adjusted == 'fox  halftime show  teaser_3',\n",
        "            \"ad_manual_adjusted\"] = \"fox  halftime show  teaser_1\"\n",
        "df_raw.loc[df_raw.ad_manual_adjusted == 'fox  halftime show  teaser_2',\n",
        "            \"ad_manual_adjusted\"] = \"fox  halftime show  teaser_1\"\n",
        "            \n",
        "print(\"df_raw: \" + str(df_raw.shape))\n",
        "df = pd.DataFrame()\n",
        "removed_Data = pd.DataFrame()\n",
        "\n",
        "for i,row  in df_raw.iterrows():\n",
        "  if row['ad_manual_adjusted'] not in ads_remove:\n",
        "    df = df.append(row)\n",
        "  else:\n",
        "    removed_Data = removed_Data.append(row)\n",
        "\n",
        "print(\"df: \" + str(df.shape))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df_raw: (7500, 19)\n",
            "df: (7394, 19)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVcqsxnrUh5b",
        "outputId": "a11b47c9-bdc6-49ef-a8ae-42d940afbbdf"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# stop = stopwords.words('english')\n",
        "stop = []\n",
        "stop.append('superbowl')\n",
        "stop.append('super')\n",
        "stop.append('bowl')\n",
        "stop.append('commercial')\n",
        "stop.append('ad')\n",
        "stop.append('commercials')\n",
        "stop.append('ads')\n",
        "\n",
        "print(len(stop))\n",
        "\n",
        "def removeMentions(text):\n",
        "\n",
        "    textBeforeMention = text.partition(\"@\")[0]\n",
        "    textAfterMention = text.partition(\"@\")[2]\n",
        "    textAfterMention =  re.sub(r':', '', textAfterMention) #cadillac join the 31k\n",
        "    tHandle = textAfterMention.partition(\" \")[0].lower() #cadillac    \n",
        "    text = textBeforeMention+ \" \" + textAfterMention  \n",
        "    return text\n",
        "\n",
        "def cleanTweet(strinp):\n",
        "    strinp = re.sub(r'RT', \"\", strinp) # Remove RT\n",
        "    strinp = strinp.lower()\n",
        "    \n",
        "    stop_removed_list = [word for word in strinp.split() if word not in (stop)]\n",
        "    stop_removed = ' '.join([str(elem) for elem in stop_removed_list])    \n",
        "    text = re.sub('https?://[A-Za-z0-9./]+', ' ', stop_removed) # Remove URLs\n",
        "    text = removeMentions(text)\n",
        "    text = re.sub('[^\\x00-\\x7F]+', ' ', text) # Remove non-ASCII chars.\n",
        "    \n",
        "    # remove punctuations except '-'\n",
        "    punctuation = ['(', ')', '[',']','?', ':', ':', ',', '.', '!', '/', '\"', \"'\", '@', '#', '&', '-', '_']\n",
        "    text = \"\".join((char for char in text if char not in punctuation))\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text) # remove all other than alphabet chars \n",
        "\n",
        "#     text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text) # remove all single characters     \n",
        "    stop_removed_l = [word for word in text.split() if word not in (stop)]\n",
        "    stop_removed = ' '.join([str(elem) for elem in stop_removed_l]) \n",
        "    return stop_removed\n",
        "\n",
        "print(cleanTweet(\"RT @cadillacabc: Joinrt the 31K james_bond\") )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "7\n",
            "cadillacabc joinrt the k jamesbond\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKqu20-F-BSQ"
      },
      "source": [
        "df['text_clean'] = df['tweet_text'].apply(lambda x: cleanTweet(x))\n",
        "df['ad_manual_adjusted'] = df['ad_manual_adjusted'].apply(lambda x: x.lower())\n",
        "df['ad_related'] = df['ad_manual_adjusted'].apply(lambda ad: 0 if ad == 'none' else 1)\n",
        "\n",
        "comma_filter = ~df['ad_manual_adjusted'].str.contains(',')\n",
        "df = df[comma_filter]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEz_c5P5JvXd",
        "outputId": "b9bbeaf3-869f-44da-e0ef-913fb76807f4"
      },
      "source": [
        "df_unique = df.drop_duplicates(subset = ['text_clean'])\n",
        "df_with_dupes = df\n",
        "df = df_unique\n",
        "\n",
        "print(df_with_dupes.shape)\n",
        "print(df.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7332, 21)\n",
            "(5845, 21)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a9z2gxCaRSG",
        "outputId": "9ffc692c-d630-4b51-fb8b-1fc34ec71abc"
      },
      "source": [
        "ad_product_df = pd.read_csv('./SB_ad_annotations_product_category_modified.csv')\n",
        "ad_product_df = ad_product_df.rename(columns = {'Ad Name': 'Ad_Name'}) # rename the column to remove space\n",
        "ad_product_df = ad_product_df.dropna() # because the file has trailing empty rows, remove them\n",
        "# remove fox half time show teaser_2 ad because its keywords is same as fox half time show teaser_2\n",
        "print(ad_product_df.shape)\n",
        "ad_product_df.drop(ad_product_df[ad_product_df['Ad_Name'] == 'FOX  Halftime Show  Teaser_2'].index, inplace = True) \n",
        "print(ad_product_df.shape)\n",
        "ad_product_dict = dict()\n",
        "\n",
        "for i, row in ad_product_df.iterrows():\n",
        "  ad_product_dict[row['Ad_Name'].lower()] = row['Product_modified'].lower()\n",
        "\n",
        "ad_product_dict['none'] = 'none'\n",
        "\n",
        "print(ad_product_dict)\n",
        "\n",
        "df['product_modified'] = df['ad_manual_adjusted'].apply(lambda ad: ad_product_dict[ad])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(75, 11)\n",
            "(74, 11)\n",
            "{'fast & furious 9  trailer': 'movie trailer', 'quibi  bank heist': 'tech company', 'tide  when is later  masked singer': 'laundry detergent', 'fox  a run at history  daytona 500': 'sporting event', 'donald j. trump for president  criminal justice reform': 'political campaign', 'walmart  famous visitors': 'tech company', 'marvel  black widow trailer': 'movie trailer', 'rocket mortgage  home': 'money', 'porsche  the heist': 'car', 'snickers  fix the world': 'food', 'hulu  tom brady s big announcement': 'streaming service', 'fox  chosen  lego masters': 'tv show/network', 'mountain dew': 'pop/soda', 'squarespace  winona in winona': 'tech company', 'new york life  love takes action': 'money', 'fox  super monday': 'tv show/network', 'hyundai  smaht pahk': 'car', 'cheetos  can t touch this': 'food', 'olay  make space for women': 'charitable program', 'fox  halftime show  teaser_1': 'half-time show', 'michelob  6 for 6-pack': 'charitable program/ alcohol', 'avocados from mexico  the avocados from mexico shopping network': 'food', 'hard rock hotels & casinos  bling cup': 'hotel', 'pringles  the infinite dimensions of rick and morty': 'food', 'turbotax  turbotax  all people are tax people remix': 'money', 'tide  bud knight': 'laundry detergent', 'genesis  going away party': 'car', 'coca-cola energy  show up': 'soda', 'planters  baby funeral': 'food', 'no time to die  trailer': 'movie trailer', 'fox  toads  the masked singer': 'tv show/network', 'google assistant  loretta': 'tech company', 'sabra  how do you  mmus?': 'food', 'weathertech  lucky dog': 'charitable program', 'verizon  the amazing things 5g won t do': 'phone network', 'hummer  gmc  quiet revolution': 'car', 'pop-tarts  pop-tarts fixed the pretzel commercial': 'food', 'minions  the rise of gru  trailer': 'movie trailer', 'fox nation  breaking news': 'tv show/network', 'premier boxing champions  wilder vs. fury ii_1': 'sporting event', 'homeland  trailer': 'tv show/network', 'fox  great american race': 'sporting event', 'amazon prime video  hunters': 'tv show/network', 'pepsi zero sugar  zero sugar. done right.': 'pop/soda', 'heinz ketchup  find the goodness  four at once': 'food', 'premier boxing champions  wilder vs. fury ii_2': 'sporting event', 'bud light seltzer  posty store  inside post s brain': 'alcohol', 'little caesars pizza  best thing since sliced bread': 'food', 'doritos  the cool ranch': 'food', 'kia  tough never quits': 'car', 'turkish airlines  step on earth': 'airline', 'reese s  rock': 'food', 'tide  ww': 'laundry detergent', 'amazon echo  before alexa': 'tech company', 'michelob  jimmy works it out': 'alcohol', 'nfl  inspire change  anquan boldin': 'charitable program', 'toyota  heroes': 'car', 'discover card  no we don’t charge annual fees': 'money', 'disney+  it s time': 'streaming service', 'discover card  yes we’re accepted': 'money', 'fox  football withdrawal syndrome': 'sporting event', 't-mobile  mama tests 5g': 'phone network', 'fox  tornado  tomorrow': 'tv show/network', 'fox  fun for all  the masked singer and lego': 'tv show/network', 'nfl  building a better game': 'charitable program', 'budweiser  typical american': 'alcohol', 'procter & gamble  when we come together': 'tech company', 'fox  not just another race': 'sporting event', 'microsoft surface  be the one': 'tech company', 'fox  i m scared': 'tv show/network', 'jeep  groundhog day [t1]': 'car', 'facebook  ready to rock?': 'social network', 'tide  finally later': 'laundry detergent', 'audi  let it go [t1]': 'car', 'none': 'none'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sADim9FH0HyI"
      },
      "source": [
        "classification_ad_product = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "un3IG7tzfaGl",
        "outputId": "188fa0c8-0a8c-4f9b-ce72-0d67bb1715f0"
      },
      "source": [
        "product_id_dict = dict()\n",
        "products = df['product_modified'].unique()\n",
        "i=0\n",
        "\n",
        "for product in products:\n",
        "  product_id_dict[product] = i\n",
        "  i = i+1\n",
        "\n",
        "n_unique_ad_produts = len(product_id_dict)\n",
        "\n",
        "if classification_ad_product:\n",
        "  n_unique_ads = n_unique_ad_produts\n",
        "  print(n_unique_ads)\n",
        "\n",
        "df['ad_product_id'] = df['product_modified'].apply(lambda x: product_id_dict[x])\n",
        "\n",
        "print(\"Size of dictionary:\"+ str(len(product_id_dict)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of dictionary:20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3LsRqs1IycD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e76f7ea5-d091-4cb4-f975-9ef20bcf8cc5"
      },
      "source": [
        "# Remove ads with less than 2 samples\n",
        "\n",
        "def getAdTweets(ad_related_twts, ad):\n",
        "  return ad_related_twts[ad_related_twts.ad_manual_adjusted == ad].shape[0]\n",
        "\n",
        "def get_filtered_data(df): \n",
        "  unique_ads = df['ad_manual_adjusted'].unique()\n",
        "  #print(unique_ads)\n",
        "  for ad in unique_ads : \n",
        "    if getAdTweets(df, ad) >=2:\n",
        "        # do nothing\n",
        "        print(\"\")\n",
        "    else:\n",
        "      # print('ad with <2 samples: '+ str(ad))\n",
        "      mask = df['ad_manual_adjusted'] != ad\n",
        "      df = df[mask]\n",
        "  return df\n",
        "\n",
        "df = get_filtered_data(df)\n",
        "print('Removed tweets for ads with <2 samples')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Removed tweets for ads with <2 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpElBhlJe50k"
      },
      "source": [
        "# #Let us try some sampling technique to remove class imbalance\n",
        "# from imblearn.over_sampling import SMOTE\n",
        "# #Over-sampling: SMOTE\n",
        "# #SMOTE (Synthetic Minority Oversampling TEchnique) consists of synthesizing elements for the minority class, \n",
        "# #based on those that already exist. It works randomly picking a point from the minority class and computing \n",
        "# #the k-nearest neighbors for this point.The synthetic points are added between the chosen point and its neighbors.\n",
        "# #We'll use ratio='minority' to resample the minority class.\n",
        "# smote = SMOTE('minority', k_neighbors = 3) # default value of n_neighbours = 6, but getting error since for a class n samples = 4 < default n_neighs which is 6\n",
        "\n",
        "# input_data_raw = df['text_clean']\n",
        "# input_labels_raw = df['ad_product_id']\n",
        "# input_data, input_labels = smote.fit_sample(input_data_raw, input_labels_raw)\n",
        "# print(input_data.shape, input_labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzYyzwOjHUkf"
      },
      "source": [
        "subset_ads = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrVzcvFXhtSY"
      },
      "source": [
        "def filter_ads(df):\n",
        "  ads_needed = ['donald j. trump for president  criminal justice reform', 'hulu  tom brady s big announcement',\n",
        "              'doritos  the cool ranch','olay  make space for women','planters  baby funeral',\n",
        "              'amazon echo  before alexa', 'jeep  groundhog day [t1]'\n",
        "              ,'none' # top 7\n",
        "              # 'walmart  famous visitors',  'bud light seltzer  posty store  inside post s brain' \n",
        "              # ,'rocket mortgage  home', 'hyundai  smaht pahk', 'google assistant  loretta',\n",
        "              # 'turbotax  turbotax  all people are tax people remix', 'jeep  groundhog day [t1]',\n",
        "              # 'pringles  the infinite dimensions of rick and morty',\n",
        "              # , 'audi  let it go [t1]' # top 15\n",
        "              # ,'fox  halftime show  teaser_1', 'cheetos  can t touch this', 'fox  toads  the masked singer',\n",
        "              # 'nfl  building a better game', 'snickers  fix the world', 't-mobile  mama tests 5g',\n",
        "              # 'avocados from mexico  the avocados from mexico shopping network', 'budweiser  typical american',\n",
        "              # 'coca-cola energy  show up', 'microsoft surface  be the one', 'sabra  how do you  mmus?',\n",
        "              # 'verizon  the amazing things 5g won t do', 'genesis  going away party', 'mountain dew' #top 30\n",
        "                ]\n",
        "  print(len(ads_needed))\n",
        "  mask = df['ad_manual_adjusted'].apply(lambda ad: ad in ads_needed)\n",
        "  df_subset = df[mask]\n",
        "  return df_subset\n",
        "\n",
        "if subset_ads == True:\n",
        "  df_subset = filter_ads(df)\n",
        "  print(df_subset.shape)\n",
        "  df_subset.head()\n",
        "  df_all = df\n",
        "  df = df_subset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEDV40A4kEEf"
      },
      "source": [
        "# df = df_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPjhoHeWiOzR"
      },
      "source": [
        "df[['ad_manual_adjusted', 'text_clean']].head()\n",
        "count = df[['ad_manual_adjusted', 'text_clean']].groupby('ad_manual_adjusted').count()\n",
        "count.head()\n",
        "count.to_csv('./train_data_support.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_WxJ3bt3vyX",
        "outputId": "f28dd9da-7214-45c7-88e4-eea7e7921d6b"
      },
      "source": [
        "# use column name 'ad_manual_adjusted' of df to \n",
        "def get_ad_related_twts(df, removeCommas = True):\n",
        "  df['ad_manual_adjusted'] = df['ad_manual_adjusted'].apply(lambda x: x.lower())\n",
        "  ad_filter = df['ad_manual_adjusted']!= 'none'\n",
        "  ad_related_twts = df[ad_filter]\n",
        "  if removeCommas:\n",
        "    ad_filter_1 = ~ad_related_twts['ad_manual_adjusted'].str.contains(',')\n",
        "    ad_related_twts = ad_related_twts[ad_filter_1]\n",
        "  return ad_related_twts\n",
        "\n",
        "def getAdTweets(ad_related_twts, ad):\n",
        "  return ad_related_twts[ad_related_twts.ad_manual_adjusted == ad].shape[0]\n",
        "\n",
        "def get_ad_id_dict(ad_related_twts): \n",
        "  n_ad_related = ad_related_twts.shape[0]\n",
        "  print(\"# ad related tweets: \"+ str(n_ad_related))\n",
        "  ads_annotated = ad_related_twts.ad_manual_adjusted.values\n",
        "  adset = set(ads_annotated)\n",
        "  print(\"unique ads:\"+ str(len(adset)))\n",
        "  ad_id_dict = {}\n",
        "  i = 0\n",
        "  for ad in adset : \n",
        "    if(getAdTweets(ad_related_twts, ad) >=2):\n",
        "      ad_id_dict[ad] = i\n",
        "      i = i+1\n",
        "    else:\n",
        "      print('ad with <2 samples: '+ str(ad))\n",
        "  print(\" No of ads with >=2 samples:\"+ str(len(ad_id_dict)))\n",
        "  ad_id_dict['none'] = len(ad_id_dict)\n",
        "  print(ad_id_dict)\n",
        "  return ad_id_dict\n",
        "\n",
        "def convertAdNameToAdId(ad_id_dict, ad_name):\n",
        "  if ad_name in ad_id_dict:\n",
        "    return ad_id_dict[ad_name]\n",
        "  else:\n",
        "    return ad_id_dict['none']\n",
        "\n",
        "ad_related_twts = get_ad_related_twts(df)\n",
        "ad_id_dict = get_ad_id_dict(get_ad_related_twts(df))\n",
        "n_unique_ads = len(ad_id_dict) # ad_id_dict has none as well, so minus 1 when using embeddings\n",
        "df['ad_manual_adjusted_id'] = df['ad_manual_adjusted'].apply(lambda x: convertAdNameToAdId(ad_id_dict,x))\n",
        "print(n_unique_ads)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# ad related tweets: 2607\n",
            "unique ads:60\n",
            " No of ads with >=2 samples:60\n",
            "{'fox  toads  the masked singer': 0, 'michelob  jimmy works it out': 1, 'hyundai  smaht pahk': 2, 'jeep  groundhog day [t1]': 3, 'heinz ketchup  find the goodness  four at once': 4, 'hummer  gmc  quiet revolution': 5, 'google assistant  loretta': 6, 'genesis  going away party': 7, 'marvel  black widow trailer': 8, 'turbotax  turbotax  all people are tax people remix': 9, 'reese s  rock': 10, 'microsoft surface  be the one': 11, 'rocket mortgage  home': 12, 'fox  a run at history  daytona 500': 13, 'sabra  how do you  mmus?': 14, 'nfl  inspire change  anquan boldin': 15, 'fox  chosen  lego masters': 16, 'budweiser  typical american': 17, 'little caesars pizza  best thing since sliced bread': 18, 'facebook  ready to rock?': 19, 'squarespace  winona in winona': 20, 'fox  great american race': 21, 'snickers  fix the world': 22, 'amazon prime video  hunters': 23, 'weathertech  lucky dog': 24, 'amazon echo  before alexa': 25, 'pop-tarts  pop-tarts fixed the pretzel commercial': 26, 'porsche  the heist': 27, 'walmart  famous visitors': 28, 'pepsi zero sugar  zero sugar. done right.': 29, 'cheetos  can t touch this': 30, 'kia  tough never quits': 31, 'procter & gamble  when we come together': 32, 'nfl  building a better game': 33, 'olay  make space for women': 34, 'fox  super monday': 35, 'discover card  yes we’re accepted': 36, 'michelob  6 for 6-pack': 37, 't-mobile  mama tests 5g': 38, 'quibi  bank heist': 39, 'hard rock hotels & casinos  bling cup': 40, 'bud light seltzer  posty store  inside post s brain': 41, 'no time to die  trailer': 42, 'coca-cola energy  show up': 43, 'hulu  tom brady s big announcement': 44, 'mountain dew': 45, 'donald j. trump for president  criminal justice reform': 46, 'verizon  the amazing things 5g won t do': 47, 'disney+  it s time': 48, 'toyota  heroes': 49, 'avocados from mexico  the avocados from mexico shopping network': 50, 'planters  baby funeral': 51, 'tide  finally later': 52, 'fox  halftime show  teaser_1': 53, 'audi  let it go [t1]': 54, 'pringles  the infinite dimensions of rick and morty': 55, 'doritos  the cool ranch': 56, 'new york life  love takes action': 57, 'tide  bud knight': 58, 'fox nation  breaking news': 59, 'none': 60}\n",
            "61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqTgz2Li0z2Y"
      },
      "source": [
        "classification_ad_product = False # enable this flag if labels are ad products(i.e buckets of ad names) instead of ad names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKbiryXY0uv6",
        "outputId": "c629a88f-02db-4ad1-d1fb-29d7510dd192"
      },
      "source": [
        "if classification_ad_product:\n",
        "  twt_train, twt_test, label_train, label_test = train_test_split(df['text_clean'], \n",
        "                                                      df['product_modified'], test_size=0.2, random_state=42\n",
        "                                                      , stratify= df.ad_manual_adjusted.values)\n",
        "  unique_ads = df['product_modified'].unique()\n",
        "  print(\"No of Unique ads:\"+ str(len(unique_ads)))\n",
        "else:\n",
        "  twt_train, twt_test, label_train, label_test = train_test_split(df['text_clean'], \n",
        "                                                      df['ad_manual_adjusted'], test_size=0.2, random_state=42\n",
        "                                                      , stratify= df.ad_manual_adjusted_id.values)\n",
        "  unique_ads = df['ad_manual_adjusted'].unique()\n",
        "  print(\"No of Unique ads:\"+ str(len(unique_ads)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No of Unique ads:61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qErXEhgW0ck9"
      },
      "source": [
        "#### Exploding sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnUAdIoh0X0d"
      },
      "source": [
        "sentence_explode = True\n",
        "sentences, test_sentences, labels, test_labels = twt_train, twt_test, label_train, label_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsm1Bs3O2Scq",
        "outputId": "1d60a9ac-bd4d-4105-d46d-05f5f5d950df"
      },
      "source": [
        "# group the keywords of ads part of a product bucket and append these to the training sentences\n",
        "ad_product_keywords_dict = ad_product_df.groupby('Product_modified')['Key Terms  Round 2'].agg(lambda x : x.sum() if x.dtype=='float64' else ' '.join(x))\n",
        "# clean the ad key words - not removing duplicate words here - TODO\n",
        "for ad_bucket in ad_product_keywords_dict.keys():\n",
        "  ad_product_keywords_dict[ad_bucket] = cleanTweet(ad_product_keywords_dict[ad_bucket])\n",
        "ad_product_df['product_modified_keywords'] = ad_product_df['Product_modified'].apply(lambda x: ad_product_keywords_dict[x])\n",
        "ad_product_df.head(2)\n",
        "\n",
        "# generate adname, ad keywords dict to use in sent exploding\n",
        "ad_name_keywords_dict = dict()\n",
        "for i, row in ad_product_df.iterrows():\n",
        "  ad_name_keywords_dict[row['Ad_Name'].lower()] = cleanTweet(row['Key Terms  Round 2'])\n",
        "ad_name_keywords_dict['none'] = 'none'\n",
        "print(ad_name_keywords_dict)\n",
        "\n",
        "ad_keywords_name_dict = dict()\n",
        "for ad_name in ad_name_keywords_dict:\n",
        "  keywords_temp = ad_name_keywords_dict[ad_name]\n",
        "  ad_keywords_name_dict[keywords_temp] = ad_name\n",
        "print(ad_keywords_name_dict)\n",
        "\n",
        "ad_product_df['ad_name_keywords'] = ad_product_df['Ad_Name'].apply(lambda x: ad_name_keywords_dict[x.lower()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'fast & furious 9  trailer': 'fast and the furious fast the furious ff f the fast saga vin diesel flying truck stunts michelle rodriguez fastfurious', 'quibi  bank heist': 'quibi bank heist robbery less than ten minutes quick bites big stories chance the rapper megan thee stallion chancetherappermeganstallion quickbites', 'tide  when is later  masked singer': 'tide laundry laundry detergent schitt emily hampshire charlie day walts', 'fox  a run at history  daytona 500': 'fox show daytona a run at history great american race', 'donald j. trump for president  criminal justice reform': 'donald trump trump change unemployment stronger safer prosperous trumpforpresident donaldtrump presidentrumpobama gop trumpsupporters', 'walmart  famous visitors': 'walmart pickup spaceship toy story buzz lightyear marvin the martian marvin martians arrival glass cleaners aliens men in black groot lego star wars r d bill bill and ted flash gordon', 'marvel  black widow trailer': 'marvel black widow scarlett johansson', 'rocket mortgage  home': 'quicken loans aquaman jason momoa rocket mortgagemomoabowl jasonmomoa rocketmorgage', 'porsche  the heist': 'porsche volkswagen the chase fast and furious taycan turbo scar chase taycanturbo', 'snickers  fix the world': 'snickers snickersfixtheworld fix the world snickershole fixtheworld', 'hulu  tom brady s big announcement': 'hulu tom brady tombrady', 'fox  chosen  lego masters': 'fox chosen lego masters competition will arnett', 'mountain dew': 'mountain dew mtn dew soda zero sugar the shining all work and no play makes jack a dull boy new mtn dew zero sugarbryan cranston mtndew theshining', 'squarespace  winona in winona': 'squarespace winona winona ryder minnesota', 'new york life  love takes action': 'love takes action greek words for love agape new york life take action lovetakesaction be good at life nyl newyorklife', 'fox  super monday': 'fox monday the office toby toby flenderson', 'hyundai  smaht pahk': 'chris evans captain america john krasinki rachel dratch boston red sox david ortiz boston accent smart parking hyundai a better way to park bettahdriveus chrisevans smahtpark bettahwaytopark', 'cheetos  can t touch this': 'mc hammer cant touch this popcorn cheetos its a cheetos thing cheetle canttouchthismchammer', 'olay  make space for women': 'allfemale spacewalk lilly singh busy phillips nicole stott taraji henson katie couric olay make space for women makespaceforwomen olay', 'fox  halftime show  teaser_1': 'fox halftime show j lo jennifer lopez shakira', 'michelob  6 for 6-pack': 'help the farmers organic pack square footmake your impact michelob gold michelobgold for pack sixforsixpack', 'avocados from mexico  the avocados from mexico shopping network': 'molly ringwald avocados mexico avonetwork avocarriermollyringwald', 'hard rock hotels & casinos  bling cup': 'hard rock international hard rock stadium miami kobe bryant jennifer lopez jlo hardrock hardrockstadium hardrockcasino', 'pringles  the infinite dimensions of rick and morty': 'rick and morty adult swim pringles pickle rick rick morty new flavor summer rickmorty adultswim', 'turbotax  turbotax  all people are tax people remix': 'turbotax all people app mobile feature taxes tax people all people are tax people allpeoplearetaxpeople', 'tide  bud knight': 'tide laundry laundry detergent charliey day bud knight stainlaundrydetergent charlieday', 'genesis  going away party': 'john legend chrissy teigen genesis hyundai kobe bryant young luxury gv sexiest man alive johnlegendchrissyteigen', 'coca-cola energy  show up': 'coke iphone coca cola coca cola energy showup the dotscococolacocacolaenergy jonah hill martin scorsese', 'planters  baby funeral': 'planters nutsmrpeanut matt walsh wesley snipesbabynut babypeanut babyfuneral baby nut', 'no time to die  trailer': 'james bond no time to die daniel craig', 'fox  toads  the masked singer': 'fox the masked singer toads', 'google assistant  loretta': 'google assistant virtual assistant googleassistant virtualassistant', 'sabra  how do you  mmus?': 'sabra humus hummus wives real housewives of new jersey real housewives caroline manzo teresa giudice tpain mix cracker kim chi drag queensdragqueens dragqueenamanda cerny howimmus', 'weathertech  lucky dog': 'weathertech pets golden retriever dog', 'verizon  the amazing things 5g won t do': 'verizon phone service phone g', 'hummer  gmc  quiet revolution': 'general motorshummer hummer ev electric pickup zero emission lebron james basketball lebronjames', 'pop-tarts  pop-tarts fixed the pretzel commercial': 'pop tarts queer eye jonathan van ness pretzel queereye poptarts', 'minions  the rise of gru  trailer': 'minions the rise of gru', 'fox nation  breaking news': 'fox fox nation breaking news', 'premier boxing champions  wilder vs. fury ii_1': 'premier boxing champions wilder vs fury ii', 'homeland  trailer': 'homeland trailer', 'fox  great american race': 'fox daytona great american race run at history', 'amazon prime video  hunters': 'hunters nazi al pacino nazi hunters prime video amazon barbeque barbecue alpacinoprimevideo hunterstv', 'pepsi zero sugar  zero sugar. done right.': 'missy elliott her pepsi zero sugar paint it black pepsi pepsizero nocoke', 'heinz ketchup  find the goodness  four at once': 'find the goodness heinz ketchup stranger things strangerthings findthegoodness', 'premier boxing champions  wilder vs. fury ii_2': 'premier boxing champions wilder vs fury ii wrestling', 'bud light seltzer  posty store  inside post s brain': 'bud light bud light seltzer post malone anheuserbusch inbev hard seltzer postmalone budlightbudweiser', 'little caesars pizza  best thing since sliced bread': 'the office rainn wilson delivery best thing since sliced bread pizza pizza pizza littlecaesars caesars', 'doritos  the cool ranch': 'sam elliott lil nas x old town road doritos cowboy cool ranch dancer billy ray cyrus wild west wild wild west make your move', 'kia  tough never quits': 'josh jacobs running back kia kia seltosraiders give it everything joshjacobs kiaseltos', 'turkish airlines  step on earth': 'turkish airlines airline flight astronauts rocket', 'reese s  rock': 'reese take five take candybarreeses', 'tide  ww': 'tide stain later charlie day wonder woman', 'amazon echo  before alexa': 'ellen degeneres alexa porta de rossi maid medieval dragon amazon before alexa middle ages beforealexa', 'michelob  jimmy works it out': 'anheuserbusch inbev michelob ultrabeer jimmy fallon working out gym john cena usain bolt brooks koepka kerri walsh jennings its only worth it if you enjoy it low carbs jimmyfallon usainbolt workingout gymbody', 'nfl  inspire change  anquan boldin': 'inspire change police shootings black men nfl inspirechange', 'toyota  heroes': 'toyota car suv highlander toyotahighlandercobiesmulders cobie smulders', 'discover card  no we don’t charge annual fees': 'pop culture no annual fee discover credit card chandler friends jack black mike myers austin powersnoannualfeediscovercredit yestodiscover', 'disney+  it s time': 'disney plus the falcon wanda vision wanda loki', 'discover card  yes we’re accepted': 'pop culture discover credit card mean girls ted noannualfeediscovercredit nodiscover', 'fox  football withdrawal syndrome': 'football withdrawal system science doctor withdrawal symptoms', 't-mobile  mama tests 5g': 'tmobile mama tests g g anthony anderson', 'fox  tornado  tomorrow': 'fox tornado lone star', 'fox  fun for all  the masked singer and lego': 'fox the masked singer lego masters', 'nfl  building a better game': 'nfl building a better game amazon web services', 'budweiser  typical american': 'kathryn bigelow budweiser american spirit typical american kindness beer stereotypes stereotypical typicalamerican americanspirit', 'procter & gamble  when we come together': 'procter and gamble when we come together sofia vergara chili bounty old spice head and shoulders olay charming febreze', 'fox  not just another race': 'fox daytona great american race run at history', 'microsoft surface  be the one': 'woman coach ers katie sowers surface female coach microsoft first female coach katiesowers erscoach microsoftsurface femalecoach', 'fox  i m scared': 'fox lone star storm tornado', 'jeep  groundhog day [t1]': 'jeep groundhog day jeep gladiator bill murray', 'facebook  ready to rock?': 'chris rock sylvester stallone groups facebook rock music stonehenge rock rocketchrisrock', 'tide  finally later': 'tide tide pod laundry detergent emily hampshire charlie day finally later', 'audi  let it go [t1]': 'maisie williams game of thrones let it go frozenaudi etron sportback traffic letitgo maisiewilliamsgameofthrones', 'none': 'none'}\n",
            "{'fast and the furious fast the furious ff f the fast saga vin diesel flying truck stunts michelle rodriguez fastfurious': 'fast & furious 9  trailer', 'quibi bank heist robbery less than ten minutes quick bites big stories chance the rapper megan thee stallion chancetherappermeganstallion quickbites': 'quibi  bank heist', 'tide laundry laundry detergent schitt emily hampshire charlie day walts': 'tide  when is later  masked singer', 'fox show daytona a run at history great american race': 'fox  a run at history  daytona 500', 'donald trump trump change unemployment stronger safer prosperous trumpforpresident donaldtrump presidentrumpobama gop trumpsupporters': 'donald j. trump for president  criminal justice reform', 'walmart pickup spaceship toy story buzz lightyear marvin the martian marvin martians arrival glass cleaners aliens men in black groot lego star wars r d bill bill and ted flash gordon': 'walmart  famous visitors', 'marvel black widow scarlett johansson': 'marvel  black widow trailer', 'quicken loans aquaman jason momoa rocket mortgagemomoabowl jasonmomoa rocketmorgage': 'rocket mortgage  home', 'porsche volkswagen the chase fast and furious taycan turbo scar chase taycanturbo': 'porsche  the heist', 'snickers snickersfixtheworld fix the world snickershole fixtheworld': 'snickers  fix the world', 'hulu tom brady tombrady': 'hulu  tom brady s big announcement', 'fox chosen lego masters competition will arnett': 'fox  chosen  lego masters', 'mountain dew mtn dew soda zero sugar the shining all work and no play makes jack a dull boy new mtn dew zero sugarbryan cranston mtndew theshining': 'mountain dew', 'squarespace winona winona ryder minnesota': 'squarespace  winona in winona', 'love takes action greek words for love agape new york life take action lovetakesaction be good at life nyl newyorklife': 'new york life  love takes action', 'fox monday the office toby toby flenderson': 'fox  super monday', 'chris evans captain america john krasinki rachel dratch boston red sox david ortiz boston accent smart parking hyundai a better way to park bettahdriveus chrisevans smahtpark bettahwaytopark': 'hyundai  smaht pahk', 'mc hammer cant touch this popcorn cheetos its a cheetos thing cheetle canttouchthismchammer': 'cheetos  can t touch this', 'allfemale spacewalk lilly singh busy phillips nicole stott taraji henson katie couric olay make space for women makespaceforwomen olay': 'olay  make space for women', 'fox halftime show j lo jennifer lopez shakira': 'fox  halftime show  teaser_1', 'help the farmers organic pack square footmake your impact michelob gold michelobgold for pack sixforsixpack': 'michelob  6 for 6-pack', 'molly ringwald avocados mexico avonetwork avocarriermollyringwald': 'avocados from mexico  the avocados from mexico shopping network', 'hard rock international hard rock stadium miami kobe bryant jennifer lopez jlo hardrock hardrockstadium hardrockcasino': 'hard rock hotels & casinos  bling cup', 'rick and morty adult swim pringles pickle rick rick morty new flavor summer rickmorty adultswim': 'pringles  the infinite dimensions of rick and morty', 'turbotax all people app mobile feature taxes tax people all people are tax people allpeoplearetaxpeople': 'turbotax  turbotax  all people are tax people remix', 'tide laundry laundry detergent charliey day bud knight stainlaundrydetergent charlieday': 'tide  bud knight', 'john legend chrissy teigen genesis hyundai kobe bryant young luxury gv sexiest man alive johnlegendchrissyteigen': 'genesis  going away party', 'coke iphone coca cola coca cola energy showup the dotscococolacocacolaenergy jonah hill martin scorsese': 'coca-cola energy  show up', 'planters nutsmrpeanut matt walsh wesley snipesbabynut babypeanut babyfuneral baby nut': 'planters  baby funeral', 'james bond no time to die daniel craig': 'no time to die  trailer', 'fox the masked singer toads': 'fox  toads  the masked singer', 'google assistant virtual assistant googleassistant virtualassistant': 'google assistant  loretta', 'sabra humus hummus wives real housewives of new jersey real housewives caroline manzo teresa giudice tpain mix cracker kim chi drag queensdragqueens dragqueenamanda cerny howimmus': 'sabra  how do you  mmus?', 'weathertech pets golden retriever dog': 'weathertech  lucky dog', 'verizon phone service phone g': 'verizon  the amazing things 5g won t do', 'general motorshummer hummer ev electric pickup zero emission lebron james basketball lebronjames': 'hummer  gmc  quiet revolution', 'pop tarts queer eye jonathan van ness pretzel queereye poptarts': 'pop-tarts  pop-tarts fixed the pretzel commercial', 'minions the rise of gru': 'minions  the rise of gru  trailer', 'fox fox nation breaking news': 'fox nation  breaking news', 'premier boxing champions wilder vs fury ii': 'premier boxing champions  wilder vs. fury ii_1', 'homeland trailer': 'homeland  trailer', 'fox daytona great american race run at history': 'fox  not just another race', 'hunters nazi al pacino nazi hunters prime video amazon barbeque barbecue alpacinoprimevideo hunterstv': 'amazon prime video  hunters', 'missy elliott her pepsi zero sugar paint it black pepsi pepsizero nocoke': 'pepsi zero sugar  zero sugar. done right.', 'find the goodness heinz ketchup stranger things strangerthings findthegoodness': 'heinz ketchup  find the goodness  four at once', 'premier boxing champions wilder vs fury ii wrestling': 'premier boxing champions  wilder vs. fury ii_2', 'bud light bud light seltzer post malone anheuserbusch inbev hard seltzer postmalone budlightbudweiser': 'bud light seltzer  posty store  inside post s brain', 'the office rainn wilson delivery best thing since sliced bread pizza pizza pizza littlecaesars caesars': 'little caesars pizza  best thing since sliced bread', 'sam elliott lil nas x old town road doritos cowboy cool ranch dancer billy ray cyrus wild west wild wild west make your move': 'doritos  the cool ranch', 'josh jacobs running back kia kia seltosraiders give it everything joshjacobs kiaseltos': 'kia  tough never quits', 'turkish airlines airline flight astronauts rocket': 'turkish airlines  step on earth', 'reese take five take candybarreeses': 'reese s  rock', 'tide stain later charlie day wonder woman': 'tide  ww', 'ellen degeneres alexa porta de rossi maid medieval dragon amazon before alexa middle ages beforealexa': 'amazon echo  before alexa', 'anheuserbusch inbev michelob ultrabeer jimmy fallon working out gym john cena usain bolt brooks koepka kerri walsh jennings its only worth it if you enjoy it low carbs jimmyfallon usainbolt workingout gymbody': 'michelob  jimmy works it out', 'inspire change police shootings black men nfl inspirechange': 'nfl  inspire change  anquan boldin', 'toyota car suv highlander toyotahighlandercobiesmulders cobie smulders': 'toyota  heroes', 'pop culture no annual fee discover credit card chandler friends jack black mike myers austin powersnoannualfeediscovercredit yestodiscover': 'discover card  no we don’t charge annual fees', 'disney plus the falcon wanda vision wanda loki': 'disney+  it s time', 'pop culture discover credit card mean girls ted noannualfeediscovercredit nodiscover': 'discover card  yes we’re accepted', 'football withdrawal system science doctor withdrawal symptoms': 'fox  football withdrawal syndrome', 'tmobile mama tests g g anthony anderson': 't-mobile  mama tests 5g', 'fox tornado lone star': 'fox  tornado  tomorrow', 'fox the masked singer lego masters': 'fox  fun for all  the masked singer and lego', 'nfl building a better game amazon web services': 'nfl  building a better game', 'kathryn bigelow budweiser american spirit typical american kindness beer stereotypes stereotypical typicalamerican americanspirit': 'budweiser  typical american', 'procter and gamble when we come together sofia vergara chili bounty old spice head and shoulders olay charming febreze': 'procter & gamble  when we come together', 'woman coach ers katie sowers surface female coach microsoft first female coach katiesowers erscoach microsoftsurface femalecoach': 'microsoft surface  be the one', 'fox lone star storm tornado': 'fox  i m scared', 'jeep groundhog day jeep gladiator bill murray': 'jeep  groundhog day [t1]', 'chris rock sylvester stallone groups facebook rock music stonehenge rock rocketchrisrock': 'facebook  ready to rock?', 'tide tide pod laundry detergent emily hampshire charlie day finally later': 'tide  finally later', 'maisie williams game of thrones let it go frozenaudi etron sportback traffic letitgo maisiewilliamsgameofthrones': 'audi  let it go [t1]', 'none': 'none'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqoXEIpR0zO1"
      },
      "source": [
        "# explode to only 0,1 classes\n",
        "def explode_data_to_two_classes(sentences, labels, classification_ad_product):\n",
        "  label_name_list = []\n",
        "  label_id_list = []\n",
        "  if classification_ad_product:\n",
        "    label_name_list = list(product_id_dict.keys())\n",
        "    label_id_list = list(product_id_dict.values())\n",
        "  else:\n",
        "    label_name_list = list(ad_id_dict.keys())\n",
        "    label_id_list = list(ad_id_dict.values())\n",
        "\n",
        "  sentences_exploded =[]\n",
        "  labels_exploded = []\n",
        "  # sent_labels_df = pd.DataFrame()\n",
        "  for i in range(len(sentences)):\n",
        "    curr_sent = sentences[i]\n",
        "    curr_label_name = labels[i]\n",
        "\n",
        "    curr_sent_exploded = []\n",
        "    curr_label_exploded = []\n",
        "    for j in range(len(label_name_list)): # expand for all products present\n",
        "      if label_name_list[j] is not 'none':\n",
        "        if classification_ad_product:\n",
        "          label_keywords = ad_product_keywords_dict[label_name_list[j]]\n",
        "        else:\n",
        "          label_keywords = ad_name_keywords_dict[label_name_list[j]]\n",
        "\n",
        "        curr_sent_exploded.append(curr_sent+\". \"+ label_keywords)\n",
        "          \n",
        "        if label_name_list[j] == curr_label_name :\n",
        "          curr_label_exploded.append(1)\n",
        "        else:\n",
        "          curr_label_exploded.append(0)\n",
        "    sentences_exploded.extend(curr_sent_exploded)\n",
        "    labels_exploded.extend(curr_label_exploded)\n",
        "\n",
        "  return sentences_exploded, labels_exploded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46qeR6mg0iUs",
        "outputId": "c7ab3f92-f343-4ad6-9599-1f7f1379b568"
      },
      "source": [
        "if sentence_explode:\n",
        "  sentences_exploded, labels_exploded = explode_data_to_two_classes(sentences.values, labels.values, classification_ad_product)\n",
        "  print(\"size of train data after exploding: \"+ str(len(sentences_exploded)))\n",
        "  test_sentences_exploded, test_labels_exploded = explode_data_to_two_classes(test_sentences.values, test_labels.values, classification_ad_product)\n",
        "  print(\"size of test data after exploding: \"+ str(len(test_sentences_exploded)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "size of train data after exploding: 280380\n",
            "size of test data after exploding: 70140\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJQ3USnpNWt9"
      },
      "source": [
        "#### Methods to extract and process Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXMJgvdM8y1R"
      },
      "source": [
        "def get_sent_exploded_results(pred_scores, pred_labels, opFileName):\n",
        "\n",
        "  sent_exploded_results_df = pd.DataFrame()\n",
        "  sent_exploded_results_df['sentence_raw'] = test_sentences_exploded\n",
        "  sent_exploded_results_df['label_pred'] = pred_labels\n",
        "  sent_exploded_results_df['label_true'] = test_labels_exploded\n",
        "  sent_exploded_results_df['pred_scores'] = pred_scores\n",
        "  sent_exploded_results_df[['sentence','keywords_appended']] = sent_exploded_results_df.sentence_raw.str.split(\".\",expand=True) \n",
        "\n",
        "  sent_exploded_results_df['keywords_appended_name'] = sent_exploded_results_df['keywords_appended'].apply(lambda x: 'none' if x is None else ad_keywords_name_dict[x.strip()])\n",
        "\n",
        "  sent_exploded_results_df.to_csv(\"./\"+opFileName)\n",
        "  print(sent_exploded_results_df.head())\n",
        "  return sent_exploded_results_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRJ_LzjwNSRi"
      },
      "source": [
        "def get_pred_2_label_name_score(df):\n",
        "  sentence = df['sentence'].iloc[0:1] # since grouped by sentence there will be only one unique sentence\n",
        "  max_pred_score = 0\n",
        "  pred_label = 0\n",
        "  pred_label_name = \"none\"\n",
        "\n",
        "  true_label = 0\n",
        "  true_label_name = \"none\"\n",
        "\n",
        "  for i, row in df.iterrows():\n",
        "    if row['label_pred'] == 1 and row['pred_scores']> max_pred_score:\n",
        "      max_pred_score = row['pred_scores']\n",
        "      pred_label = 1\n",
        "      pred_label_name = row['keywords_appended_name']\n",
        "    # if all 0  => not related to any ad => none\n",
        "    # if all 2 => none\n",
        "\n",
        "    # check for the true label as well \n",
        "    if row['label_true'] == 1 :\n",
        "        true_label = 1\n",
        "        true_label_name = row['keywords_appended_name']\n",
        "   # default 0 which will be none     \n",
        "\n",
        "  result = pd.DataFrame()\n",
        "  result['sentence'] = sentence\n",
        "  result['pred_label_name'] = pred_label_name\n",
        "  result['max_pred_score'] = max_pred_score\n",
        "  result['true_label'] = true_label\n",
        "  result['true_label_name'] = true_label_name\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIj7zaDYNHlO"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "def getClassificationReport(sent_exploded_results_df, opFileName1, opFileName2):\n",
        "  sentence_results_df = pd.DataFrame()\n",
        "  sentence_results_df[['sentence_1', 'pred_label_name', 'max_pred_score', 'true_label', 'true_label_name']] = sent_exploded_results_df.groupby('sentence').apply(get_pred_2_label_name_score)\n",
        "\n",
        "  print(sentence_results_df.shape)\n",
        "  print(sentence_results_df.head())\n",
        "  sentence_results_df.to_csv('./'+ opFileName1)\n",
        "\n",
        "  true_names = sentence_results_df['true_label_name']\n",
        "  pred_names = sentence_results_df['pred_label_name']\n",
        "  class_report = classification_report(true_names, pred_names,output_dict=True)\n",
        "\n",
        "  classification_report_df = pd.DataFrame(class_report).transpose()\n",
        "  print(classification_report_df.head())\n",
        "  classification_report_df.to_csv('./'+ opFileName2)\n",
        "  return classification_report_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLkbCoBzq5xY"
      },
      "source": [
        "ANN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxWjxeg7cNkM"
      },
      "source": [
        "# Run this to free up RAM\n",
        "# del(X_train)\n",
        "# del(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtH8xlUPq_R6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31919650-62ae-4732-8576-31c9bc867f2c"
      },
      "source": [
        "# first neural network with keras make predictions\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import tensorflow\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "if sentence_explode:\n",
        "  print(\"Joint Learning\")\n",
        "  sentences = sentences_exploded\n",
        "  test_sentences = test_sentences_exploded\n",
        "  labels = labels_exploded\n",
        "  label_test = test_labels_exploded\n",
        "\n",
        "# vectorizer = CountVectorizer()\n",
        "vectorizer = TfidfVectorizer(max_features = 6000) #max_features = 150\n",
        "vectorizer.fit(sentences)\n",
        "X_train = vectorizer.transform(sentences)\n",
        "X_test  = vectorizer.transform(test_sentences)\n",
        "input_dim = X_train.shape[1] \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Joint Learning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ar1VGcKETyfn",
        "outputId": "99d7c4a6-30e3-42a0-ebac-8c26404abe50"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(280380, 6100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rZnorTRgu9l"
      },
      "source": [
        "# fit the keras model on the dataset\n",
        "if sentence_explode:\n",
        "  no_classes = 2\n",
        "else:\n",
        "  no_classes = 61\n",
        "X_train = X_train.toarray()\n",
        "X_test = X_test.toarray()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwDCORCYqs1h"
      },
      "source": [
        "# define the keras model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim= input_dim, activation='relu'))\n",
        "# model.add(Dense(8, activation='relu'))\n",
        "if sentence_explode:\n",
        "  model.add(Dense(no_classes, activation='softmax')) #sigmoid\n",
        "else:\n",
        "  model.add(Dense(no_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYyFaRIKhwHY",
        "outputId": "068abe42-f284-4df8-8c30-c0bf091e7f71"
      },
      "source": [
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(labels)# label_train?\n",
        "encoded_train = encoder.transform(labels)\n",
        "one_hot_y = tensorflow.keras.utils.to_categorical(encoded_train)\n",
        "\n",
        "print(encoded_train.shape)\n",
        "print(one_hot_y.shape)\n",
        "print(X_train.shape)\n",
        "\n",
        "model.fit(X_train, one_hot_y , epochs=50, batch_size=200)\n",
        "\n",
        "encoded_test = encoder.transform(label_test)\n",
        "\n",
        "# make class predictions with the model\n",
        "label_pred_ann = model.predict_classes(X_test) ##   label_pred = np.argmax(pred_probs, axis=1)\n",
        "pred_probs = model.predict_proba(X_test)\n",
        "label_pred_ann_scores = np.max(pred_probs, axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(280380,)\n",
            "(280380, 2)\n",
            "(280380, 6100)\n",
            "Epoch 1/50\n",
            "1402/1402 [==============================] - 26s 8ms/step - loss: 0.2283 - accuracy: 0.9797\n",
            "Epoch 2/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0389 - accuracy: 0.9923\n",
            "Epoch 3/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0355 - accuracy: 0.9926\n",
            "Epoch 4/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0346 - accuracy: 0.9926\n",
            "Epoch 5/50\n",
            "1402/1402 [==============================] - 10s 7ms/step - loss: 0.0340 - accuracy: 0.9927\n",
            "Epoch 6/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0345 - accuracy: 0.9925\n",
            "Epoch 7/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0322 - accuracy: 0.9927\n",
            "Epoch 8/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0334 - accuracy: 0.9924\n",
            "Epoch 9/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0338 - accuracy: 0.9923\n",
            "Epoch 10/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0331 - accuracy: 0.9925\n",
            "Epoch 11/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0329 - accuracy: 0.9924\n",
            "Epoch 12/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0328 - accuracy: 0.9924\n",
            "Epoch 13/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0322 - accuracy: 0.9925\n",
            "Epoch 14/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0312 - accuracy: 0.9928\n",
            "Epoch 15/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0323 - accuracy: 0.9924\n",
            "Epoch 16/50\n",
            "1402/1402 [==============================] - 11s 7ms/step - loss: 0.0316 - accuracy: 0.9926\n",
            "Epoch 17/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0314 - accuracy: 0.9925\n",
            "Epoch 18/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0316 - accuracy: 0.9926\n",
            "Epoch 19/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0304 - accuracy: 0.9928\n",
            "Epoch 20/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0315 - accuracy: 0.9925\n",
            "Epoch 21/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0306 - accuracy: 0.9926\n",
            "Epoch 22/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0302 - accuracy: 0.9927\n",
            "Epoch 23/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0314 - accuracy: 0.9923\n",
            "Epoch 24/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0308 - accuracy: 0.9923\n",
            "Epoch 25/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0295 - accuracy: 0.9927\n",
            "Epoch 26/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0295 - accuracy: 0.9927\n",
            "Epoch 27/50\n",
            "1402/1402 [==============================] - 10s 7ms/step - loss: 0.0303 - accuracy: 0.9924\n",
            "Epoch 28/50\n",
            "1402/1402 [==============================] - 10s 7ms/step - loss: 0.0301 - accuracy: 0.9924\n",
            "Epoch 29/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0290 - accuracy: 0.9927\n",
            "Epoch 30/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0289 - accuracy: 0.9926\n",
            "Epoch 31/50\n",
            "1402/1402 [==============================] - 12s 9ms/step - loss: 0.0276 - accuracy: 0.9930\n",
            "Epoch 32/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0286 - accuracy: 0.9927\n",
            "Epoch 33/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0282 - accuracy: 0.9927\n",
            "Epoch 34/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0274 - accuracy: 0.9926\n",
            "Epoch 35/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0277 - accuracy: 0.9927\n",
            "Epoch 36/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0282 - accuracy: 0.9924\n",
            "Epoch 37/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0278 - accuracy: 0.9925\n",
            "Epoch 38/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0266 - accuracy: 0.9927\n",
            "Epoch 39/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0267 - accuracy: 0.9926\n",
            "Epoch 40/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0262 - accuracy: 0.9928\n",
            "Epoch 41/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0264 - accuracy: 0.9927\n",
            "Epoch 42/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0252 - accuracy: 0.9930\n",
            "Epoch 43/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0256 - accuracy: 0.9928\n",
            "Epoch 44/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0254 - accuracy: 0.9930\n",
            "Epoch 45/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0253 - accuracy: 0.9930\n",
            "Epoch 46/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0253 - accuracy: 0.9929\n",
            "Epoch 47/50\n",
            "1402/1402 [==============================] - 10s 7ms/step - loss: 0.0246 - accuracy: 0.9933\n",
            "Epoch 48/50\n",
            "1402/1402 [==============================] - 10s 7ms/step - loss: 0.0240 - accuracy: 0.9933\n",
            "Epoch 49/50\n",
            "1402/1402 [==============================] - 11s 8ms/step - loss: 0.0240 - accuracy: 0.9934\n",
            "Epoch 50/50\n",
            "1402/1402 [==============================] - 10s 7ms/step - loss: 0.0233 - accuracy: 0.9936\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRuBIN37rWaE"
      },
      "source": [
        "sent_exploded_results_df_ann = get_sent_exploded_results(label_pred_ann_scores,label_pred_ann,\\\n",
        "                                                              \"sent_exploded_results_df_ann.csv\" )\n",
        "class_report_ann = getClassificationReport(sent_exploded_results_df_ann, \"sent_exp_group_results_ann.csv\",\\\n",
        "                        \"classification_report_sent_explode_ann.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAQslulbcyBb"
      },
      "source": [
        "stop here for ann"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o5JgfCoBGH1"
      },
      "source": [
        "# for multi class classification ot sent_explode = False\n",
        "classification_report_ann = classification_report(encoded_test, label_pred, target_names= set(label_test),output_dict=True)\n",
        "classification_report_ann_df = pd.DataFrame(classification_report_ann).transpose()\n",
        "if sentence_explode: \n",
        "  classification_report_ann_df.to_csv('./classification_report_ann_exploded.csv')\n",
        "else:\n",
        "  classification_report_ann_df.to_csv('./classification_report_ann.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WynMl6_r1dzN"
      },
      "source": [
        "# multi-class classification with Keras\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "input_dim = X_train.shape[1] \n",
        "n_classes = 2\n",
        "# define baseline model\n",
        "def baseline_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(8, input_dim=input_dim, activation='relu'))\n",
        "\tmodel.add(Dense(n_classes, activation='softmax'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "estimator = KerasClassifier(build_fn=baseline_model, epochs=50, batch_size=100, verbose=0)\n",
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "results = cross_val_score(estimator, X_train.astype(float), one_hot_y, cv=kfold)\n",
        "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLSTkvjS3Zqg"
      },
      "source": [
        "stop here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USqUnkaKEWR6"
      },
      "source": [
        "#### Naive Bayes Classifier for Multinomial Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-0w2LpLDhoU"
      },
      "source": [
        "\n",
        "# nb = Pipeline([('vect', CountVectorizer()),\n",
        "#                ('tfidf', TfidfTransformer()),\n",
        "#                ('clf', MultinomialNB()),\n",
        "#               ])\n",
        "# nb.fit(twt_train, label_train)\n",
        "\n",
        "# label_pred_nb = nb.predict(twt_test)\n",
        "\n",
        "\n",
        "# print('accuracy %s' % accuracy_score(label_pred_nb, label_test))\n",
        "# classification_report_nb = classification_report(label_test, label_pred_nb, target_names= set(label_test),output_dict=True)\n",
        "# classification_report_nb_df = pd.DataFrame(classification_report_nb).transpose()\n",
        "# classification_report_nb_df.to_csv('./classification_report_nb.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lcv_vrhEZEN"
      },
      "source": [
        "#### Linear Support Vector Machine\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgUd-bgvEYfC"
      },
      "source": [
        "# from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# sgd = Pipeline([('vect', CountVectorizer()),\n",
        "#                 ('tfidf', TfidfTransformer()),\n",
        "#                 ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=10, tol=None)),\n",
        "#                ])\n",
        "# if sentence_explode:\n",
        "#   sgd.fit(sentences_exploded, labels_exploded)\n",
        "  \n",
        "# else:\n",
        "#   sgd.fit(twt_train, label_train)\n",
        "#   label_pred_sgd = sgd.predict(twt_test)\n",
        "\n",
        "#   print('accuracy %s' % accuracy_score(label_pred_sgd, label_test))\n",
        "#   classification_report_sgd = classification_report(label_test, label_pred_sgd, target_names= set(label_test), output_dict=True)\n",
        "#   classification_report_sgd_df = pd.DataFrame(classification_report_sgd).transpose()\n",
        "#   classification_report_sgd_df.to_csv('./classification_report_sgd.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tZP5LJK2ylw"
      },
      "source": [
        "# from sklearn.linear_model import SGDClassifier\n",
        "# from sklearn.calibration import CalibratedClassifierCV\n",
        "# sentence_explode = True\n",
        "# if sentence_explode:\n",
        "#   weights = {0:1, 1:1000}\n",
        "#   sgd = Pipeline([('vect', CountVectorizer()),\n",
        "#                 ('tfidf', TfidfTransformer()),\n",
        "#                 ('clf', CalibratedClassifierCV((SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3,\\\n",
        "#                         random_state=42, max_iter=5, tol=None, class_weight=weights)))),\n",
        "#                ])\n",
        "#   sgd.fit(sentences_exploded, labels_exploded)\n",
        "#   pred_probs = sgd.predict_proba(test_sentences_exploded)\n",
        "#   label_pred_sgd = np.argmax(pred_probs, axis=1)\n",
        "#   label_pred_sgd_scores = np.max(pred_probs, axis=1)\n",
        "#   classes = sgd.classes_\n",
        "#   print(label_pred_sgd_scores.shape)\n",
        "#   # label_pred_sgd = sgd.predict(test_sentences_exploded)\n",
        "#   print('accuracy %s' % accuracy_score(label_pred_sgd, test_labels_exploded)) #accuracy 0.9898916452808668\n",
        "#   sent_exploded_results_df_sgd = get_sent_exploded_results(label_pred_sgd_scores,label_pred_sgd,\\\n",
        "#                                                               \"sent_exploded_results_df_sgd.csv\" )\n",
        "#   class_report_sgd = getClassificationReport(sent_exploded_results_df_sgd, \"sent_exp_group_results_sgd.csv\",\\\n",
        "#                         \"classification_report_sent_explode_sgd.csv\")\n",
        "# else:\n",
        "#   sgd.fit(twt_train, label_train)\n",
        "#   label_pred_sgd = sgd.predict(twt_test)\n",
        "#   print('accuracy %s' % accuracy_score(label_pred_sgd, label_test))\n",
        "#   classification_report_sgd = classification_report(label_test, label_pred_sgd, target_names= set(label_test), output_dict=True)\n",
        "#   classification_report_sgd_df = pd.DataFrame(classification_report_sgd).transpose()\n",
        "#   classification_report_sgd_df.to_csv('./classification_report_sgd.csv')\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2NZpy_YHcOb"
      },
      "source": [
        "#### Non Linear SVM with TF-IDF vectorization. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSFsi05Jbmei"
      },
      "source": [
        "##SVM\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "weights = {0:1, 1:100} # SVM with weights: https://machinelearningmastery.com/cost-sensitive-svm-for-imbalanced-classification/\n",
        "\n",
        "model = svm.SVC(gamma = 'scale', probability=True)\n",
        "\n",
        "sgd = Pipeline([('vect', CountVectorizer()),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('clf', model)\n",
        "               ])\n",
        "data_size = len(sentences_exploded)\n",
        "chunk_size = 10000\n",
        "n_iters = 1+ int(data_size/chunk_size)\n",
        "print(n_iters)\n",
        "for i in range(0, n_iters):\n",
        "  startInd = i*10000\n",
        "  endInd = min((i+1)*chunk_size, data_size)\n",
        "  sgd.fit(sentences_exploded[ startInd: endInd], labels_exploded[startInd: endInd])\n",
        "  print(i)\n",
        "\n",
        "pred_probs = sgd.predict_proba(test_sentences_exploded)\n",
        "label_pred_sgd = np.argmax(pred_probs, axis=1)\n",
        "label_pred_sgd_scores = np.max(pred_probs, axis=1)\n",
        "classes = sgd.classes_\n",
        "print(label_pred_sgd_scores.shape)\n",
        "print('accuracy %s' % accuracy_score(label_pred_sgd, test_labels_exploded)) #accuracy 0.9898916452808668\n",
        "sent_exploded_results_df_sgd = get_sent_exploded_results(label_pred_sgd_scores,label_pred_sgd,\\\n",
        "                                                              \"sent_exploded_results_df_sgd.csv\" )\n",
        "class_report_sgd = getClassificationReport(sent_exploded_results_df_sgd, \"sent_exp_group_results_sgd.csv\",\\\n",
        "                        \"classification_report_sent_explode_sgd.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDmUchZ0v0vr"
      },
      "source": [
        "class_report_sgd.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGI-599Lv0Fp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbR-fkb3HHtq"
      },
      "source": [
        "stop here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt8JPIDjX3rj"
      },
      "source": [
        "#### SVM with TF-IDF vectorization and One Vs One classifier \n",
        "\n",
        "**Issues:**\n",
        " \n",
        "\n",
        "1.   One Vs One not specifically needed since in Joint Learning there are only two classes\n",
        "2.   one Vs one does not support predict_proba\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBaQ-Mropltt"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNZvEPOmd3TV"
      },
      "source": [
        "# works only to predict classes but not probabilities hence classification\n",
        "# within ads is not possible in joint learning setting\n",
        "# ref one-vs-one classifier: https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "model = svm.SVC(gamma = 'scale', probability=True)\n",
        "ovo = OneVsOneClassifier(model)\n",
        "\n",
        "# tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
        "tfidf = TfidfVectorizer(max_features = 500)\n",
        "sentences_exploded_features = tfidf.fit_transform(sentences_exploded)\n",
        "print(sentences_exploded_features.shape)\n",
        "\n",
        "data_size = len(sentences_exploded)\n",
        "chunk_size = 5000\n",
        "n_iters = 1+ int(data_size/chunk_size)\n",
        "print(n_iters)\n",
        "for i in range(0, n_iters):\n",
        "  startInd = i*chunk_size\n",
        "  endInd = min((i+1)*chunk_size, data_size)\n",
        "  ovo.fit(sentences_exploded_features[startInd: endInd], labels_exploded[startInd: endInd])\n",
        "  print(i)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0gIB1nfDlkE"
      },
      "source": [
        "one Vs One classifier cannot predict probabilities hence this code only gives results from 0 Vs 1 classes in joint learning setting\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5PlatPq6fRk"
      },
      "source": [
        "test_sentences_exploded_features = tfidf.fit_transform(test_sentences_exploded)\n",
        "label_pred_sgd_scores =  ovo.predict(test_sentences_exploded_features)\n",
        "print('accuracy %s' % accuracy_score(label_pred_sgd_scores, test_labels_exploded))\n",
        "classification_report_sgd = classification_report(test_labels_exploded, label_pred_sgd_scores, target_names= set(test_labels_exploded), output_dict = True)\n",
        "classification_report_sgd_df = pd.DataFrame(classification_report_sgd).transpose()\n",
        "classification_report_sgd_df.to_csv('./classification_report_sgd.csv')\n",
        "\n",
        "# one Vs One classifier cannot predict probabilities hence this code does not work \n",
        "\n",
        "# test_sentences_exploded_features = tfidf.fit_transform(test_sentences_exploded)\n",
        "# pred_probs = ovo.predict_proba(test_sentences_exploded_features)\n",
        "# label_pred_sgd = np.argmax(pred_probs, axis=1)\n",
        "# label_pred_sgd_scores = np.max(pred_probs, axis=1)\n",
        "# classes = ovo.classes_\n",
        "# print(label_pred_sgd_scores.shape)\n",
        "# print('accuracy %s' % accuracy_score(label_pred_sgd, test_labels_exploded)) #accuracy 0.9898916452808668\n",
        "# sent_exploded_results_df_sgd = get_sent_exploded_results(label_pred_sgd_scores,label_pred_sgd,\\\n",
        "#                                                               \"sent_exploded_results_df_sgd.csv\" )\n",
        "# class_report_sgd = getClassificationReport(sent_exploded_results_df_sgd, \"sent_exp_group_results_sgd.csv\",\\\n",
        "#                         \"classification_report_sent_explode_sgd.csv\")\n",
        "# ovo.partial_fit(sentences_exploded_features, labels_exploded, classes = list(set(labels_exploded)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyG7VlkHChNA"
      },
      "source": [
        "classification_report_sgd_df.head(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIL-MgfbE_SA"
      },
      "source": [
        "#### LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEyLUQpdD7gg"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logreg = Pipeline([('vect', CountVectorizer()),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('clf', LogisticRegression(n_jobs=1, C=1e5, solver = 'saga', max_iter = 5000)),\n",
        "               ])\n",
        "if sentence_explode:\n",
        "  logreg.fit(sentences_exploded, labels_exploded)\n",
        "  pred_probs = logreg.predict_proba(test_sentences_exploded)\n",
        "  label_pred_logreg = np.argmax(pred_probs, axis=1)\n",
        "  label_pred_logreg_scores = np.max(pred_probs, axis=1)\n",
        "  classes = logreg.classes_\n",
        "  label_pred_logreg = [classes[i] for i in label_pred_logreg]\n",
        "  print('accuracy %s' % accuracy_score(label_pred_logreg, test_labels_exploded)) #accuracy 0.857\n",
        "\n",
        "else:\n",
        "  logreg.fit(twt_train, label_train)\n",
        "  label_pred_logreg = logreg.predict(twt_test)\n",
        "  print('accuracy %s' % accuracy_score(label_pred_logreg, label_test)) #accuracy 0.8306244653550042\n",
        "  classification_report_logreg = classification_report(label_test, label_pred_logreg, target_names= set(label_test), output_dict=True)\n",
        "  classification_report_logreg_df = pd.DataFrame(classification_report_logreg).transpose()\n",
        "  classification_report_logreg_df.to_csv('./classification_report_logreg.csv')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMmKN73RVqeC"
      },
      "source": [
        "sent_exploded_results_df_log_reg = get_sent_exploded_results(label_pred_logreg_scores,label_pred_logreg,\\\n",
        "                                                              \"sent_exploded_results_df_logreg.csv\" )\n",
        "getClassificationReport(sent_exploded_results_df_log_reg, \"sent_exp_group_results_logReg.csv\",\\\n",
        "                        \"classification_report_sent_explode_logReg.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lziVreLFRN1J"
      },
      "source": [
        "#### Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwC5dW4tOzyq"
      },
      "source": [
        "train_data = pd.DataFrame()\n",
        "train_data['data'] = twt_train\n",
        "train_data['label'] = label_train\n",
        "train_data.to_csv('./log_reg_train_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oD4aWoXdFIfV"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    # print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=90)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "y_pred = label_pred_logreg\n",
        "cnf_matrix = confusion_matrix(label_test, y_pred)\n",
        "\n",
        "# Plot normalized confusion matrix\n",
        "fig = plt.figure()\n",
        "fig.set_size_inches(50, 50, forward=True)\n",
        "#fig.align_labels()\n",
        "\n",
        "# fig.subplots_adjust(left=0.0, right=1.0, bottom=0.0, top=1.0)\n",
        "plot_confusion_matrix(cnf_matrix, classes=np.asarray(unique_ads), normalize=True,\n",
        "                      title='Normalized confusion matrix')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjjE-5QGf9OQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}