{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentBERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNeqmDRkCatWk2Qb3jVN2Jj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6e4110a1d3e940c5a329f88d1eaab0fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2315e578501e4178b34c6d9c27d27a8e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b17da589fd464f9994e149d909eaa40f",
              "IPY_MODEL_5eba2e5f02f84aafbe31a3a198d96891"
            ]
          }
        },
        "2315e578501e4178b34c6d9c27d27a8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b17da589fd464f9994e149d909eaa40f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1490bc8c0e734001bee74c8138b24112",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 244733649,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 244733649,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_08ca89cea0654ffa96e59243cb9a492a"
          }
        },
        "5eba2e5f02f84aafbe31a3a198d96891": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_49c40ae64b924a4b8a20328f2f28f4f1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 245M/245M [00:15&lt;00:00, 16.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8078d7e3025848a88d601cbe71ea90ff"
          }
        },
        "1490bc8c0e734001bee74c8138b24112": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "08ca89cea0654ffa96e59243cb9a492a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "49c40ae64b924a4b8a20328f2f28f4f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8078d7e3025848a88d601cbe71ea90ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Roopana/Twitter-Ad-Analysis/blob/master/SentBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMfECRFV4Sq5"
      },
      "source": [
        "Ref : https://www.sbert.net/docs/training/overview.html#creating-networks-from-scratch\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezL-7kHR159P",
        "outputId": "872e43bc-86ef-49d8-bf26-32ee7af13187"
      },
      "source": [
        "pip install -U sentence-transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/87/49dc49e13ac107ce912c2f3f3fd92252c6d4221e88d1e6c16747044a11d8/sentence-transformers-1.1.0.tar.gz (78kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81kB 3.6MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.1MB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.8.1+cu101)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.2MB 23.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3MB 38.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 901kB 43.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.10.1)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.1)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-1.1.0-cp37-none-any.whl size=119615 sha256=fc28a5e8ee5031ee13326c0b13b4c9d00e6dad92b86685181671081fdf16d853\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/cb/21/1066bff3027215c760ca14a198f698bca8fccb92e33e2327eb\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: tokenizers, sacremoses, transformers, sentencepiece, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.45 sentence-transformers-1.1.0 sentencepiece-0.1.95 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th47f8pS12WB"
      },
      "source": [
        "Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUaMZl7l1l2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "6e4110a1d3e940c5a329f88d1eaab0fb",
            "2315e578501e4178b34c6d9c27d27a8e",
            "b17da589fd464f9994e149d909eaa40f",
            "5eba2e5f02f84aafbe31a3a198d96891",
            "1490bc8c0e734001bee74c8138b24112",
            "08ca89cea0654ffa96e59243cb9a492a",
            "49c40ae64b924a4b8a20328f2f28f4f1",
            "8078d7e3025848a88d601cbe71ea90ff"
          ]
        },
        "outputId": "e870c9dd-3867-4fcd-fd7b-f0086f1800c1"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "model = SentenceTransformer('distilbert-base-nli-mean-tokens')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e4110a1d3e940c5a329f88d1eaab0fb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=244733649.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gsl3ppgi52kB",
        "outputId": "daf0fd30-aeb2-48a0-f91b-16e0356e357d"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_2 = pd.read_csv(\"./2_man_ann_sb.csv\",  index_col= None)\n",
        "df_2 = df_2.dropna(subset = ['tweet_text'])\n",
        "\n",
        "df_1 = pd.read_csv(\"./mann_ann_sb.csv\", index_col= None)\n",
        "\n",
        "df_3 = pd.read_csv(\"./3_man_ann_sb_full_1.csv\", index_col = None)\n",
        "df_3 = df_3.dropna(subset = ['tweet_text'])\n",
        "\n",
        "df_raw = df_1.append(df_2).append(df_3) # using batch 1 and batch 2 for training\n",
        "\n",
        "print(df_raw.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7500, 19)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSIOy26R5-FF",
        "outputId": "33e4383c-842d-4c67-df75-82d0f5ea89f2"
      },
      "source": [
        "# Remove ads marked as below because they are not available in ad annotations file although tweets mention them\n",
        "# commercials, joe biden, pizzahut, joe bieden, michael bloomberg, mike bloomberg, scientology...\n",
        "ads_remove = ['commercials', 'joe biden', 'pizzahut', 'joe bieden', 'michael bloomberg', 'mike bloomberg', \n",
        "              'scientology','papa johns',  'bakari',  'secret', 'dashlane', 'bernie the peoples perfume',\n",
        "              'ram trucks', 'golden gronks', \"bush's best\", 'ragged old flag', 'patience', 'guitar hero',\n",
        "              'disney mulan']\n",
        "\n",
        "# ads_rename = ['nfl100', 'tide']\n",
        "# rename ads with spelling faults while manually adding the annotation \n",
        "\n",
        "df_raw['ad_manual_adjusted'] = df_raw['ad_manual_adjusted'].apply(lambda x: x.lower())\n",
        "df_raw.loc[df_raw.ad_manual_adjusted == \"discover card  no we donâ€šÃ¤Ã´t charge annual fees\", \n",
        "       \"ad_manual_adjusted\"] = \"discover card  no we donâ€™t charge annual fees\"\n",
        "df_raw.loc[df_raw.ad_manual_adjusted == \"doritos the cool ranch\", \n",
        "       \"ad_manual_adjusted\"] = \"doritos  the cool ranch\"\n",
        "df_raw.loc[df_raw.ad_manual_adjusted == \"discover card yes we're accepted\", \n",
        "       \"ad_manual_adjusted\"] =  \"discover card  yes weâ€™re accepted\"\n",
        "df_raw.loc[df_raw.ad_manual_adjusted == \"discover card yes weâ€™re accepted\", \n",
        "       \"ad_manual_adjusted\"] =  \"discover card  yes weâ€™re accepted\"\n",
        "df_raw.loc[df_raw.ad_manual_adjusted == \"discover card  yes we're accepted\", \n",
        "       \"ad_manual_adjusted\"] =  \"discover card  yes weâ€™re accepted\"\n",
        "df_raw.loc[df_raw.ad_manual_adjusted == \"budweiser typical american\", \n",
        "       \"ad_manual_adjusted\"] = \"budweiser  typical american\"\n",
        "df_raw.loc[df_raw.ad_manual_adjusted == 'fox  halftime show  teaser_3',\n",
        "            \"ad_manual_adjusted\"] = \"fox  halftime show  teaser_1\"\n",
        "df_raw.loc[df_raw.ad_manual_adjusted == 'fox  halftime show  teaser_2',\n",
        "            \"ad_manual_adjusted\"] = \"fox  halftime show  teaser_1\"\n",
        "            \n",
        "print(df_raw.shape)\n",
        "df = pd.DataFrame()\n",
        "removed_Data = pd.DataFrame()\n",
        "\n",
        "for i,row  in df_raw.iterrows():\n",
        "  if row['ad_manual_adjusted'] not in ads_remove:\n",
        "    df = df.append(row)\n",
        "  else:\n",
        "    removed_Data = removed_Data.append(row)\n",
        "print(df.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7500, 19)\n",
            "(7394, 19)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jUwMr-P7jo_"
      },
      "source": [
        "classType = 'sent_exploded' # binary or multi-class or sent-exploded"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5etga0l7kJ8",
        "outputId": "3365e59b-628f-4b6a-d0ef-346f3db9c82b"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "# stop.append('superbowl')\n",
        "# stop.append('super') \n",
        "# stop.append('bowl')\n",
        "\n",
        "# remove for multi class since almost all ads have these words\n",
        "if classType is not 'binary':\n",
        "  stop.append('commercial')\n",
        "  stop.append('ad')\n",
        "  stop.append('commercials')\n",
        "  stop.append('ads')\n",
        "print(len(stop))\n",
        "\n",
        "def removeMentions(text):\n",
        "\n",
        "    textBeforeMention = text.partition(\"@\")[0]\n",
        "    textAfterMention = text.partition(\"@\")[2]\n",
        "    textAfterMention =  re.sub(r':', '', textAfterMention) #cadillac join the 31k\n",
        "    tHandle = textAfterMention.partition(\" \")[0].lower() #cadillac    \n",
        "    text = textBeforeMention+ \" \" + textAfterMention  \n",
        "    return text\n",
        "\n",
        "def cleanTweet(strinp):\n",
        "    strinp = re.sub(r'RT', \"\", strinp) # Remove RT\n",
        "    strinp = strinp.lower()\n",
        "    \n",
        "    stop_removed_list = [word for word in strinp.split() if word not in (stop)]\n",
        "    stop_removed = ' '.join([str(elem) for elem in stop_removed_list])    \n",
        "    text = re.sub('https?://[A-Za-z0-9./]+', ' ', stop_removed) # Remove URLs\n",
        "    text = removeMentions(text)\n",
        "    text = re.sub('[^\\x00-\\x7F]+', ' ', text) # Remove non-ASCII chars.\n",
        "    \n",
        "    # remove punctuations except '-'\n",
        "    punctuation = ['(', ')', '[',']','?', ':', ':', ',', '.', '!', '/', '\"', \"'\", '@', '#', '&', '-', '_']\n",
        "    text = \"\".join((char for char in text if char not in punctuation))\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text) # remove all other than alphabet chars \n",
        "\n",
        "#     text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text) # remove all single characters     \n",
        "    stop_removed_l = [word for word in text.split() if word not in (stop)]\n",
        "    stop_removed = ' '.join([str(elem) for elem in stop_removed_l]) \n",
        "    return stop_removed\n",
        "\n",
        "print(cleanTweet(\"RT @cadillacabc: Joinrt the 31K james_bond\") )"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "183\n",
            "cadillacabc joinrt k jamesbond\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOBfatuk7znG"
      },
      "source": [
        "df['text_clean'] = df['tweet_text'].apply(lambda x: cleanTweet(x))\n",
        "df['ad_manual_adjusted'] = df['ad_manual_adjusted'].apply(lambda x: x.lower())\n",
        "df['ad_related'] = df['ad_manual_adjusted'].apply(lambda ad: 0 if ad == 'none' else 1)\n",
        "\n",
        "comma_filter = ~df['ad_manual_adjusted'].str.contains(',')\n",
        "df = df[comma_filter]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUKBKwW2738L",
        "outputId": "2f2dd349-e45e-4c77-9fc9-dac00a00a3bc"
      },
      "source": [
        "df_unique = df.drop_duplicates(subset = ['text_clean'])\n",
        "df_with_dupes = df\n",
        "df = df_unique\n",
        "\n",
        "print(df_with_dupes.shape)\n",
        "print(df.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7332, 21)\n",
            "(5824, 21)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MYnXpVK8HYJ",
        "outputId": "7f578f73-0b7e-4ca7-8ec0-790a57a41bda"
      },
      "source": [
        "ad_product_df = pd.read_csv('./SB_ad_annotations_product_category_modified.csv')\n",
        "ad_product_df = ad_product_df.rename(columns = {'Ad Name': 'Ad_Name'}) # rename the column to remove space\n",
        "ad_product_df = ad_product_df.dropna() # because the file has trailing empty rows, remove them\n",
        "# remove fox half time show teaser_2 ad because its keywords is same as fox half time show teaser_2\n",
        "print(ad_product_df.shape)\n",
        "ad_product_df.drop(ad_product_df[ad_product_df['Ad_Name'] == 'FOX  Halftime Show  Teaser_2'].index, inplace = True) \n",
        "print(ad_product_df.shape)\n",
        "ad_product_dict = dict()\n",
        "\n",
        "ad_product_df['Product_modified'] = ad_product_df['Product_modified'].apply(lambda x: x.lower())\n",
        "for i, row in ad_product_df.iterrows():\n",
        "  ad_product_dict[row['Ad_Name'].lower()] = row['Product_modified'].lower()\n",
        "\n",
        "ad_product_dict['none'] = 'none'\n",
        "print(ad_product_dict)\n",
        "\n",
        "df['product_modified'] = df['ad_manual_adjusted'].apply(lambda ad: ad_product_dict[ad])\n",
        "df['product_modified'] = df['product_modified'].apply(lambda x: x.lower())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(75, 11)\n",
            "(74, 11)\n",
            "{'fast & furious 9  trailer': 'movie trailer', 'quibi  bank heist': 'tech company', 'tide  when is later  masked singer': 'laundry detergent', 'fox  a run at history  daytona 500': 'sporting event', 'donald j. trump for president  criminal justice reform': 'political campaign', 'walmart  famous visitors': 'tech company', 'marvel  black widow trailer': 'movie trailer', 'rocket mortgage  home': 'money', 'porsche  the heist': 'car', 'snickers  fix the world': 'food', 'hulu  tom brady s big announcement': 'streaming service', 'fox  chosen  lego masters': 'tv show/network', 'mountain dew': 'pop/soda', 'squarespace  winona in winona': 'tech company', 'new york life  love takes action': 'money', 'fox  super monday': 'tv show/network', 'hyundai  smaht pahk': 'car', 'cheetos  can t touch this': 'food', 'olay  make space for women': 'charitable program', 'fox  halftime show  teaser_1': 'half-time show', 'michelob  6 for 6-pack': 'charitable program/ alcohol', 'avocados from mexico  the avocados from mexico shopping network': 'food', 'hard rock hotels & casinos  bling cup': 'hotel', 'pringles  the infinite dimensions of rick and morty': 'food', 'turbotax  turbotax  all people are tax people remix': 'money', 'tide  bud knight': 'laundry detergent', 'genesis  going away party': 'car', 'coca-cola energy  show up': 'soda', 'planters  baby funeral': 'food', 'no time to die  trailer': 'movie trailer', 'fox  toads  the masked singer': 'tv show/network', 'google assistant  loretta': 'tech company', 'sabra  how do you  mmus?': 'food', 'weathertech  lucky dog': 'charitable program', 'verizon  the amazing things 5g won t do': 'phone network', 'hummer  gmc  quiet revolution': 'car', 'pop-tarts  pop-tarts fixed the pretzel commercial': 'food', 'minions  the rise of gru  trailer': 'movie trailer', 'fox nation  breaking news': 'tv show/network', 'premier boxing champions  wilder vs. fury ii_1': 'sporting event', 'homeland  trailer': 'tv show/network', 'fox  great american race': 'sporting event', 'amazon prime video  hunters': 'tv show/network', 'pepsi zero sugar  zero sugar. done right.': 'pop/soda', 'heinz ketchup  find the goodness  four at once': 'food', 'premier boxing champions  wilder vs. fury ii_2': 'sporting event', 'bud light seltzer  posty store  inside post s brain': 'alcohol', 'little caesars pizza  best thing since sliced bread': 'food', 'doritos  the cool ranch': 'food', 'kia  tough never quits': 'car', 'turkish airlines  step on earth': 'airline', 'reese s  rock': 'food', 'tide  ww': 'laundry detergent', 'amazon echo  before alexa': 'tech company', 'michelob  jimmy works it out': 'alcohol', 'nfl  inspire change  anquan boldin': 'charitable program', 'toyota  heroes': 'car', 'discover card  no we donâ€™t charge annual fees': 'money', 'disney+  it s time': 'streaming service', 'discover card  yes weâ€™re accepted': 'money', 'fox  football withdrawal syndrome': 'sporting event', 't-mobile  mama tests 5g': 'phone network', 'fox  tornado  tomorrow': 'tv show/network', 'fox  fun for all  the masked singer and lego': 'tv show/network', 'nfl  building a better game': 'charitable program', 'budweiser  typical american': 'alcohol', 'procter & gamble  when we come together': 'tech company', 'fox  not just another race': 'sporting event', 'microsoft surface  be the one': 'tech company', 'fox  i m scared': 'tv show/network', 'jeep  groundhog day [t1]': 'car', 'facebook  ready to rock?': 'social network', 'tide  finally later': 'laundry detergent', 'audi  let it go [t1]': 'car', 'none': 'none'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpiDhITtjPou",
        "outputId": "a2373020-35d2-45f6-f6e4-0f9a9b0cc3be"
      },
      "source": [
        "# group the keywords of ads part of a product bucket and append these to the training sentences\n",
        "ad_product_keywords_dict = ad_product_df.groupby('Product_modified')['Key Terms  Round 2'].agg(lambda x : x.sum() if x.dtype=='float64' else ' '.join(x))\n",
        "# clean the ad key words - not removing duplicate words here - TODO\n",
        "for ad_bucket in ad_product_keywords_dict.keys():\n",
        "  ad_product_keywords_dict[ad_bucket] = cleanTweet(ad_product_keywords_dict[ad_bucket])\n",
        "ad_product_df['product_modified_keywords'] = ad_product_df['Product_modified'].apply(lambda x: ad_product_keywords_dict[x])\n",
        "ad_product_df.head(2)\n",
        "\n",
        "# generate adname, ad keywords dict to use in sent exploding\n",
        "ad_name_keywords_dict = dict()\n",
        "for i, row in ad_product_df.iterrows():\n",
        "  ad_name_keywords_dict[row['Ad_Name'].lower()] = cleanTweet(row['Key Terms  Round 2'])\n",
        "ad_name_keywords_dict['none'] = 'none'\n",
        "print(ad_name_keywords_dict)\n",
        "\n",
        "ad_keywords_name_dict = dict()\n",
        "for ad_name in ad_name_keywords_dict:\n",
        "  keywords_temp = ad_name_keywords_dict[ad_name]\n",
        "  ad_keywords_name_dict[keywords_temp] = ad_name\n",
        "print(ad_keywords_name_dict)\n",
        "\n",
        "ad_product_df['ad_name_keywords'] = ad_product_df['Ad_Name'].apply(lambda x: ad_name_keywords_dict[x.lower()])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'fast & furious 9  trailer': 'fast furious fast furious ff f fast saga vin diesel flying truck stunts michelle rodriguez fastfurious', 'quibi  bank heist': 'quibi bank heist robbery less ten minutes quick bites big stories chance rapper megan thee stallion chancetherappermeganstallion quickbites', 'tide  when is later  masked singer': 'tide laundry laundry detergent schitt emily hampshire charlie day walts', 'fox  a run at history  daytona 500': 'fox show daytona run history great american race', 'donald j. trump for president  criminal justice reform': 'donald trump trump change unemployment stronger safer prosperous trumpforpresident donaldtrump presidentrumpobama gop trumpsupporters', 'walmart  famous visitors': 'walmart pickup spaceship toy story buzz lightyear marvin martian marvin martians arrival glass cleaners aliens men black groot lego star wars r bill bill ted flash gordon', 'marvel  black widow trailer': 'marvel black widow scarlett johansson', 'rocket mortgage  home': 'quicken loans aquaman jason momoa rocket mortgagemomoabowl jasonmomoa rocketmorgage', 'porsche  the heist': 'porsche volkswagen chase fast furious taycan turbo scar chase taycanturbo', 'snickers  fix the world': 'snickers snickersfixtheworld fix world snickershole fixtheworld', 'hulu  tom brady s big announcement': 'hulu tom brady tombrady', 'fox  chosen  lego masters': 'fox chosen lego masters competition arnett', 'mountain dew': 'mountain dew mtn dew soda zero sugar shining work play makes jack dull boy new mtn dew zero sugarbryan cranston mtndew theshining', 'squarespace  winona in winona': 'squarespace winona winona ryder minnesota', 'new york life  love takes action': 'love takes action greek words love agape new york life take action lovetakesaction good life nyl newyorklife', 'fox  super monday': 'fox super monday office toby toby flenderson', 'hyundai  smaht pahk': 'chris evans captain america john krasinki rachel dratch boston red sox david ortiz boston accent smart parking hyundai better way park bettahdriveus chrisevans smahtpark bettahwaytopark', 'cheetos  can t touch this': 'mc hammer cant touch popcorn cheetos cheetos thing cheetle canttouchthismchammer', 'olay  make space for women': 'allfemale spacewalk lilly singh busy phillips nicole stott taraji henson katie couric olay make space women makespaceforwomen olay', 'fox  halftime show  teaser_1': 'fox halftime show j lo jennifer lopez shakira', 'michelob  6 for 6-pack': 'help farmers organic pack square footmake impact michelob gold michelobgold pack sixforsixpack', 'avocados from mexico  the avocados from mexico shopping network': 'molly ringwald avocados mexico avonetwork avocarriermollyringwald', 'hard rock hotels & casinos  bling cup': 'hard rock international hard rock stadium miami kobe bryant jennifer lopez jlo hardrock hardrockstadium hardrockcasino', 'pringles  the infinite dimensions of rick and morty': 'rick morty adult swim pringles pickle rick rick morty new flavor summer rickmorty adultswim', 'turbotax  turbotax  all people are tax people remix': 'turbotax people app mobile feature taxes tax people people tax people allpeoplearetaxpeople', 'tide  bud knight': 'tide laundry laundry detergent charliey day bud knight stainlaundrydetergent charlieday', 'genesis  going away party': 'john legend chrissy teigen genesis hyundai kobe bryant young luxury gv sexiest man alive johnlegendchrissyteigen', 'coca-cola energy  show up': 'coke iphone coca cola coca cola energy showup dotscococolacocacolaenergy jonah hill martin scorsese', 'planters  baby funeral': 'planters nutsmrpeanut matt walsh wesley snipesbabynut babypeanut babyfuneral baby nut', 'no time to die  trailer': 'james bond time die daniel craig', 'fox  toads  the masked singer': 'fox masked singer toads', 'google assistant  loretta': 'google assistant virtual assistant googleassistant virtualassistant', 'sabra  how do you  mmus?': 'sabra humus hummus wives real housewives new jersey real housewives caroline manzo teresa giudice tpain mix cracker kim chi drag queensdragqueens dragqueenamanda cerny howimmus', 'weathertech  lucky dog': 'weathertech pets golden retriever dog', 'verizon  the amazing things 5g won t do': 'verizon phone service phone g', 'hummer  gmc  quiet revolution': 'general motorshummer hummer ev electric pickup zero emission lebron james basketball lebronjames', 'pop-tarts  pop-tarts fixed the pretzel commercial': 'pop tarts queer eye jonathan van ness pretzel queereye poptarts', 'minions  the rise of gru  trailer': 'minions rise gru', 'fox nation  breaking news': 'fox fox nation breaking news', 'premier boxing champions  wilder vs. fury ii_1': 'premier boxing champions wilder vs fury ii', 'homeland  trailer': 'homeland trailer', 'fox  great american race': 'fox daytona great american race run history', 'amazon prime video  hunters': 'hunters nazi al pacino nazi hunters prime video amazon barbeque barbecue alpacinoprimevideo hunterstv', 'pepsi zero sugar  zero sugar. done right.': 'missy elliott pepsi zero sugar paint black pepsi pepsizero nocoke', 'heinz ketchup  find the goodness  four at once': 'find goodness heinz ketchup stranger things strangerthings findthegoodness', 'premier boxing champions  wilder vs. fury ii_2': 'premier boxing champions wilder vs fury ii wrestling', 'bud light seltzer  posty store  inside post s brain': 'bud light bud light seltzer post malone anheuserbusch inbev hard seltzer postmalone budlightbudweiser', 'little caesars pizza  best thing since sliced bread': 'office rainn wilson delivery best thing since sliced bread pizza pizza pizza littlecaesars caesars', 'doritos  the cool ranch': 'sam elliott lil nas x old town road doritos cowboy cool ranch dancer billy ray cyrus wild west wild wild west make move', 'kia  tough never quits': 'josh jacobs running back kia kia seltosraiders give everything joshjacobs kiaseltos', 'turkish airlines  step on earth': 'turkish airlines airline flight astronauts rocket', 'reese s  rock': 'reese take five take candybarreeses', 'tide  ww': 'tide stain later charlie day wonder woman', 'amazon echo  before alexa': 'ellen degeneres alexa porta de rossi maid medieval dragon amazon alexa middle ages beforealexa', 'michelob  jimmy works it out': 'anheuserbusch inbev michelob ultrabeer jimmy fallon working gym john cena usain bolt brooks koepka kerri walsh jennings worth enjoy low carbs jimmyfallon usainbolt workingout gymbody', 'nfl  inspire change  anquan boldin': 'inspire change police shootings black men nfl inspirechange', 'toyota  heroes': 'toyota car suv highlander toyotahighlandercobiesmulders cobie smulders', 'discover card  no we donâ€™t charge annual fees': 'pop culture annual fee discover credit card chandler friends jack black mike myers austin powersnoannualfeediscovercredit yestodiscover', 'disney+  it s time': 'disney plus falcon wanda vision wanda loki', 'discover card  yes weâ€™re accepted': 'pop culture discover credit card mean girls ted noannualfeediscovercredit nodiscover', 'fox  football withdrawal syndrome': 'football withdrawal system science doctor withdrawal symptoms', 't-mobile  mama tests 5g': 'tmobile mama tests g g anthony anderson', 'fox  tornado  tomorrow': 'fox tornado lone star', 'fox  fun for all  the masked singer and lego': 'fox masked singer lego masters', 'nfl  building a better game': 'nfl building better game amazon web services', 'budweiser  typical american': 'kathryn bigelow budweiser american spirit typical american kindness beer stereotypes stereotypical typicalamerican americanspirit', 'procter & gamble  when we come together': 'procter gamble come together sofia vergara chili bounty old spice head shoulders olay charming febreze', 'fox  not just another race': 'fox daytona great american race run history', 'microsoft surface  be the one': 'woman coach ers katie sowers surface female coach microsoft first female coach katiesowers erscoach microsoftsurface femalecoach', 'fox  i m scared': 'fox lone star storm tornado', 'jeep  groundhog day [t1]': 'jeep groundhog day jeep gladiator bill murray', 'facebook  ready to rock?': 'chris rock sylvester stallone groups facebook rock music stonehenge rock rocketchrisrock', 'tide  finally later': 'tide tide pod laundry detergent emily hampshire charlie day finally later', 'audi  let it go [t1]': 'maisie williams game thrones let go frozenaudi etron sportback traffic letitgo maisiewilliamsgameofthrones', 'none': 'none'}\n",
            "{'fast furious fast furious ff f fast saga vin diesel flying truck stunts michelle rodriguez fastfurious': 'fast & furious 9  trailer', 'quibi bank heist robbery less ten minutes quick bites big stories chance rapper megan thee stallion chancetherappermeganstallion quickbites': 'quibi  bank heist', 'tide laundry laundry detergent schitt emily hampshire charlie day walts': 'tide  when is later  masked singer', 'fox show daytona run history great american race': 'fox  a run at history  daytona 500', 'donald trump trump change unemployment stronger safer prosperous trumpforpresident donaldtrump presidentrumpobama gop trumpsupporters': 'donald j. trump for president  criminal justice reform', 'walmart pickup spaceship toy story buzz lightyear marvin martian marvin martians arrival glass cleaners aliens men black groot lego star wars r bill bill ted flash gordon': 'walmart  famous visitors', 'marvel black widow scarlett johansson': 'marvel  black widow trailer', 'quicken loans aquaman jason momoa rocket mortgagemomoabowl jasonmomoa rocketmorgage': 'rocket mortgage  home', 'porsche volkswagen chase fast furious taycan turbo scar chase taycanturbo': 'porsche  the heist', 'snickers snickersfixtheworld fix world snickershole fixtheworld': 'snickers  fix the world', 'hulu tom brady tombrady': 'hulu  tom brady s big announcement', 'fox chosen lego masters competition arnett': 'fox  chosen  lego masters', 'mountain dew mtn dew soda zero sugar shining work play makes jack dull boy new mtn dew zero sugarbryan cranston mtndew theshining': 'mountain dew', 'squarespace winona winona ryder minnesota': 'squarespace  winona in winona', 'love takes action greek words love agape new york life take action lovetakesaction good life nyl newyorklife': 'new york life  love takes action', 'fox super monday office toby toby flenderson': 'fox  super monday', 'chris evans captain america john krasinki rachel dratch boston red sox david ortiz boston accent smart parking hyundai better way park bettahdriveus chrisevans smahtpark bettahwaytopark': 'hyundai  smaht pahk', 'mc hammer cant touch popcorn cheetos cheetos thing cheetle canttouchthismchammer': 'cheetos  can t touch this', 'allfemale spacewalk lilly singh busy phillips nicole stott taraji henson katie couric olay make space women makespaceforwomen olay': 'olay  make space for women', 'fox halftime show j lo jennifer lopez shakira': 'fox  halftime show  teaser_1', 'help farmers organic pack square footmake impact michelob gold michelobgold pack sixforsixpack': 'michelob  6 for 6-pack', 'molly ringwald avocados mexico avonetwork avocarriermollyringwald': 'avocados from mexico  the avocados from mexico shopping network', 'hard rock international hard rock stadium miami kobe bryant jennifer lopez jlo hardrock hardrockstadium hardrockcasino': 'hard rock hotels & casinos  bling cup', 'rick morty adult swim pringles pickle rick rick morty new flavor summer rickmorty adultswim': 'pringles  the infinite dimensions of rick and morty', 'turbotax people app mobile feature taxes tax people people tax people allpeoplearetaxpeople': 'turbotax  turbotax  all people are tax people remix', 'tide laundry laundry detergent charliey day bud knight stainlaundrydetergent charlieday': 'tide  bud knight', 'john legend chrissy teigen genesis hyundai kobe bryant young luxury gv sexiest man alive johnlegendchrissyteigen': 'genesis  going away party', 'coke iphone coca cola coca cola energy showup dotscococolacocacolaenergy jonah hill martin scorsese': 'coca-cola energy  show up', 'planters nutsmrpeanut matt walsh wesley snipesbabynut babypeanut babyfuneral baby nut': 'planters  baby funeral', 'james bond time die daniel craig': 'no time to die  trailer', 'fox masked singer toads': 'fox  toads  the masked singer', 'google assistant virtual assistant googleassistant virtualassistant': 'google assistant  loretta', 'sabra humus hummus wives real housewives new jersey real housewives caroline manzo teresa giudice tpain mix cracker kim chi drag queensdragqueens dragqueenamanda cerny howimmus': 'sabra  how do you  mmus?', 'weathertech pets golden retriever dog': 'weathertech  lucky dog', 'verizon phone service phone g': 'verizon  the amazing things 5g won t do', 'general motorshummer hummer ev electric pickup zero emission lebron james basketball lebronjames': 'hummer  gmc  quiet revolution', 'pop tarts queer eye jonathan van ness pretzel queereye poptarts': 'pop-tarts  pop-tarts fixed the pretzel commercial', 'minions rise gru': 'minions  the rise of gru  trailer', 'fox fox nation breaking news': 'fox nation  breaking news', 'premier boxing champions wilder vs fury ii': 'premier boxing champions  wilder vs. fury ii_1', 'homeland trailer': 'homeland  trailer', 'fox daytona great american race run history': 'fox  not just another race', 'hunters nazi al pacino nazi hunters prime video amazon barbeque barbecue alpacinoprimevideo hunterstv': 'amazon prime video  hunters', 'missy elliott pepsi zero sugar paint black pepsi pepsizero nocoke': 'pepsi zero sugar  zero sugar. done right.', 'find goodness heinz ketchup stranger things strangerthings findthegoodness': 'heinz ketchup  find the goodness  four at once', 'premier boxing champions wilder vs fury ii wrestling': 'premier boxing champions  wilder vs. fury ii_2', 'bud light bud light seltzer post malone anheuserbusch inbev hard seltzer postmalone budlightbudweiser': 'bud light seltzer  posty store  inside post s brain', 'office rainn wilson delivery best thing since sliced bread pizza pizza pizza littlecaesars caesars': 'little caesars pizza  best thing since sliced bread', 'sam elliott lil nas x old town road doritos cowboy cool ranch dancer billy ray cyrus wild west wild wild west make move': 'doritos  the cool ranch', 'josh jacobs running back kia kia seltosraiders give everything joshjacobs kiaseltos': 'kia  tough never quits', 'turkish airlines airline flight astronauts rocket': 'turkish airlines  step on earth', 'reese take five take candybarreeses': 'reese s  rock', 'tide stain later charlie day wonder woman': 'tide  ww', 'ellen degeneres alexa porta de rossi maid medieval dragon amazon alexa middle ages beforealexa': 'amazon echo  before alexa', 'anheuserbusch inbev michelob ultrabeer jimmy fallon working gym john cena usain bolt brooks koepka kerri walsh jennings worth enjoy low carbs jimmyfallon usainbolt workingout gymbody': 'michelob  jimmy works it out', 'inspire change police shootings black men nfl inspirechange': 'nfl  inspire change  anquan boldin', 'toyota car suv highlander toyotahighlandercobiesmulders cobie smulders': 'toyota  heroes', 'pop culture annual fee discover credit card chandler friends jack black mike myers austin powersnoannualfeediscovercredit yestodiscover': 'discover card  no we donâ€™t charge annual fees', 'disney plus falcon wanda vision wanda loki': 'disney+  it s time', 'pop culture discover credit card mean girls ted noannualfeediscovercredit nodiscover': 'discover card  yes weâ€™re accepted', 'football withdrawal system science doctor withdrawal symptoms': 'fox  football withdrawal syndrome', 'tmobile mama tests g g anthony anderson': 't-mobile  mama tests 5g', 'fox tornado lone star': 'fox  tornado  tomorrow', 'fox masked singer lego masters': 'fox  fun for all  the masked singer and lego', 'nfl building better game amazon web services': 'nfl  building a better game', 'kathryn bigelow budweiser american spirit typical american kindness beer stereotypes stereotypical typicalamerican americanspirit': 'budweiser  typical american', 'procter gamble come together sofia vergara chili bounty old spice head shoulders olay charming febreze': 'procter & gamble  when we come together', 'woman coach ers katie sowers surface female coach microsoft first female coach katiesowers erscoach microsoftsurface femalecoach': 'microsoft surface  be the one', 'fox lone star storm tornado': 'fox  i m scared', 'jeep groundhog day jeep gladiator bill murray': 'jeep  groundhog day [t1]', 'chris rock sylvester stallone groups facebook rock music stonehenge rock rocketchrisrock': 'facebook  ready to rock?', 'tide tide pod laundry detergent emily hampshire charlie day finally later': 'tide  finally later', 'maisie williams game thrones let go frozenaudi etron sportback traffic letitgo maisiewilliamsgameofthrones': 'audi  let it go [t1]', 'none': 'none'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eG1duwodju1c",
        "outputId": "fdc244e7-4151-4db1-b7cf-c5cab05cadfa"
      },
      "source": [
        "# use column name 'ad_manual_adjusted' of df to get ad_name\n",
        "def get_ad_related_twts(df, removeCommas = True):\n",
        "  df['ad_manual_adjusted'] = df['ad_manual_adjusted'].apply(lambda x: x.lower())\n",
        "  ad_filter = df['ad_manual_adjusted']!= 'none'\n",
        "  ad_related_twts = df[ad_filter]\n",
        "  if removeCommas:\n",
        "    ad_filter_1 = ~ad_related_twts['ad_manual_adjusted'].str.contains(',')\n",
        "    ad_related_twts = ad_related_twts[ad_filter_1]\n",
        "  return ad_related_twts\n",
        "\n",
        "def getAdTweets(ad_related_twts, ad):\n",
        "  return ad_related_twts[ad_related_twts.ad_manual_adjusted == ad].shape[0]\n",
        "\n",
        "def get_ad_id_dict(ad_related_twts): \n",
        "  n_ad_related = ad_related_twts.shape[0]\n",
        "  print(\"# ad related tweets: \"+ str(n_ad_related))\n",
        "  ads_annotated = ad_related_twts.ad_manual_adjusted.values\n",
        "  adset = set(ads_annotated)\n",
        "  print(\"unique ads:\"+ str(len(adset)))\n",
        "  ad_id_dict = {}\n",
        "  i = 0\n",
        "  for ad in adset : \n",
        "    if(getAdTweets(ad_related_twts, ad) >=2):\n",
        "      ad_id_dict[ad] = i\n",
        "      i = i+1\n",
        "    else:\n",
        "      print('ad with <2 samples: '+ str(ad))\n",
        "  print(\" No of ads with >=2 samples:\"+ str(len(ad_id_dict)))\n",
        "  ad_id_dict['none'] = len(ad_id_dict)\n",
        "  print(ad_id_dict)\n",
        "  return ad_id_dict\n",
        "\n",
        "def convertAdNameToAdId(ad_id_dict, ad_name):\n",
        "  if ad_name in ad_id_dict:\n",
        "    return ad_id_dict[ad_name]\n",
        "  else:\n",
        "    return ad_id_dict['none']\n",
        "\n",
        "ad_related_twts = get_ad_related_twts(df)\n",
        "ad_id_dict = get_ad_id_dict(get_ad_related_twts(df))\n",
        "n_unique_ads = len(ad_id_dict) # ad_id_dict has none as well, so minus 1 when using embeddings\n",
        "df['ad_manual_adjusted_id'] = df['ad_manual_adjusted'].apply(lambda x: convertAdNameToAdId(ad_id_dict,x))\n",
        "print(n_unique_ads)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# ad related tweets: 2597\n",
            "unique ads:63\n",
            "ad with <2 samples: tide  ww\n",
            "ad with <2 samples: discover card  no we donâ€™t charge annual fees\n",
            "ad with <2 samples: fast & furious 9  trailer\n",
            " No of ads with >=2 samples:60\n",
            "{'planters  baby funeral': 0, 'hummer  gmc  quiet revolution': 1, 'heinz ketchup  find the goodness  four at once': 2, 'reese s  rock': 3, 'pop-tarts  pop-tarts fixed the pretzel commercial': 4, 'turbotax  turbotax  all people are tax people remix': 5, 'microsoft surface  be the one': 6, 'genesis  going away party': 7, 'fox  toads  the masked singer': 8, 'weathertech  lucky dog': 9, 'michelob  6 for 6-pack': 10, 'facebook  ready to rock?': 11, 'new york life  love takes action': 12, 'nfl  inspire change  anquan boldin': 13, 'audi  let it go [t1]': 14, 'hyundai  smaht pahk': 15, 'doritos  the cool ranch': 16, 'pringles  the infinite dimensions of rick and morty': 17, 'marvel  black widow trailer': 18, 'tide  finally later': 19, 'avocados from mexico  the avocados from mexico shopping network': 20, 'google assistant  loretta': 21, 'little caesars pizza  best thing since sliced bread': 22, 'donald j. trump for president  criminal justice reform': 23, 'quibi  bank heist': 24, 'kia  tough never quits': 25, 'rocket mortgage  home': 26, 'fox  great american race': 27, 'fox nation  breaking news': 28, 'fox  super monday': 29, 'toyota  heroes': 30, 'jeep  groundhog day [t1]': 31, 'disney+  it s time': 32, 'cheetos  can t touch this': 33, 'coca-cola energy  show up': 34, 'pepsi zero sugar  zero sugar. done right.': 35, 'hard rock hotels & casinos  bling cup': 36, 'michelob  jimmy works it out': 37, 't-mobile  mama tests 5g': 38, 'fox  a run at history  daytona 500': 39, 'bud light seltzer  posty store  inside post s brain': 40, 'nfl  building a better game': 41, 'mountain dew': 42, 'walmart  famous visitors': 43, 'budweiser  typical american': 44, 'amazon prime video  hunters': 45, 'snickers  fix the world': 46, 'hulu  tom brady s big announcement': 47, 'procter & gamble  when we come together': 48, 'verizon  the amazing things 5g won t do': 49, 'squarespace  winona in winona': 50, 'discover card  yes weâ€™re accepted': 51, 'tide  bud knight': 52, 'olay  make space for women': 53, 'fox  halftime show  teaser_1': 54, 'fox  chosen  lego masters': 55, 'amazon echo  before alexa': 56, 'no time to die  trailer': 57, 'porsche  the heist': 58, 'sabra  how do you  mmus?': 59, 'none': 60}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "61\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQvTArLZhCUO",
        "outputId": "d39da072-fe7b-4ea0-8178-4d53a45ddd3b"
      },
      "source": [
        "from sentence_transformers import util\n",
        "\n",
        "tweets = df['text_clean'].values\n",
        "ad_names = df['ad_manual_adjusted'].values\n",
        "ad_keywords = list(ad_name_keywords_dict.values())\n",
        "\n",
        "twt_embeddings = model.encode(tweets, convert_to_tensor=True)\n",
        "ad_embeddings = model.encode(ad_keywords, convert_to_tensor=True)\n",
        "\n",
        "#Compute cosine-similarities for each sentence with each other sentence\n",
        "cosine_scores = util.pytorch_cos_sim(twt_embeddings, ad_embeddings)\n",
        "\n",
        "#Find the pairs with the highest cosine similarity scores\n",
        "pairs = []\n",
        "for i in range(len(cosine_scores)-1):\n",
        "    max_ad_keywords = ''\n",
        "    max_score = 0\n",
        "    ad_sentBERT = ''\n",
        "    max_score_ad_id = 0\n",
        "    for j in range(0, len(ad_embeddings)):\n",
        "        if(cosine_scores[i][j] > max_score):\n",
        "          max_score = cosine_scores[i][j]\n",
        "          max_ad_keywords = ad_keywords[j]\n",
        "          ad_sentBERT = ad_keywords_name_dict[max_ad_keywords]\n",
        "          max_score_ad_id = j\n",
        "        # pairs.append({'index': [i, j], 'score': cosine_scores[i][j],'tweet': tweets[i],  'ad_sentBERT_keywords': curr_ad_keywords,'ad_sentBERT': ad_keywords_name_dict[curr_ad_keywords] })\n",
        "    pairs.append({'index': [i, max_score_ad_id], 'score': max_score,'tweet': tweets[i],  'ad_sentBERT_keywords': max_ad_keywords,'ad_sentBERT': ad_sentBERT })\n",
        "#Sort scores in decreasing order\n",
        "#pairs = sorted(pairs, key= lambda x: x['score'], reverse=True)\n",
        "\n",
        "for pair in pairs[0:10]:\n",
        "    i, j = pair['index']\n",
        "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(tweets[i], ad_keywords[j], pair['score']))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "man wanna look brightside thought fringe playoff team year started shit \t\t fox super monday office toby toby flenderson \t\t Score: 0.6268\n",
            "harphampeg lynnelanae wgravelee darngoood franklingraham pepsi idea feel female given \t\t pop culture discover credit card mean girls ted noannualfeediscovercredit nodiscover \t\t Score: 0.7167\n",
            "footballpei see available silent auction tonight super bowl party please go facebook page \t\t inspire change police shootings black men nfl inspirechange \t\t Score: 0.6939\n",
            "budweiserusa party city heat superbowlliv weekend miami giving away limitededition bud bow \t\t rick morty adult swim pringles pickle rick rick morty new flavor summer rickmorty adultswim \t\t Score: 0.7421\n",
            "pair localjackson products getting superbowl rings callawaycharge breeland speaks bspeaks \t\t pop culture annual fee discover credit card chandler friends jack black mike myers austin powersnoannualfeediscovercredit yestodiscover \t\t Score: 0.7406\n",
            "stansdad j lo better sing favorite song halftimeshow jlo \t\t john legend chrissy teigen genesis hyundai kobe bryant young luxury gv sexiest man alive johnlegendchrissyteigen \t\t Score: 0.6892\n",
            "president donald trump scheduled attend north carolina opportunity summit charlotte north carolin \t\t donald trump trump change unemployment stronger safer prosperous trumpforpresident donaldtrump presidentrumpobama gop trumpsupporters \t\t Score: 0.6622\n",
            "tommyg legend tkelce superbowl champ \t\t premier boxing champions wilder vs fury ii \t\t Score: 0.6932\n",
            "ladygaga jlo shakira special guests incredible fun halftime show danced smiled whole \t\t fox halftime show j lo jennifer lopez shakira \t\t Score: 0.6007\n",
            "mitchharper andy reid steve young bout cougs shoutout dirty dan playing us beautiful \t\t john legend chrissy teigen genesis hyundai kobe bryant young luxury gv sexiest man alive johnlegendchrissyteigen \t\t Score: 0.7053\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0tjZSyVnuBb"
      },
      "source": [
        "df_pairs = pd.DataFrame(pairs, columns=['index', 'score', 'tweet', 'ad_sentBERT_keywords', 'ad_sentBERT'])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIljirCOA2ct",
        "outputId": "fcd25f5c-5e81-4385-ad6b-e4d98a69ac8e"
      },
      "source": [
        "df['ad_sentBERT'] = df_pairs['ad_sentBERT']\n",
        "df['score'] = df_pairs['score']\n",
        "df['match'] = df.apply(lambda row: 1 if row['ad_sentBERT'] == row['ad_manual_adjusted'] else 0, axis = 1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deJGuf-xlxV-",
        "outputId": "f2b5aef9-631a-4261-faa9-a2c99ce9e654"
      },
      "source": [
        "df['ad_sentBERT'].head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                    fox  super monday\n",
              "1                    discover card  yes weâ€™re accepted\n",
              "2                   nfl  inspire change  anquan boldin\n",
              "3    pringles  the infinite dimensions of rick and ...\n",
              "4        discover card  no we donâ€™t charge annual fees\n",
              "Name: ad_sentBERT, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dO3POKQINji9",
        "outputId": "726685a3-8f4d-4d25-8e5b-b4aaf87e27c1"
      },
      "source": [
        "print(df.shape)\n",
        "print(df['match'].sum())\n",
        "print(df['match'].sum()*100/ df.shape[0])\n",
        "df.to_csv('./sent_bert_results.csv')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5824, 26)\n",
            "109\n",
            "1.871565934065934\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Os1XcxU4KWc"
      },
      "source": [
        "**Train the model**\n",
        "\n",
        "Loss Function - CosineSimilarityLoss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x97BgUcj3J0N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "37b99feb-bd86-4ef7-b0b5-d5e5e9df53ee"
      },
      "source": [
        "# train_examples = [InputExample(texts=['My first sentence', 'My second sentence'], label=0.8),\n",
        "#    InputExample(texts=['Another pair', 'Unrelated sentence'], label=0.3)]\n",
        "\n",
        "# train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
        "# train_loss = losses.CosineSimilarityLoss(model)\n",
        "\n",
        "# model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100, evaluator=evaluator, evaluation_steps=500)\n",
        "# #           ,output_path=model_save_path)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-6785d3b2663f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCosineSimilarityLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_objectives\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#           ,output_path=model_save_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'evaluator' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ4CksHY3His"
      },
      "source": [
        "**Evaluate model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Loovr8e2xY4"
      },
      "source": [
        "# from sentence_transformers import evaluation\n",
        "# sentences1 = ['This list contains the first column', 'With your sentences', 'You want your model to evaluate on']\n",
        "# sentences2 = ['Sentences contains the other column', 'The evaluator matches sentences1[i] with sentences2[i]', 'Compute the cosine similarity and compares it to scores[i]']\n",
        "# scores = [0.3, 0.6, 0.2]\n",
        "\n",
        "# evaluator = evaluation.EmbeddingSimilarityEvaluator(sentences1, sentences2, scores)\n",
        "# #evaluator = EmbeddingSimilarityEvaluator.from_input_examples(sts_reader.get_examples('sts-dev.csv'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quFiqB42bbjG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8t7yOy4Vbeoe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}