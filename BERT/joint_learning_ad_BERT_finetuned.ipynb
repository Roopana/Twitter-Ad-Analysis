{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"joint_learning_ad_product_BERT_FineTuned.ipynb","provenance":[{"file_id":"1YsD_ZJAMDkZDvUMz5wqnpcC6IZC1-5uG","timestamp":1599619054270},{"file_id":"1iyk0gHvox5IxQzGx29Rm0Cf6JyzDn2mS","timestamp":1599608606258}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"42e193cdace64fdda6aabf8963fb1726":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_311f7a48476d400f9c68b18cf4d435bf","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_abb1a1b2bc534062825afa93e9cc49c9","IPY_MODEL_fd0471795cb54424896c659d8d8db399"]}},"311f7a48476d400f9c68b18cf4d435bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"abb1a1b2bc534062825afa93e9cc49c9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_57ac71dcc2c943679389f01b2a84b13d","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9fd03fe258c44078b8554c09f2729034"}},"fd0471795cb54424896c659d8d8db399":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b12b288523324a52835e06f69ed03e43","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:00&lt;00:00, 643kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_da3f865dc37040728430a19fb122dafa"}},"57ac71dcc2c943679389f01b2a84b13d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9fd03fe258c44078b8554c09f2729034":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b12b288523324a52835e06f69ed03e43":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"da3f865dc37040728430a19fb122dafa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0174e78ae92747f1b2500b704c283711":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_06f30295d2c5464e93b517eb3ea85be9","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_208b39486536463f828bd67be257e41e","IPY_MODEL_cb952d83621e47a794b9a5d44eccd348"]}},"06f30295d2c5464e93b517eb3ea85be9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"208b39486536463f828bd67be257e41e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f51bc37b30dd4d25a7ccffc9c15d375a","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":433,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":433,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_181652e059104d15b707aec9c3109b46"}},"cb952d83621e47a794b9a5d44eccd348":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f0f3e84761a54955b5be270372fae331","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 433/433 [00:41&lt;00:00, 10.6B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bac73a31485743fc9c74dcacca95369e"}},"f51bc37b30dd4d25a7ccffc9c15d375a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"181652e059104d15b707aec9c3109b46":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f0f3e84761a54955b5be270372fae331":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"bac73a31485743fc9c74dcacca95369e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d0f553c2176d4f979dcc91cef8acd1a3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6346ef7b71e045c991caa3a92b90d152","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c86f72d0ef3f46828636a4a3a0c2a106","IPY_MODEL_a670018c96954bd296b57b9a14202e00"]}},"6346ef7b71e045c991caa3a92b90d152":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c86f72d0ef3f46828636a4a3a0c2a106":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d2a947a6349744d9bf22f662ae757f35","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":440473133,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":440473133,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5398dbeb40004f41ba0936d069f58fe7"}},"a670018c96954bd296b57b9a14202e00":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_776fe15c66b44b69ae123e72f3c4e1c7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 440M/440M [00:21&lt;00:00, 20.9MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f1ee41c4955d4d10b3e516364a5502f5"}},"d2a947a6349744d9bf22f662ae757f35":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5398dbeb40004f41ba0936d069f58fe7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"776fe15c66b44b69ae123e72f3c4e1c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f1ee41c4955d4d10b3e516364a5502f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"7UZGfwZqK0Nl"},"source":["# Code for BERT-Joint Leraning Model \n","# to classify tweets as corresponding ad related\n","# accuracy = >86% \n","# data : superbowl"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g0js6FkReDxH","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600538222835,"user_tz":300,"elapsed":7899,"user":{"displayName":"Roopana Chenchu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhSh75QN1jzZ_cyXlnOrn2AdzeDgk7tzwAvqiW=s64","userId":"09352419684951286614"}},"outputId":"ef549e01-d0ab-4f02-c67f-463450250164"},"source":["#Confirm that GPU is detected\n","import tensorflow as tf\n","# Get the GPU device name.\n","device_name = tf.test.gpu_device_name()\n","# The device name should look like the following:\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pcxJDqIzUpVX"},"source":["import os\n","\n","CUDA_LAUNCH_BLOCKING=1\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Y0GAwWze4an"},"source":["In order for torch to use the GPU, we need to identify and specify the GPU as the device. Later, in our training loop, we will load data onto the device."]},{"cell_type":"code","metadata":{"id":"Iw86lColeSd8","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1600538226813,"user_tz":300,"elapsed":11826,"user":{"displayName":"Roopana Chenchu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhSh75QN1jzZ_cyXlnOrn2AdzeDgk7tzwAvqiW=s64","userId":"09352419684951286614"}},"outputId":"ef4de509-56c2-4256-8f4e-6e7a163bb4e7"},"source":["import torch\n","# If there's a GPU available...\n","isGPUavailable = False\n","if torch.cuda.is_available():    \n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","    isGPUavailable = True\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla K80\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Lkcj_vj8e6_N","colab":{"base_uri":"https://localhost:8080/","height":615},"executionInfo":{"status":"ok","timestamp":1600538234321,"user_tz":300,"elapsed":19325,"user":{"displayName":"Roopana Chenchu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhSh75QN1jzZ_cyXlnOrn2AdzeDgk7tzwAvqiW=s64","userId":"09352419684951286614"}},"outputId":"11998c2d-7432-4d8a-ce17-558f545b6e9a"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/05/c8c55b600308dc04e95100dc8ad8a244dd800fe75dfafcf1d6348c6f6209/transformers-3.1.0-py3-none-any.whl (884kB)\n","\r\u001b[K     |▍                               | 10kB 17.9MB/s eta 0:00:01\r\u001b[K     |▊                               | 20kB 3.6MB/s eta 0:00:01\r\u001b[K     |█▏                              | 30kB 3.9MB/s eta 0:00:01\r\u001b[K     |█▌                              | 40kB 4.4MB/s eta 0:00:01\r\u001b[K     |█▉                              | 51kB 4.7MB/s eta 0:00:01\r\u001b[K     |██▎                             | 61kB 4.9MB/s eta 0:00:01\r\u001b[K     |██▋                             | 71kB 5.2MB/s eta 0:00:01\r\u001b[K     |███                             | 81kB 5.3MB/s eta 0:00:01\r\u001b[K     |███▍                            | 92kB 5.3MB/s eta 0:00:01\r\u001b[K     |███▊                            | 102kB 5.3MB/s eta 0:00:01\r\u001b[K     |████                            | 112kB 5.3MB/s eta 0:00:01\r\u001b[K     |████▌                           | 122kB 5.3MB/s eta 0:00:01\r\u001b[K     |████▉                           | 133kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 143kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 153kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 163kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 174kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 184kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 194kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 204kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 215kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 225kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 235kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████                       | 245kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 256kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 266kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 276kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 286kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 296kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 307kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 317kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 327kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 337kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 348kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 358kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 368kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 378kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 389kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 399kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 409kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 419kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 430kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████                | 440kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 450kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 460kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 471kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 481kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 491kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 501kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 512kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 522kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 532kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 542kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 552kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 563kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 573kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 583kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 593kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 604kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 614kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 624kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 634kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 645kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 655kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 665kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 675kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 686kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 696kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 706kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 716kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 727kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 737kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 747kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 757kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 768kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 778kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 788kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 798kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 808kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 819kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 829kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 839kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 849kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 860kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 870kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 880kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 890kB 5.3MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Collecting tokenizers==0.8.1.rc2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 17.6MB/s \n","\u001b[?25hCollecting sentencepiece!=0.1.92\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 42.6MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 42.9MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=d2fbece3d8fbd2182a100ed3edc141ee4bb3c536038ab145d18e39134defe2a6\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"W42_2PYqCgH7","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600538234357,"user_tz":300,"elapsed":19353,"user":{"displayName":"Roopana Chenchu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhSh75QN1jzZ_cyXlnOrn2AdzeDgk7tzwAvqiW=s64","userId":"09352419684951286614"}},"outputId":"a04bfd2c-9c2a-48ef-e3fe-21545fc2050d"},"source":["import pandas as pd\n","\n","df_2 = pd.read_csv(\"./2_man_ann_sb.csv\",  index_col= None)\n","df_2 = df_2.dropna(subset = ['tweet_text'])\n","\n","df_1 = pd.read_csv(\"./mann_ann_sb.csv\", index_col= None)\n","\n","df_3 = pd.read_csv(\"./3_man_ann_sb_full_1.csv\", index_col = None)\n","df_3 = df_3.dropna(subset = ['tweet_text'])\n","\n","df_raw = df_1.append(df_2).append(df_3) # using batch 1 and batch 2 for training\n","\n","print(df_raw.shape)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(7500, 19)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"40oqlyJtZ8RO"},"source":["Data Cleaning"]},{"cell_type":"code","metadata":{"id":"9GZHKAseX50Z","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1600538354702,"user_tz":300,"elapsed":139680,"user":{"displayName":"Roopana Chenchu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhSh75QN1jzZ_cyXlnOrn2AdzeDgk7tzwAvqiW=s64","userId":"09352419684951286614"}},"outputId":"d0d3b79d-9801-43e1-dc70-9754e86284e4"},"source":["# Remove ads marked as below because they are not available in ad annotations file although tweets mention them\n","# commercials, joe biden, pizzahut, joe bieden, michael bloomberg, mike bloomberg, scientology...\n","ads_remove = ['commercials', 'joe biden', 'pizzahut', 'joe bieden', 'michael bloomberg', 'mike bloomberg', \n","              'scientology','papa johns',  'bakari',  'secret', 'dashlane', 'bernie the peoples perfume',\n","              'ram trucks', 'golden gronks', \"bush's best\", 'ragged old flag', 'patience', 'guitar hero',\n","              'disney mulan']\n","\n","# ads_rename = ['nfl100', 'tide']\n","\n","# rename ads with spelling faults while manually adding the annotation \n","\n","df_raw['ad_manual_adjusted'] = df_raw['ad_manual_adjusted'].apply(lambda x: x.lower())\n","df_raw.loc[df_raw.ad_manual_adjusted == \"discover card  no we don‚äôt charge annual fees\", \n","       \"ad_manual_adjusted\"] = \"discover card  no we don’t charge annual fees\"\n","df_raw.loc[df_raw.ad_manual_adjusted == \"doritos the cool ranch\", \n","       \"ad_manual_adjusted\"] = \"doritos  the cool ranch\"\n","df_raw.loc[df_raw.ad_manual_adjusted == \"discover card yes we're accepted\", \n","       \"ad_manual_adjusted\"] =  \"discover card  yes we’re accepted\"\n","df_raw.loc[df_raw.ad_manual_adjusted == \"discover card yes we’re accepted\", \n","       \"ad_manual_adjusted\"] =  \"discover card  yes we’re accepted\"\n","df_raw.loc[df_raw.ad_manual_adjusted == \"discover card  yes we're accepted\", \n","       \"ad_manual_adjusted\"] =  \"discover card  yes we’re accepted\"\n","df_raw.loc[df_raw.ad_manual_adjusted == \"budweiser typical american\", \n","       \"ad_manual_adjusted\"] = \"budweiser  typical american\"\n","df_raw.loc[df_raw.ad_manual_adjusted == 'fox  halftime show  teaser_3',\n","            \"ad_manual_adjusted\"] = \"fox  halftime show  teaser_1\"\n","df_raw.loc[df_raw.ad_manual_adjusted == 'fox  halftime show  teaser_2',\n","            \"ad_manual_adjusted\"] = \"fox  halftime show  teaser_1\"\n","            \n","print(df_raw.shape)\n","df = pd.DataFrame()\n","removed_Data = pd.DataFrame()\n","\n","for i,row  in df_raw.iterrows():\n","  if row['ad_manual_adjusted'] not in ads_remove:\n","    df = df.append(row)\n","  else:\n","    removed_Data = removed_Data.append(row)\n","print(df.shape)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(7500, 19)\n","(7394, 19)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TRuAKLCt5d0M"},"source":["classType = 'sent_exploded' # binary or multi-class or sent-exploded"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XZBd9hvF28UI","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1600538355379,"user_tz":300,"elapsed":140339,"user":{"displayName":"Roopana Chenchu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhSh75QN1jzZ_cyXlnOrn2AdzeDgk7tzwAvqiW=s64","userId":"09352419684951286614"}},"outputId":"e74ebf54-d051-43f3-a884-ad6aab2bb5da"},"source":["from nltk.corpus import stopwords\n","import re\n","import nltk\n","nltk.download('stopwords')\n","\n","stop = stopwords.words('english')\n","# stop.append('superbowl')\n","# stop.append('super') \n","# stop.append('bowl')\n","\n","# remove for multi class since almost all ads have these words\n","if classType is not 'binary':\n","  stop.append('commercial')\n","  stop.append('ad')\n","  stop.append('commercials')\n","  stop.append('ads')\n","print(len(stop))\n","\n","def removeMentions(text):\n","\n","    textBeforeMention = text.partition(\"@\")[0]\n","    textAfterMention = text.partition(\"@\")[2]\n","    textAfterMention =  re.sub(r':', '', textAfterMention) #cadillac join the 31k\n","    tHandle = textAfterMention.partition(\" \")[0].lower() #cadillac    \n","    text = textBeforeMention+ \" \" + textAfterMention  \n","    return text\n","\n","def cleanTweet(strinp):\n","    strinp = re.sub(r'RT', \"\", strinp) # Remove RT\n","    strinp = strinp.lower()\n","    \n","    stop_removed_list = [word for word in strinp.split() if word not in (stop)]\n","    stop_removed = ' '.join([str(elem) for elem in stop_removed_list])    \n","    text = re.sub('https?://[A-Za-z0-9./]+', ' ', stop_removed) # Remove URLs\n","    text = removeMentions(text)\n","    text = re.sub('[^\\x00-\\x7F]+', ' ', text) # Remove non-ASCII chars.\n","    \n","    # remove punctuations except '-'\n","    punctuation = ['(', ')', '[',']','?', ':', ':', ',', '.', '!', '/', '\"', \"'\", '@', '#', '&', '-', '_']\n","    text = \"\".join((char for char in text if char not in punctuation))\n","    text = re.sub('[^a-zA-Z]', ' ', text) # remove all other than alphabet chars \n","\n","#     text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text) # remove all single characters     \n","    stop_removed_l = [word for word in text.split() if word not in (stop)]\n","    stop_removed = ' '.join([str(elem) for elem in stop_removed_l]) \n","    return stop_removed\n","\n","print(cleanTweet(\"RT @cadillacabc: Joinrt the 31K james_bond\") )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","183\n","cadillacabc joinrt k jamesbond\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3hNx_DQu3khZ"},"source":["df['text_clean'] = df['tweet_text'].apply(lambda x: cleanTweet(x))\n","df['ad_manual_adjusted'] = df['ad_manual_adjusted'].apply(lambda x: x.lower())\n","df['ad_related'] = df['ad_manual_adjusted'].apply(lambda ad: 0 if ad == 'none' else 1)\n","\n","comma_filter = ~df['ad_manual_adjusted'].str.contains(',')\n","df = df[comma_filter]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RWctf0pwduYj","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1600538358682,"user_tz":300,"elapsed":143627,"user":{"displayName":"Roopana Chenchu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhSh75QN1jzZ_cyXlnOrn2AdzeDgk7tzwAvqiW=s64","userId":"09352419684951286614"}},"outputId":"544d3673-f5cd-4cda-c5b5-11d3fd78e4c3"},"source":["df_unique = df.drop_duplicates(subset = ['text_clean'])\n","df_with_dupes = df\n","df = df_unique\n","\n","print(df_with_dupes.shape)\n","print(df.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(7332, 21)\n","(5824, 21)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_t2znfvajEpz","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1600538958057,"user_tz":300,"elapsed":13798,"user":{"displayName":"Roopana Chenchu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhSh75QN1jzZ_cyXlnOrn2AdzeDgk7tzwAvqiW=s64","userId":"09352419684951286614"}},"outputId":"fb28e129-9255-45a8-f34b-5990aadb97e3"},"source":["#tweets that are not manually annotated\n","remaining_tweets = pd.read_csv(\"./remaining_tweets_non_man.csv\")\n","print(remaining_tweets.columns)\n","\n","remaining_tweets['text_clean'] = remaining_tweets['tweet_text'].apply(lambda x: cleanTweet(x))\n","print(remaining_tweets.shape)\n","\n","remaining_tweets = remaining_tweets.drop_duplicates(subset = ['text_clean'])\n","print(remaining_tweets.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(108235, 10)\n","(52085, 10)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ehUMO3bPLKSF","colab":{"base_uri":"https://localhost:8080/","height":258},"executionInfo":{"status":"ok","timestamp":1600538358683,"user_tz":300,"elapsed":143617,"user":{"displayName":"Roopana Chenchu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhSh75QN1jzZ_cyXlnOrn2AdzeDgk7tzwAvqiW=s64","userId":"09352419684951286614"}},"outputId":"72e48fcc-6ef6-4ab0-91eb-cbe0c74773ec"},"source":["ad_product_df = pd.read_csv('./SB_ad_annotations_product_category_modified.csv')\n","ad_product_df = ad_product_df.rename(columns = {'Ad Name': 'Ad_Name'}) # rename the column to remove space\n","ad_product_df = ad_product_df.dropna() # because the file has trailing empty rows, remove them\n","# remove fox half time show teaser_2 ad because its keywords is same as fox half time show teaser_2\n","print(ad_product_df.shape)\n","ad_product_df.drop(ad_product_df[ad_product_df['Ad_Name'] == 'FOX  Halftime Show  Teaser_2'].index, inplace = True) \n","print(ad_product_df.shape)\n","ad_product_dict = dict()\n","\n","ad_product_df['Product_modified'] = ad_product_df['Product_modified'].apply(lambda x: x.lower())\n","for i, row in ad_product_df.iterrows():\n","  ad_product_dict[row['Ad_Name'].lower()] = row['Product_modified'].lower()\n","\n","ad_product_dict['none'] = 'none'\n","print(ad_product_dict)\n","\n","df['product_modified'] = df['ad_manual_adjusted'].apply(lambda ad: ad_product_dict[ad])\n","df['product_modified'] = df['product_modified'].apply(lambda x: x.lower())\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(75, 11)\n","(74, 11)\n","{'fast & furious 9  trailer': 'movie trailer', 'quibi  bank heist': 'tech company', 'tide  when is later  masked singer': 'laundry detergent', 'fox  a run at history  daytona 500': 'sporting event', 'donald j. trump for president  criminal justice reform': 'political campaign', 'walmart  famous visitors': 'tech company', 'marvel  black widow trailer': 'movie trailer', 'rocket mortgage  home': 'money', 'porsche  the heist': 'car', 'snickers  fix the world': 'food', 'hulu  tom brady s big announcement': 'streaming service', 'fox  chosen  lego masters': 'tv show/network', 'mountain dew': 'pop/soda', 'squarespace  winona in winona': 'tech company', 'new york life  love takes action': 'money', 'fox  super monday': 'tv show/network', 'hyundai  smaht pahk': 'car', 'cheetos  can t touch this': 'food', 'olay  make space for women': 'charitable program', 'fox  halftime show  teaser_1': 'half-time show', 'michelob  6 for 6-pack': 'charitable program/ alcohol', 'avocados from mexico  the avocados from mexico shopping network': 'food', 'hard rock hotels & casinos  bling cup': 'hotel', 'pringles  the infinite dimensions of rick and morty': 'food', 'turbotax  turbotax  all people are tax people remix': 'money', 'tide  bud knight': 'laundry detergent', 'genesis  going away party': 'car', 'coca-cola energy  show up': 'soda', 'planters  baby funeral': 'food', 'no time to die  trailer': 'movie trailer', 'fox  toads  the masked singer': 'tv show/network', 'google assistant  loretta': 'tech company', 'sabra  how do you  mmus?': 'food', 'weathertech  lucky dog': 'charitable program', 'verizon  the amazing things 5g won t do': 'phone network', 'hummer  gmc  quiet revolution': 'car', 'pop-tarts  pop-tarts fixed the pretzel commercial': 'food', 'minions  the rise of gru  trailer': 'movie trailer', 'fox nation  breaking news': 'tv show/network', 'premier boxing champions  wilder vs. fury ii_1': 'sporting event', 'homeland  trailer': 'tv show/network', 'fox  great american race': 'sporting event', 'amazon prime video  hunters': 'tv show/network', 'pepsi zero sugar  zero sugar. done right.': 'pop/soda', 'heinz ketchup  find the goodness  four at once': 'food', 'premier boxing champions  wilder vs. fury ii_2': 'sporting event', 'bud light seltzer  posty store  inside post s brain': 'alcohol', 'little caesars pizza  best thing since sliced bread': 'food', 'doritos  the cool ranch': 'food', 'kia  tough never quits': 'car', 'turkish airlines  step on earth': 'airline', 'reese s  rock': 'food', 'tide  ww': 'laundry detergent', 'amazon echo  before alexa': 'tech company', 'michelob  jimmy works it out': 'alcohol', 'nfl  inspire change  anquan boldin': 'charitable program', 'toyota  heroes': 'car', 'discover card  no we don’t charge annual fees': 'money', 'disney+  it s time': 'streaming service', 'discover card  yes we’re accepted': 'money', 'fox  football withdrawal syndrome': 'sporting event', 't-mobile  mama tests 5g': 'phone network', 'fox  tornado  tomorrow': 'tv show/network', 'fox  fun for all  the masked singer and lego': 'tv show/network', 'nfl  building a better game': 'charitable program', 'budweiser  typical american': 'alcohol', 'procter & gamble  when we come together': 'tech company', 'fox  not just another race': 'sporting event', 'microsoft surface  be the one': 'tech company', 'fox  i m scared': 'tv show/network', 'jeep  groundhog day [t1]': 'car', 'facebook  ready to rock?': 'social network', 'tide  finally later': 'laundry detergent', 'audi  let it go [t1]': 'car', 'none': 'none'}\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"t5jTn2nCzYuL","colab":{"base_uri":"https://localhost:8080/","height":403},"executionInfo":{"status":"ok","timestamp":1600538358685,"user_tz":300,"elapsed":143612,"user":{"displayName":"Roopana Chenchu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhSh75QN1jzZ_cyXlnOrn2AdzeDgk7tzwAvqiW=s64","userId":"09352419684951286614"}},"outputId":"39694431-5ef7-4cb0-fe91-129d5bac45b0"},"source":["# group the keywords of ads part of a product bucket and append these to the training sentences\n","ad_product_keywords_dict = ad_product_df.groupby('Product_modified')['Key Terms  Round 2'].agg(lambda x : x.sum() if x.dtype=='float64' else ' '.join(x))\n","# clean the ad key words - not removing duplicate words here - TODO\n","for ad_bucket in ad_product_keywords_dict.keys():\n","  ad_product_keywords_dict[ad_bucket] = cleanTweet(ad_product_keywords_dict[ad_bucket])\n","ad_product_df['product_modified_keywords'] = ad_product_df['Product_modified'].apply(lambda x: ad_product_keywords_dict[x])\n","ad_product_df.head(2)\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Ad Number</th>\n","      <th>Brand Name</th>\n","      <th>Ad_Name</th>\n","      <th>Product_modified</th>\n","      <th>Product</th>\n","      <th>Key Terms  Round 1</th>\n","      <th>Key Terms  Round 2</th>\n","      <th>Excitatory Potential</th>\n","      <th>Emotional vs. Rational</th>\n","      <th>Semantic Affinity</th>\n","      <th>Valence</th>\n","      <th>product_modified_keywords</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.0</td>\n","      <td>Trailer</td>\n","      <td>Fast &amp; Furious 9  Trailer</td>\n","      <td>movie trailer</td>\n","      <td>Movie Trailer</td>\n","      <td>fast and the furious, fast &amp; the furious, fast...</td>\n","      <td>fast and the furious, fast &amp; the furious, ff9,...</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>fast furious fast furious ff f fast saga vin d...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2.0</td>\n","      <td>Quibi</td>\n","      <td>Quibi  Bank Heist</td>\n","      <td>tech company</td>\n","      <td>Video Platform</td>\n","      <td>quibi, bank heist, robbery, less than ten minu...</td>\n","      <td>quibi, bank heist, robbery, less than ten minu...</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>quibi bank heist robbery less ten minutes quic...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Ad Number  ...                          product_modified_keywords\n","0        1.0  ...  fast furious fast furious ff f fast saga vin d...\n","1        2.0  ...  quibi bank heist robbery less ten minutes quic...\n","\n","[2 rows x 12 columns]"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"AGnpExcJNbF6","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1600538358749,"user_tz":300,"elapsed":143669,"user":{"displayName":"Roopana Chenchu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhSh75QN1jzZ_cyXlnOrn2AdzeDgk7tzwAvqiW=s64","userId":"09352419684951286614"}},"outputId":"c52b7548-6a71-43e1-b108-4f987e695145"},"source":["# generate adname, ad keywords dict to use in sent exploding\n","ad_name_keywords_dict = dict()\n","for i, row in ad_product_df.iterrows():\n","  ad_name_keywords_dict[row['Ad_Name'].lower()] = cleanTweet(row['Key Terms  Round 2'])\n","ad_name_keywords_dict['none'] = 'none'\n","print(ad_name_keywords_dict)\n","\n","ad_keywords_name_dict = dict()\n","for ad_name in ad_name_keywords_dict:\n","  keywords_temp = ad_name_keywords_dict[ad_name]\n","  ad_keywords_name_dict[keywords_temp] = ad_name\n","print(ad_keywords_name_dict)\n","\n","ad_product_df['ad_name_keywords'] = ad_product_df['Ad_Name'].apply(lambda x: ad_name_keywords_dict[x.lower()])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'fast & furious 9  trailer': 'fast furious fast furious ff f fast saga vin diesel flying truck stunts michelle rodriguez fastfurious', 'quibi  bank heist': 'quibi bank heist robbery less ten minutes quick bites big stories chance rapper megan thee stallion chancetherappermeganstallion quickbites', 'tide  when is later  masked singer': 'tide laundry laundry detergent schitt emily hampshire charlie day walts', 'fox  a run at history  daytona 500': 'fox show daytona run history great american race', 'donald j. trump for president  criminal justice reform': 'donald trump trump change unemployment stronger safer prosperous trumpforpresident donaldtrump presidentrumpobama gop trumpsupporters', 'walmart  famous visitors': 'walmart pickup spaceship toy story buzz lightyear marvin martian marvin martians arrival glass cleaners aliens men black groot lego star wars r bill bill ted flash gordon', 'marvel  black widow trailer': 'marvel black widow scarlett johansson', 'rocket mortgage  home': 'quicken loans aquaman jason momoa rocket mortgagemomoabowl jasonmomoa rocketmorgage', 'porsche  the heist': 'porsche volkswagen chase fast furious taycan turbo scar chase taycanturbo', 'snickers  fix the world': 'snickers snickersfixtheworld fix world snickershole fixtheworld', 'hulu  tom brady s big announcement': 'hulu tom brady tombrady', 'fox  chosen  lego masters': 'fox chosen lego masters competition arnett', 'mountain dew': 'mountain dew mtn dew soda zero sugar shining work play makes jack dull boy new mtn dew zero sugarbryan cranston mtndew theshining', 'squarespace  winona in winona': 'squarespace winona winona ryder minnesota', 'new york life  love takes action': 'love takes action greek words love agape new york life take action lovetakesaction good life nyl newyorklife', 'fox  super monday': 'fox super monday office toby toby flenderson', 'hyundai  smaht pahk': 'chris evans captain america john krasinki rachel dratch boston red sox david ortiz boston accent smart parking hyundai better way park bettahdriveus chrisevans smahtpark bettahwaytopark', 'cheetos  can t touch this': 'mc hammer cant touch popcorn cheetos cheetos thing cheetle canttouchthismchammer', 'olay  make space for women': 'allfemale spacewalk lilly singh busy phillips nicole stott taraji henson katie couric olay make space women makespaceforwomen olay', 'fox  halftime show  teaser_1': 'fox halftime show j lo jennifer lopez shakira', 'michelob  6 for 6-pack': 'help farmers organic pack square footmake impact michelob gold michelobgold pack sixforsixpack', 'avocados from mexico  the avocados from mexico shopping network': 'molly ringwald avocados mexico avonetwork avocarriermollyringwald', 'hard rock hotels & casinos  bling cup': 'hard rock international hard rock stadium miami kobe bryant jennifer lopez jlo hardrock hardrockstadium hardrockcasino', 'pringles  the infinite dimensions of rick and morty': 'rick morty adult swim pringles pickle rick rick morty new flavor summer rickmorty adultswim', 'turbotax  turbotax  all people are tax people remix': 'turbotax people app mobile feature taxes tax people people tax people allpeoplearetaxpeople', 'tide  bud knight': 'tide laundry laundry detergent charliey day bud knight stainlaundrydetergent charlieday', 'genesis  going away party': 'john legend chrissy teigen genesis hyundai kobe bryant young luxury gv sexiest man alive johnlegendchrissyteigen', 'coca-cola energy  show up': 'coke iphone coca cola coca cola energy showup dotscococolacocacolaenergy jonah hill martin scorsese', 'planters  baby funeral': 'planters nutsmrpeanut matt walsh wesley snipesbabynut babypeanut babyfuneral baby nut', 'no time to die  trailer': 'james bond time die daniel craig', 'fox  toads  the masked singer': 'fox masked singer toads', 'google assistant  loretta': 'google assistant virtual assistant googleassistant virtualassistant', 'sabra  how do you  mmus?': 'sabra humus hummus wives real housewives new jersey real housewives caroline manzo teresa giudice tpain mix cracker kim chi drag queensdragqueens dragqueenamanda cerny howimmus', 'weathertech  lucky dog': 'weathertech pets golden retriever dog', 'verizon  the amazing things 5g won t do': 'verizon phone service phone g', 'hummer  gmc  quiet revolution': 'general motorshummer hummer ev electric pickup zero emission lebron james basketball lebronjames', 'pop-tarts  pop-tarts fixed the pretzel commercial': 'pop tarts queer eye jonathan van ness pretzel queereye poptarts', 'minions  the rise of gru  trailer': 'minions rise gru', 'fox nation  breaking news': 'fox fox nation breaking news', 'premier boxing champions  wilder vs. fury ii_1': 'premier boxing champions wilder vs fury ii', 'homeland  trailer': 'homeland trailer', 'fox  great american race': 'fox daytona great american race run history', 'amazon prime video  hunters': 'hunters nazi al pacino nazi hunters prime video amazon barbeque barbecue alpacinoprimevideo hunterstv', 'pepsi zero sugar  zero sugar. done right.': 'missy elliott pepsi zero sugar paint black pepsi pepsizero nocoke', 'heinz ketchup  find the goodness  four at once': 'find goodness heinz ketchup stranger things strangerthings findthegoodness', 'premier boxing champions  wilder vs. fury ii_2': 'premier boxing champions wilder vs fury ii wrestling', 'bud light seltzer  posty store  inside post s brain': 'bud light bud light seltzer post malone anheuserbusch inbev hard seltzer postmalone budlightbudweiser', 'little caesars pizza  best thing since sliced bread': 'office rainn wilson delivery best thing since sliced bread pizza pizza pizza littlecaesars caesars', 'doritos  the cool ranch': 'sam elliott lil nas x old town road doritos cowboy cool ranch dancer billy ray cyrus wild west wild wild west make move', 'kia  tough never quits': 'josh jacobs running back kia kia seltosraiders give everything joshjacobs kiaseltos', 'turkish airlines  step on earth': 'turkish airlines airline flight astronauts rocket', 'reese s  rock': 'reese take five take candybarreeses', 'tide  ww': 'tide stain later charlie day wonder woman', 'amazon echo  before alexa': 'ellen degeneres alexa porta de rossi maid medieval dragon amazon alexa middle ages beforealexa', 'michelob  jimmy works it out': 'anheuserbusch inbev michelob ultrabeer jimmy fallon working gym john cena usain bolt brooks koepka kerri walsh jennings worth enjoy low carbs jimmyfallon usainbolt workingout gymbody', 'nfl  inspire change  anquan boldin': 'inspire change police shootings black men nfl inspirechange', 'toyota  heroes': 'toyota car suv highlander toyotahighlandercobiesmulders cobie smulders', 'discover card  no we don’t charge annual fees': 'pop culture annual fee discover credit card chandler friends jack black mike myers austin powersnoannualfeediscovercredit yestodiscover', 'disney+  it s time': 'disney plus falcon wanda vision wanda loki', 'discover card  yes we’re accepted': 'pop culture discover credit card mean girls ted noannualfeediscovercredit nodiscover', 'fox  football withdrawal syndrome': 'football withdrawal system science doctor withdrawal symptoms', 't-mobile  mama tests 5g': 'tmobile mama tests g g anthony anderson', 'fox  tornado  tomorrow': 'fox tornado lone star', 'fox  fun for all  the masked singer and lego': 'fox masked singer lego masters', 'nfl  building a better game': 'nfl building better game amazon web services', 'budweiser  typical american': 'kathryn bigelow budweiser american spirit typical american kindness beer stereotypes stereotypical typicalamerican americanspirit', 'procter & gamble  when we come together': 'procter gamble come together sofia vergara chili bounty old spice head shoulders olay charming febreze', 'fox  not just another race': 'fox daytona great american race run history', 'microsoft surface  be the one': 'woman coach ers katie sowers surface female coach microsoft first female coach katiesowers erscoach microsoftsurface femalecoach', 'fox  i m scared': 'fox lone star storm tornado', 'jeep  groundhog day [t1]': 'jeep groundhog day jeep gladiator bill murray', 'facebook  ready to rock?': 'chris rock sylvester stallone groups facebook rock music stonehenge rock rocketchrisrock', 'tide  finally later': 'tide tide pod laundry detergent emily hampshire charlie day finally later', 'audi  let it go [t1]': 'maisie williams game thrones let go frozenaudi etron sportback traffic letitgo maisiewilliamsgameofthrones', 'none': 'none'}\n","{'fast furious fast furious ff f fast saga vin diesel flying truck stunts michelle rodriguez fastfurious': 'fast & furious 9  trailer', 'quibi bank heist robbery less ten minutes quick bites big stories chance rapper megan thee stallion chancetherappermeganstallion quickbites': 'quibi  bank heist', 'tide laundry laundry detergent schitt emily hampshire charlie day walts': 'tide  when is later  masked singer', 'fox show daytona run history great american race': 'fox  a run at history  daytona 500', 'donald trump trump change unemployment stronger safer prosperous trumpforpresident donaldtrump presidentrumpobama gop trumpsupporters': 'donald j. trump for president  criminal justice reform', 'walmart pickup spaceship toy story buzz lightyear marvin martian marvin martians arrival glass cleaners aliens men black groot lego star wars r bill bill ted flash gordon': 'walmart  famous visitors', 'marvel black widow scarlett johansson': 'marvel  black widow trailer', 'quicken loans aquaman jason momoa rocket mortgagemomoabowl jasonmomoa rocketmorgage': 'rocket mortgage  home', 'porsche volkswagen chase fast furious taycan turbo scar chase taycanturbo': 'porsche  the heist', 'snickers snickersfixtheworld fix world snickershole fixtheworld': 'snickers  fix the world', 'hulu tom brady tombrady': 'hulu  tom brady s big announcement', 'fox chosen lego masters competition arnett': 'fox  chosen  lego masters', 'mountain dew mtn dew soda zero sugar shining work play makes jack dull boy new mtn dew zero sugarbryan cranston mtndew theshining': 'mountain dew', 'squarespace winona winona ryder minnesota': 'squarespace  winona in winona', 'love takes action greek words love agape new york life take action lovetakesaction good life nyl newyorklife': 'new york life  love takes action', 'fox super monday office toby toby flenderson': 'fox  super monday', 'chris evans captain america john krasinki rachel dratch boston red sox david ortiz boston accent smart parking hyundai better way park bettahdriveus chrisevans smahtpark bettahwaytopark': 'hyundai  smaht pahk', 'mc hammer cant touch popcorn cheetos cheetos thing cheetle canttouchthismchammer': 'cheetos  can t touch this', 'allfemale spacewalk lilly singh busy phillips nicole stott taraji henson katie couric olay make space women makespaceforwomen olay': 'olay  make space for women', 'fox halftime show j lo jennifer lopez shakira': 'fox  halftime show  teaser_1', 'help farmers organic pack square footmake impact michelob gold michelobgold pack sixforsixpack': 'michelob  6 for 6-pack', 'molly ringwald avocados mexico avonetwork avocarriermollyringwald': 'avocados from mexico  the avocados from mexico shopping network', 'hard rock international hard rock stadium miami kobe bryant jennifer lopez jlo hardrock hardrockstadium hardrockcasino': 'hard rock hotels & casinos  bling cup', 'rick morty adult swim pringles pickle rick rick morty new flavor summer rickmorty adultswim': 'pringles  the infinite dimensions of rick and morty', 'turbotax people app mobile feature taxes tax people people tax people allpeoplearetaxpeople': 'turbotax  turbotax  all people are tax people remix', 'tide laundry laundry detergent charliey day bud knight stainlaundrydetergent charlieday': 'tide  bud knight', 'john legend chrissy teigen genesis hyundai kobe bryant young luxury gv sexiest man alive johnlegendchrissyteigen': 'genesis  going away party', 'coke iphone coca cola coca cola energy showup dotscococolacocacolaenergy jonah hill martin scorsese': 'coca-cola energy  show up', 'planters nutsmrpeanut matt walsh wesley snipesbabynut babypeanut babyfuneral baby nut': 'planters  baby funeral', 'james bond time die daniel craig': 'no time to die  trailer', 'fox masked singer toads': 'fox  toads  the masked singer', 'google assistant virtual assistant googleassistant virtualassistant': 'google assistant  loretta', 'sabra humus hummus wives real housewives new jersey real housewives caroline manzo teresa giudice tpain mix cracker kim chi drag queensdragqueens dragqueenamanda cerny howimmus': 'sabra  how do you  mmus?', 'weathertech pets golden retriever dog': 'weathertech  lucky dog', 'verizon phone service phone g': 'verizon  the amazing things 5g won t do', 'general motorshummer hummer ev electric pickup zero emission lebron james basketball lebronjames': 'hummer  gmc  quiet revolution', 'pop tarts queer eye jonathan van ness pretzel queereye poptarts': 'pop-tarts  pop-tarts fixed the pretzel commercial', 'minions rise gru': 'minions  the rise of gru  trailer', 'fox fox nation breaking news': 'fox nation  breaking news', 'premier boxing champions wilder vs fury ii': 'premier boxing champions  wilder vs. fury ii_1', 'homeland trailer': 'homeland  trailer', 'fox daytona great american race run history': 'fox  not just another race', 'hunters nazi al pacino nazi hunters prime video amazon barbeque barbecue alpacinoprimevideo hunterstv': 'amazon prime video  hunters', 'missy elliott pepsi zero sugar paint black pepsi pepsizero nocoke': 'pepsi zero sugar  zero sugar. done right.', 'find goodness heinz ketchup stranger things strangerthings findthegoodness': 'heinz ketchup  find the goodness  four at once', 'premier boxing champions wilder vs fury ii wrestling': 'premier boxing champions  wilder vs. fury ii_2', 'bud light bud light seltzer post malone anheuserbusch inbev hard seltzer postmalone budlightbudweiser': 'bud light seltzer  posty store  inside post s brain', 'office rainn wilson delivery best thing since sliced bread pizza pizza pizza littlecaesars caesars': 'little caesars pizza  best thing since sliced bread', 'sam elliott lil nas x old town road doritos cowboy cool ranch dancer billy ray cyrus wild west wild wild west make move': 'doritos  the cool ranch', 'josh jacobs running back kia kia seltosraiders give everything joshjacobs kiaseltos': 'kia  tough never quits', 'turkish airlines airline flight astronauts rocket': 'turkish airlines  step on earth', 'reese take five take candybarreeses': 'reese s  rock', 'tide stain later charlie day wonder woman': 'tide  ww', 'ellen degeneres alexa porta de rossi maid medieval dragon amazon alexa middle ages beforealexa': 'amazon echo  before alexa', 'anheuserbusch inbev michelob ultrabeer jimmy fallon working gym john cena usain bolt brooks koepka kerri walsh jennings worth enjoy low carbs jimmyfallon usainbolt workingout gymbody': 'michelob  jimmy works it out', 'inspire change police shootings black men nfl inspirechange': 'nfl  inspire change  anquan boldin', 'toyota car suv highlander toyotahighlandercobiesmulders cobie smulders': 'toyota  heroes', 'pop culture annual fee discover credit card chandler friends jack black mike myers austin powersnoannualfeediscovercredit yestodiscover': 'discover card  no we don’t charge annual fees', 'disney plus falcon wanda vision wanda loki': 'disney+  it s time', 'pop culture discover credit card mean girls ted noannualfeediscovercredit nodiscover': 'discover card  yes we’re accepted', 'football withdrawal system science doctor withdrawal symptoms': 'fox  football withdrawal syndrome', 'tmobile mama tests g g anthony anderson': 't-mobile  mama tests 5g', 'fox tornado lone star': 'fox  tornado  tomorrow', 'fox masked singer lego masters': 'fox  fun for all  the masked singer and lego', 'nfl building better game amazon web services': 'nfl  building a better game', 'kathryn bigelow budweiser american spirit typical american kindness beer stereotypes stereotypical typicalamerican americanspirit': 'budweiser  typical american', 'procter gamble come together sofia vergara chili bounty old spice head shoulders olay charming febreze': 'procter & gamble  when we come together', 'woman coach ers katie sowers surface female coach microsoft first female coach katiesowers erscoach microsoftsurface femalecoach': 'microsoft surface  be the one', 'fox lone star storm tornado': 'fox  i m scared', 'jeep groundhog day jeep gladiator bill murray': 'jeep  groundhog day [t1]', 'chris rock sylvester stallone groups facebook rock music stonehenge rock rocketchrisrock': 'facebook  ready to rock?', 'tide tide pod laundry detergent emily hampshire charlie day finally later': 'tide  finally later', 'maisie williams game thrones let go frozenaudi etron sportback traffic letitgo maisiewilliamsgameofthrones': 'audi  let it go [t1]', 'none': 'none'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3cN9RJSuy5_0"},"source":["ad_product_df.to_csv('./ad_product_df.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"adXsvteiWTSX"},"source":["import pandas as pd\n","\n","pd.set_option('display.max_rows', 100) \n","ad_count = df.groupby('ad_manual_adjusted')['ad_manual_adjusted'].count()\n","ad_count.to_csv('./ad_count.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lO5ZqNbxNw7O","colab":{"base_uri":"https://localhost:8080/","height":360},"executionInfo":{"status":"ok","timestamp":1600538359112,"user_tz":300,"elapsed":144008,"user":{"displayName":"Roopana Chenchu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhSh75QN1jzZ_cyXlnOrn2AdzeDgk7tzwAvqiW=s64","userId":"09352419684951286614"}},"outputId":"f80d95da-bcf2-43f1-ce70-3a80574a6cf8"},"source":["# use column name 'ad_manual_adjusted' of df to \n","def get_ad_related_twts(df, removeCommas = True):\n","  df['ad_manual_adjusted'] = df['ad_manual_adjusted'].apply(lambda x: x.lower())\n","  ad_filter = df['ad_manual_adjusted']!= 'none'\n","  ad_related_twts = df[ad_filter]\n","  if removeCommas:\n","    ad_filter_1 = ~ad_related_twts['ad_manual_adjusted'].str.contains(',')\n","    ad_related_twts = ad_related_twts[ad_filter_1]\n","  return ad_related_twts\n","\n","def getAdTweets(ad_related_twts, ad):\n","  return ad_related_twts[ad_related_twts.ad_manual_adjusted == ad].shape[0]\n","\n","def get_ad_id_dict(ad_related_twts): \n","  n_ad_related = ad_related_twts.shape[0]\n","  print(\"# ad related tweets: \"+ str(n_ad_related))\n","  ads_annotated = ad_related_twts.ad_manual_adjusted.values\n","  adset = set(ads_annotated)\n","  print(\"unique ads:\"+ str(len(adset)))\n","  ad_id_dict = {}\n","  i = 0\n","  for ad in adset : \n","    if(getAdTweets(ad_related_twts, ad) >=2):\n","      ad_id_dict[ad] = i\n","      i = i+1\n","    else:\n","      print('ad with <2 samples: '+ str(ad))\n","  print(\" No of ads with >=2 samples:\"+ str(len(ad_id_dict)))\n","  ad_id_dict['none'] = len(ad_id_dict)\n","  print(ad_id_dict)\n","  return ad_id_dict\n","\n","def convertAdNameToAdId(ad_id_dict, ad_name):\n","  if ad_name in ad_id_dict:\n","    return ad_id_dict[ad_name]\n","  else:\n","    return ad_id_dict['none']\n","\n","ad_related_twts = get_ad_related_twts(df)\n","ad_id_dict = get_ad_id_dict(get_ad_related_twts(df))\n","n_unique_ads = len(ad_id_dict) # ad_id_dict has none as well, so minus 1 when using embeddings\n","df['ad_manual_adjusted_id'] = df['ad_manual_adjusted'].apply(lambda x: convertAdNameToAdId(ad_id_dict,x))\n","print(n_unique_ads)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["# ad related tweets: 2597\n","unique ads:63\n","ad with <2 samples: tide  ww\n","ad with <2 samples: fast & furious 9  trailer\n","ad with <2 samples: discover card  no we don’t charge annual fees\n"," No of ads with >=2 samples:60\n","{'snickers  fix the world': 0, 'amazon prime video  hunters': 1, 'pepsi zero sugar  zero sugar. done right.': 2, 'fox  super monday': 3, 'procter & gamble  when we come together': 4, 'genesis  going away party': 5, 't-mobile  mama tests 5g': 6, 'pop-tarts  pop-tarts fixed the pretzel commercial': 7, 'porsche  the heist': 8, 'no time to die  trailer': 9, 'tide  bud knight': 10, 'planters  baby funeral': 11, 'fox  a run at history  daytona 500': 12, 'tide  finally later': 13, 'michelob  6 for 6-pack': 14, 'toyota  heroes': 15, 'jeep  groundhog day [t1]': 16, 'google assistant  loretta': 17, 'heinz ketchup  find the goodness  four at once': 18, 'verizon  the amazing things 5g won t do': 19, 'turbotax  turbotax  all people are tax people remix': 20, 'weathertech  lucky dog': 21, 'kia  tough never quits': 22, 'michelob  jimmy works it out': 23, 'new york life  love takes action': 24, 'fox  halftime show  teaser_1': 25, 'walmart  famous visitors': 26, 'hulu  tom brady s big announcement': 27, 'squarespace  winona in winona': 28, 'microsoft surface  be the one': 29, 'donald j. trump for president  criminal justice reform': 30, 'doritos  the cool ranch': 31, 'little caesars pizza  best thing since sliced bread': 32, 'nfl  inspire change  anquan boldin': 33, 'sabra  how do you  mmus?': 34, 'fox nation  breaking news': 35, 'avocados from mexico  the avocados from mexico shopping network': 36, 'disney+  it s time': 37, 'fox  great american race': 38, 'reese s  rock': 39, 'hard rock hotels & casinos  bling cup': 40, 'fox  toads  the masked singer': 41, 'nfl  building a better game': 42, 'budweiser  typical american': 43, 'discover card  yes we’re accepted': 44, 'mountain dew': 45, 'hyundai  smaht pahk': 46, 'amazon echo  before alexa': 47, 'hummer  gmc  quiet revolution': 48, 'cheetos  can t touch this': 49, 'bud light seltzer  posty store  inside post s brain': 50, 'audi  let it go [t1]': 51, 'quibi  bank heist': 52, 'olay  make space for women': 53, 'fox  chosen  lego masters': 54, 'coca-cola energy  show up': 55, 'pringles  the infinite dimensions of rick and morty': 56, 'marvel  black widow trailer': 57, 'facebook  ready to rock?': 58, 'rocket mortgage  home': 59, 'none': 60}\n","61\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  This is separate from the ipykernel package so we can avoid doing imports until\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"4zTyyggto6dO"},"source":["classification_ad_product = False # set to True if classification labels are ad products instead of ad names"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w2SHYJq3o4x7","colab":{"base_uri":"https://localhost:8080/","height":156},"executionInfo":{"status":"ok","timestamp":1600538359118,"user_tz":300,"elapsed":143999,"user":{"displayName":"Roopana Chenchu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhSh75QN1jzZ_cyXlnOrn2AdzeDgk7tzwAvqiW=s64","userId":"09352419684951286614"}},"outputId":"b4b3c2d2-178c-4a0d-efed-2eee63257042"},"source":["product_id_dict = dict()\n","products = df['product_modified'].unique()\n","i=0\n","\n","for product in products:\n","  product_id_dict[product] = i\n","  i = i+1\n","\n","n_unique_ad_produts = len(product_id_dict)\n","\n","if classification_ad_product:\n","  n_unique_ads = n_unique_ad_produts\n","\n","df['ad_product_id'] = df['product_modified'].apply(lambda x: product_id_dict[x])\n","\n","print(n_unique_ads)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["61\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"OMKdZdM69-H6","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600538359121,"user_tz":300,"elapsed":143995,"user":{"displayName":"Roopana Chenchu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhSh75QN1jzZ_cyXlnOrn2AdzeDgk7tzwAvqiW=s64","userId":"09352419684951286614"}},"outputId":"4bcf9c30-0cfa-46ca-990e-93f9e0604e68"},"source":["# train test split for multi class classification\n","from sklearn.model_selection import train_test_split\n","\n","n = df.shape[0]\n","if classification_ad_product: # if classification labels are ad_products alter the labels accordingly using this variable\n","  sentences, test_sentences, labels, test_labels = train_test_split(df.text_clean.values, \n","                  df.ad_product_id.values, \n","                  #  random_state = 2018, \n","                   test_size = 0.2, stratify = df.ad_product_id.values)\n","else:\n","  sentences, test_sentences, labels, test_labels = train_test_split(df.text_clean.values, \n","                  df.ad_manual_adjusted_id.values, \n","                  #  random_state = 2018, \n","                   test_size = 0.2, stratify = df.ad_manual_adjusted_id.values)\n","\n","train_size = len(sentences)\n","test_size = len(test_sentences)\n","print( \"Train size: \"+ str(train_size)+\" test size:\" + str(test_size))\n","                "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train size: 4659 test size:1165\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uCnlxyz6il50","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600538359126,"user_tz":300,"elapsed":143993,"user":{"displayName":"Roopana Chenchu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhSh75QN1jzZ_cyXlnOrn2AdzeDgk7tzwAvqiW=s64","userId":"09352419684951286614"}},"outputId":"511f4104-9efd-454b-a07d-1a9ffaa86abd"},"source":["print(len(test_labels))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1165\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_DIT_kuR6RCc","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600538359129,"user_tz":300,"elapsed":143988,"user":{"displayName":"Roopana Chenchu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhSh75QN1jzZ_cyXlnOrn2AdzeDgk7tzwAvqiW=s64","userId":"09352419684951286614"}},"outputId":"0c0a87e1-1ea4-4dcf-d934-cbc7084c0f88"},"source":["print(len(labels))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["4659\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ixlnFpq0c_hC"},"source":["def explode_data(sentences, labels, classification_ad_product):\n","  label_name_list = []\n","  label_id_list = []\n","  if classification_ad_product:\n","    label_name_list = list(product_id_dict.keys())\n","    label_id_list = list(product_id_dict.values())\n","  else:\n","    label_name_list = list(ad_id_dict.keys())\n","    label_id_list = list(ad_id_dict.values())\n","\n","  sentences_exploded =[]\n","  labels_exploded = []\n","  # sent_labels_df = pd.DataFrame()\n","  for i in range(len(sentences)):\n","    curr_sent = sentences[i]\n","    curr_label = labels[i]\n","    curr_label_name = label_name_list[curr_label]\n","\n","    if curr_label_name is not 'none' :\n","      curr_sent_exploded = []\n","      curr_label_exploded = []\n","      for j in range(len(label_name_list)): # expand for all products present\n","        if label_name_list[j] is not 'none':\n","          if classification_ad_product:\n","            label_keywords = ad_product_keywords_dict[label_name_list[j]]\n","          else:\n","            label_keywords = ad_name_keywords_dict[label_name_list[j]]\n","          curr_sent_exploded.append(curr_sent+\". \"+ label_keywords)\n","          \n","          if label_name_list[j] == curr_label_name :\n","            curr_label_exploded.append(1)\n","          else:\n","            curr_label_exploded.append(0)\n","      sentences_exploded.extend(curr_sent_exploded)\n","      labels_exploded.extend(curr_label_exploded)\n","    else:\n","      sentences_exploded.append(curr_sent)\n","      labels_exploded.append(2)\n","  return sentences_exploded, labels_exploded\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uJ5HxNqeJJ0v"},"source":["# explode to only 0,1 classes\n","def explode_data_to_two_classes(sentences, labels, classification_ad_product):\n","  label_name_list = []\n","  label_id_list = []\n","  if classification_ad_product:\n","    label_name_list = list(product_id_dict.keys())\n","    label_id_list = list(product_id_dict.values())\n","  else:\n","    label_name_list = list(ad_id_dict.keys())\n","    label_id_list = list(ad_id_dict.values())\n","\n","  sentences_exploded =[]\n","  labels_exploded = []\n","  # sent_labels_df = pd.DataFrame()\n","  for i in range(len(sentences)):\n","    curr_sent = sentences[i]\n","    curr_label = labels[i]\n","    curr_label_name = label_name_list[curr_label]\n","\n","    curr_sent_exploded = []\n","    curr_label_exploded = []\n","    for j in range(len(label_name_list)): # expand for all products present\n","      if label_name_list[j] is not 'none':\n","        if classification_ad_product:\n","          label_keywords = ad_product_keywords_dict[label_name_list[j]]\n","        else:\n","          label_keywords = ad_name_keywords_dict[label_name_list[j]]\n","        curr_sent_exploded.append(curr_sent+\". \"+ label_keywords)\n","          \n","        if label_name_list[j] == curr_label_name :\n","          curr_label_exploded.append(1)\n","        else:\n","          curr_label_exploded.append(0)\n","    sentences_exploded.extend(curr_sent_exploded)\n","    labels_exploded.extend(curr_label_exploded)\n","\n","  return sentences_exploded, labels_exploded\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l4SbjVaLpWVJ"},"source":["def explode_sentences_to_two_classes(sentences, classification_ad_product):\n","  label_name_list = []\n","  label_id_list = []\n","  if classification_ad_product:\n","    label_name_list = list(product_id_dict.keys())\n","    label_id_list = list(product_id_dict.values())\n","  else:\n","    label_name_list = list(ad_id_dict.keys())\n","    label_id_list = list(ad_id_dict.values())\n","\n","  sentences_exploded =[]\n","  for i in range(len(sentences)):\n","    curr_sent = sentences[i]\n","    curr_sent_exploded = []\n","    for j in range(len(label_name_list)): # expand for all products present\n","      if label_name_list[j] is not 'none':\n","        if classification_ad_product:\n","          label_keywords = ad_product_keywords_dict[label_name_list[j]]\n","        else:\n","          label_keywords = ad_name_keywords_dict[label_name_list[j]]\n","        curr_sent_exploded.append(curr_sent+\". \"+ label_keywords)\n","\n","    sentences_exploded.extend(curr_sent_exploded)\n","\n","  return sentences_exploded"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J5Gt-8ZGdGsQ","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1600538359138,"user_tz":300,"elapsed":143977,"user":{"displayName":"Roopana Chenchu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhSh75QN1jzZ_cyXlnOrn2AdzeDgk7tzwAvqiW=s64","userId":"09352419684951286614"}},"outputId":"7d9ba819-cb3b-4e93-f7b0-294335fdfb61"},"source":["if classType == 'sent_exploded':\n","  sentences_exploded, labels_exploded = explode_data_to_two_classes(sentences, labels, classification_ad_product)\n","  print(\"size of train data after exploding: \"+ str(len(sentences_exploded)))\n","  test_sentences_exploded, test_labels_exploded = explode_data_to_two_classes(test_sentences, test_labels, classification_ad_product)\n","  print(\"size of test data after exploding: \"+ str(len(test_sentences_exploded)))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["size of train data after exploding: 279540\n","size of test data after exploding: 69900\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5ZqH_34eqL0l","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600540782279,"user_tz":300,"elapsed":2419,"user":{"displayName":"Roopana Chenchu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhSh75QN1jzZ_cyXlnOrn2AdzeDgk7tzwAvqiW=s64","userId":"09352419684951286614"}},"outputId":"f1450bda-25d3-4acc-dce5-0b77588d7f1c"},"source":["  rem_sentences_exploded = explode_sentences_to_two_classes(remaining_tweets.text_clean.values, classification_ad_product)\n","  print(len(rem_sentences_exploded))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["3125100\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-O-HDfPm0yUz","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600538359140,"user_tz":300,"elapsed":143973,"user":{"displayName":"Roopana Chenchu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhSh75QN1jzZ_cyXlnOrn2AdzeDgk7tzwAvqiW=s64","userId":"09352419684951286614"}},"outputId":"873df77e-3553-4217-dadc-b7a928e68611"},"source":["# train test split for binary classification\n","import numpy as np\n","binary_sentences, binary_test_sentences, binary_labels, binary_test_labels = train_test_split(df.text_clean.values, df.ad_related.values, \n","                  #  random_state = 2018, \n","                   test_size=0.2)\n","def get_bin_from_multi_class(labels):\n","  binary_labels = []\n","  for ad_id in labels:\n","    if ad_id == ad_id_dict['none']:\n","      binary_labels.append(0)\n","    else:\n","      binary_labels.append(1)\n","  return binary_labels\n","\n","# binary_sentences = sentences\n","# binary_test_sentences = test_sentences\n","# print(len(ad_id_dict))\n","# print(labels)\n","# binary_labels = get_bin_from_multi_class(labels)\n","# binary_test_labels = get_bin_from_multi_class(test_labels)\n","\n","binary_train_size = len(binary_sentences)\n","binary_test_size = len(binary_test_sentences)\n","print(\"Total data set size: \"+ str(df.shape[0])+\", train size: \"+ str(binary_train_size)+\", test size: \" + str(binary_test_size))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total data set size: 5824, train size: 4659, test size: 1165\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XYEXu9YH02QF"},"source":["Transform our dataset into the format that BERT can be trained on"]},{"cell_type":"code","metadata":{"id":"qHQ4bb-SiFyt","colab":{"base_uri":"https://localhost:8080/","height":100,"referenced_widgets":["42e193cdace64fdda6aabf8963fb1726","311f7a48476d400f9c68b18cf4d435bf","abb1a1b2bc534062825afa93e9cc49c9","fd0471795cb54424896c659d8d8db399","57ac71dcc2c943679389f01b2a84b13d","9fd03fe258c44078b8554c09f2729034","b12b288523324a52835e06f69ed03e43","da3f865dc37040728430a19fb122dafa"]},"executionInfo":{"status":"ok","timestamp":1600538359143,"user_tz":300,"elapsed":143968,"user":{"displayName":"Roopana Chenchu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhSh75QN1jzZ_cyXlnOrn2AdzeDgk7tzwAvqiW=s64","userId":"09352419684951286614"}},"outputId":"81b6bdbb-0a09-4812-ca05-347d35850b87"},"source":["from transformers import BertTokenizer\n","# Load the BERT tokenizer.\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","print('Loaded BERT tokenizer.')\n","vocab_tokens = list(tokenizer.vocab.keys())\n","print(\"Original Vocab size: \" + str(len(vocab_tokens)))\n"," "],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"42e193cdace64fdda6aabf8963fb1726","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Loaded BERT tokenizer.\n","Original Vocab size: 30522\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VBEqDuxy5L9M","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1600538359155,"user_tz":300,"elapsed":143974,"user":{"displayName":"Roopana Chenchu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhSh75QN1jzZ_cyXlnOrn2AdzeDgk7tzwAvqiW=s64","userId":"09352419684951286614"}},"outputId":"54971d4b-c4fd-4b6f-e2ac-9116f471648a"},"source":["import nltk\n","nltk.download('wordnet')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"yTTLNCzx-_SZ","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600538361491,"user_tz":300,"elapsed":146304,"user":{"displayName":"Roopana Chenchu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhSh75QN1jzZ_cyXlnOrn2AdzeDgk7tzwAvqiW=s64","userId":"09352419684951286614"}},"outputId":"3f62d694-bcac-4037-d74f-1d6c51065f68"},"source":["from nltk.stem import WordNetLemmatizer\n","wordnet_lemmatizer = WordNetLemmatizer()\n","\n","def get_lemmatize_words(plural_words):\n","  sing_words = []\n","  for word in plural_words:\n","    sing_words.append(wordnet_lemmatizer.lemmatize(word))\n","  return sing_words\n","\n","words = [\"apples\", \"sheep\", \"oranges\", \"cats\", \"people\", \"dice\", \"pence\", \"trump\"]\n","lem_words = get_lemmatize_words(words)\n","print(lem_words)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['apple', 'sheep', 'orange', 'cat', 'people', 'dice', 'penny', 'trump']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FPfE2myLCz_I"},"source":["# Challenge: There are some words that are not separated although can be two legit words if separted\n","# lightyear, jasonmamoa, trumpsupporters, condimentfood, fixtheworld, budlightbudweiser, microsoftsurface, femalecoach, typicalamerican\n","# poptarts, quickbites, googleassistant, smahtpark, spacewalk, letitgo, bettahwaytopark, trumpforpresident, hardrockstadium,\n","# alpacinoprimevideo, femalecoach, inspirechange, motorshummer, newyorklife, stainlaundrydetergent\n","\n","def get_missing_words_vocab(tokenizer, words):\n","  missing_words = []\n","  vocab = tokenizer.get_vocab()\n","  print(\"Actual word list of ad tokens:\"+ str(len(words)))\n","  for word in words:\n","    if word not in vocab:\n","      missing_words.append(word)\n","  print(str(len(missing_words))+\" missing_words\")\n","  return missing_words\n","\n","# missing_words = get_missing_words_vocab(tokenizer, keys_to_add)\n","# print(missing_words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XsD-oEL0iNut","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1600541013830,"user_tz":300,"elapsed":177771,"user":{"displayName":"Roopana Chenchu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhSh75QN1jzZ_cyXlnOrn2AdzeDgk7tzwAvqiW=s64","userId":"09352419684951286614"}},"outputId":"5eed49da-38d7-49c6-d3ec-64e49f2a288c"},"source":["# Tokenize all of the sentences and map the tokens to their word IDs.\n","def get_input_ids(sentences):\n","  input_ids = []\n","  # For every sentence...\n","  for sent in sentences:\n","      # `encode` will:\n","      #   (1) Tokenize the sentence.\n","      #   (2) Prepend the `[CLS]` token to the start.\n","      #   (3) Append the `[SEP]` token to the end.\n","      #   (4) Map tokens to their IDs.\n","      encoded_sent = tokenizer.encode(\n","                          sent,                      # Sentence to encode.\n","                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                          # This function also supports truncation and conversion\n","                          # to pytorch tensors, but we need to do padding, so we\n","                          # can't use these features :( .\n","                          # max_length = 128,          # Truncate all sentences.\n","                          # return_tensors = 'pt',     # Return pytorch tensors.\n","                    )\n","\n","      input_ids.append(encoded_sent)\n","  return input_ids\n","\n","if classType == 'binary':\n","  print('binary')\n","  input_ids = get_input_ids(binary_sentences)\n","elif classType == 'sent_exploded':\n","  print('sent_exploded')\n","  input_ids = get_input_ids(sentences_exploded)\n","else:\n","  print('multi class')\n","  input_ids = get_input_ids(sentences)\n","\n","print(len(input_ids))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["sent_exploded\n","279540\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QEfkgBVv9JXH"},"source":["# binary_trained_embeddings = model.get_input_embeddings()\n","# print(binary_trained_embeddings)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bss0IhfHk2aD","colab":{"base_uri":"https://localhost:8080/","height":220,"referenced_widgets":["0174e78ae92747f1b2500b704c283711","06f30295d2c5464e93b517eb3ea85be9","208b39486536463f828bd67be257e41e","cb952d83621e47a794b9a5d44eccd348","f51bc37b30dd4d25a7ccffc9c15d375a","181652e059104d15b707aec9c3109b46","f0f3e84761a54955b5be270372fae331","bac73a31485743fc9c74dcacca95369e","d0f553c2176d4f979dcc91cef8acd1a3","6346ef7b71e045c991caa3a92b90d152","c86f72d0ef3f46828636a4a3a0c2a106","a670018c96954bd296b57b9a14202e00","d2a947a6349744d9bf22f662ae757f35","5398dbeb40004f41ba0936d069f58fe7","776fe15c66b44b69ae123e72f3c4e1c7","f1ee41c4955d4d10b3e516364a5502f5"]},"executionInfo":{"status":"ok","timestamp":1600538575161,"user_tz":300,"elapsed":359931,"user":{"displayName":"Roopana Chenchu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhSh75QN1jzZ_cyXlnOrn2AdzeDgk7tzwAvqiW=s64","userId":"09352419684951286614"}},"outputId":"f5da3b0b-4037-496e-bcc6-57ce8250575b"},"source":["from transformers import BertForSequenceClassification, AdamW, BertConfig, BertModel\n","# Load BertForSequenceClassification, the pretrained BERT model with a single \n","# linear classification layer on top. \n","if classType == 'binary':\n","  num_labels = 2\n","elif classType == 'sent_exploded':\n","  num_labels = 2 # 0 - not same as the appended add, 1- same as the appended add,  (2 - none)\n","else:\n","  print(n_unique_ads)\n","  num_labels = n_unique_ads\n","\n","model = BertForSequenceClassification.from_pretrained(\n","    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = num_labels,\n","    # num_labels = n_unique_ads, # The number of output labels=2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = True, # Whether the model returns all hidden-states.\n",")\n","# model.resize_token_embeddings(len(tokenizer)) # since new tokens are added\n","# Tell pytorch to run this model on the GPU.\n","if isGPUavailable:\n","  model.cuda()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0174e78ae92747f1b2500b704c283711","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d0f553c2176d4f979dcc91cef8acd1a3","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"1HA7XuUz9dTU"},"source":["# model.set_input_embeddings(binary_trained_embeddings)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5jxskjmNjv1x","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600538575176,"user_tz":300,"elapsed":359928,"user":{"displayName":"Roopana Chenchu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhSh75QN1jzZ_cyXlnOrn2AdzeDgk7tzwAvqiW=s64","userId":"09352419684951286614"}},"outputId":"d1961669-b4fc-4716-dcda-e049ab7e863b"},"source":["print('Max sentence length: ', max([len(sen) for sen in input_ids]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Max sentence length:  88\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vOjgb_tSj8Bs","colab":{"base_uri":"https://localhost:8080/","height":136},"executionInfo":{"status":"ok","timestamp":1600538577041,"user_tz":300,"elapsed":361785,"user":{"displayName":"Roopana Chenchu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhSh75QN1jzZ_cyXlnOrn2AdzeDgk7tzwAvqiW=s64","userId":"09352419684951286614"}},"outputId":"96e3692c-da62-4562-ec94-5f06f1386e79"},"source":["#let’s choose MAX_LEN = 64 and apply the padding\n","# We'll borrow the `pad_sequences` utility function to do this.\n","from keras.preprocessing.sequence import pad_sequences\n","# Set the maximum sequence length.\n","# I've chosen MAXLEN somewhat arbitrarily. It's slightly larger than the max training sentence length\n","if classType == 'sent_exploded' and classification_ad_product:\n","  MAX_LEN = 300\n","elif classType == 'sent_exploded' and ~classification_ad_product:\n","  MAX_LEN = 100\n","else:\n","  MAX_LEN = 40\n","print(\"MAX_LEN:\"+ str(MAX_LEN))\n","print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n","print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n","# Pad our input tokens with value 0.\n","# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n","# as opposed to the beginning.\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n","                          value=0, truncating=\"post\", padding=\"post\")\n","print('\\nDone.')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["MAX_LEN:100\n","\n","Padding/truncating all sentences to 100 values...\n","\n","Padding token: \"[PAD]\", ID: 0\n","\n","Done.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YSCcU3htTL-T"},"source":["The attention mask simply makes it explicit which tokens are actual words versus which are padding.\n","The BERT vocabulary does not use the ID 0, so if a token ID is 0, then it’s padding, and otherwise it’s a real token."]},{"cell_type":"code","metadata":{"id":"QikSI06kkIm2"},"source":["# Create attention masks\n","attention_masks = []\n","# For each sentence...\n","for sent in input_ids:\n","    \n","    # Create the attention mask.\n","    #   - If a token ID is 0, then it's padding, set the mask to 0.\n","    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n","    att_mask = [int(token_id > 0) for token_id in sent]\n","    \n","    # Store the attention mask for this sentence.\n","    attention_masks.append(att_mask)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Piy1b52SkUsa"},"source":["#training and validation split\n","\n","from sklearn.model_selection import train_test_split\n","# Use 90% for training and 10% for validation.\n","\n","if classType == 'binary':\n","  input_labels = binary_labels\n","elif classType == 'sent_exploded':\n","  input_labels = labels_exploded\n","else:\n","  input_labels = labels\n","\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, input_labels, \n","                                                            # random_state=2018, \n","                                                            test_size=0.1)\n","# Do the same for the masks.\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_labels,\n","                                            #  random_state=2018, \n","                                             test_size=0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TWtZ_hlmkcpJ"},"source":["#Converting to PyTorch Data Types\n","# Convert all inputs and labels into torch tensors, the required datatype for our model.\n","train_inputs = torch.tensor(train_inputs)\n","validation_inputs = torch.tensor(validation_inputs)\n","train_labels = torch.tensor(train_labels)\n","validation_labels = torch.tensor(validation_labels)\n","train_masks = torch.tensor(train_masks)\n","validation_masks = torch.tensor(validation_masks)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lRztCvATkrUJ"},"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","# The DataLoader needs to know our batch size for training, so we specify it here.\n","# For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32.\n","batch_size = 32\n","# Create the DataLoader for our training set.\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","# Create the DataLoader for our validation set.\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qD8UOxdDlXtm"},"source":["let’s load BERT! There are a few different pre-trained BERT models available. “bert-base-uncased” means the version that has only lowercase letters (“uncased”) and is the smaller version of the two (“base” vs “large”)\n","\n"]},{"cell_type":"code","metadata":{"id":"5zGV1y8LlE_d","colab":{"base_uri":"https://localhost:8080/","height":612},"executionInfo":{"status":"ok","timestamp":1600538601384,"user_tz":300,"elapsed":386099,"user":{"displayName":"Roopana Chenchu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhSh75QN1jzZ_cyXlnOrn2AdzeDgk7tzwAvqiW=s64","userId":"09352419684951286614"}},"outputId":"1706b129-a1af-4287-8efd-ba1772dfa29b"},"source":["# Get all of the model's parameters as a list of tuples.\n","params = list(model.named_parameters())\n","print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n","print('==== Embedding Layer ====\\n')\n","for p in params[0:5]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","print('\\n==== First Transformer ====\\n')\n","for p in params[5:21]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","print('\\n==== Output Layer ====\\n')\n","for p in params[-4:]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The BERT model has 201 different named parameters.\n","\n","==== Embedding Layer ====\n","\n","bert.embeddings.word_embeddings.weight                  (30522, 768)\n","bert.embeddings.position_embeddings.weight                (512, 768)\n","bert.embeddings.token_type_embeddings.weight                (2, 768)\n","bert.embeddings.LayerNorm.weight                              (768,)\n","bert.embeddings.LayerNorm.bias                                (768,)\n","\n","==== First Transformer ====\n","\n","bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n","bert.encoder.layer.0.attention.self.query.bias                (768,)\n","bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n","bert.encoder.layer.0.attention.self.key.bias                  (768,)\n","bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n","bert.encoder.layer.0.attention.self.value.bias                (768,)\n","bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n","bert.encoder.layer.0.attention.output.dense.bias              (768,)\n","bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n","bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n","bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n","bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n","bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n","bert.encoder.layer.0.output.dense.bias                        (768,)\n","bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n","bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n","\n","==== Output Layer ====\n","\n","bert.pooler.dense.weight                                  (768, 768)\n","bert.pooler.dense.bias                                        (768,)\n","classifier.weight                                           (2, 768)\n","classifier.bias                                                 (2,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tlzNhXail07W"},"source":["Optimizer & Learning Rate Scheduler:\n","Now that we have our model loaded we need to grab the training hyperparameters from within the stored model.\n","For the purposes of fine-tuning, the authors recommend choosing from the following values:\n","Batch size: 16, 32 (We chose 32 when creating our DataLoaders).\n","Learning rate (Adam): 5e-5, 3e-5, 2e-5 (We’ll use 2e-5).\n","Number of epochs: 2, 3, 4 (We’ll use 4).\n","The epsilon parameter eps = 1e-8 is “a very small number to prevent any division by zero in the implementation”\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"PXOyFLmCliw7"},"source":["# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n","# I believe the 'W' stands for 'Weight Decay fix\"\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )\n","from transformers import get_linear_schedule_with_warmup\n","# Number of training epochs (authors recommend between 2 and 4)\n","epochs = 4\n","if classType == 'sent_exploded' and ~classification_ad_product:\n","  epochs = 2 # reducing the no. of epochs since train time is 30 min per epoch giving a train accuracy of 99 % and loss of 0.05 in each epoch\n","# Total number of training steps is number of batches * number of epochs.\n","total_steps = len(train_dataloader) * epochs\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xe1VgG4Tq85o"},"source":["Define a helper function to find accuracy"]},{"cell_type":"code","metadata":{"id":"i0RZmdmxluIa"},"source":["import numpy as np\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OETTpqlhrCr7"},"source":["Helper function for formatting elapsed times.\n"]},{"cell_type":"code","metadata":{"id":"hOLafL31rGJV"},"source":["import time\n","import datetime\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bGMkZxZVrdOt"},"source":["Kick off training"]},{"cell_type":"code","metadata":{"id":"LrMEahWGrJxW"},"source":["dont run this\n","\n","import random\n","import numpy as np\n","import tensorflow as tf\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 42\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","# Store the average loss after each epoch so we can plot them.\n","loss_values = []\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","    # Reset the total loss for this epoch.\n","    total_loss = 0\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model.train()\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        model.zero_grad()        \n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # This will return the loss (rather than the model output) because we\n","        # have provided the `labels`.\n","        # The documentation for this `model` function is here: \n","        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","        outputs = model(b_input_ids, \n","                    token_type_ids=None, \n","                    attention_mask = b_input_mask\n","                     ,labels=b_labels)\n","                  # )        \n","        # The call to `model` always returns a tuple, so we need to pull the loss value out of the tuple.\n","        loss = outputs[0]\n","\n","        # # Accumulate the training loss over all of the batches so that we can\n","        # # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # # single value; the `.item()` function just returns the Python value \n","        # # from the tensor.\n","        total_loss += loss.item()\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","        # Clip the norm of the gradients to 1.0. This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","        # Update the learning rate.\n","        scheduler.step()\n","    # Calculate the average loss over the training data.\n","    avg_train_loss = total_loss / len(train_dataloader)            \n","    \n","    # Store the loss value for plotting the learning curve.\n","    loss_values.append(avg_train_loss)\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","    print(\"\")\n","    print(\"Running Validation...\")\n","    t0 = time.time()\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","    # Tracking variables \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","    # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","        \n","        # Add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # Unpack the inputs from our dataloader\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        # Telling the model not to compute or store gradients, saving memory and\n","        # speeding up validation\n","        with torch.no_grad():        \n","            # Forward pass, calculate logit predictions.\n","            # This will return the logits rather than the loss because we have\n","            # not provided labels.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            # The documentation for this `model` function is here: \n","            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","            outputs = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask)\n","        \n","        # Get the \"logits\" output by the model. The \"logits\" are the output\n","        # values prior to applying an activation function like the softmax.\n","        logits = outputs[0]\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        # Calculate the accuracy for this batch of test sentences.\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        \n","        # Accumulate the total accuracy.\n","        eval_accuracy += tmp_eval_accuracy\n","        # Track the number of batches\n","        nb_eval_steps += 1\n","    # Report the final accuracy for this validation run.\n","    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n","print(\"\")\n","print(\"Training complete!\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ct1fywhS73v8"},"source":["print(' No of labels:'+ str(len(label_ids)))\n","print(len(label_ids.flatten()))\n","print(label_ids)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BgARwwtw713E"},"source":["import pickle\n","\n","def save_model(model, tokenizer):\n","  trained_model = './model.pickle'\n","  tokenizer_model = './tokenizer.pickle'\n","  pickle.dump(model, open(trained_model, 'wb'))\n","  pickle.dump(tokenizer, open(tokenizer_model, 'wb') )\n","\n","save_model(model, tokenizer)\n","print('model saved')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z604bwCrrDNh","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600541016320,"user_tz":300,"elapsed":1409,"user":{"displayName":"Roopana Chenchu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhSh75QN1jzZ_cyXlnOrn2AdzeDgk7tzwAvqiW=s64","userId":"09352419684951286614"}},"outputId":"392d29b9-12d1-42e5-b9a0-73db5564bf25"},"source":["import pickle\n","\n","model = pickle.load(open('./model_smax_0_1.pickle', 'rb'))\n","print('model loaded')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["model loaded\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9ICZ9fVFrUw7"},"source":["#visualize training loss\n","# import plotly.express as px\n","# f = pd.DataFrame(loss_values)\n","# f.columns=['Loss']\n","# fig = px.line(f, x=f.index, y=f.Loss)\n","# fig.update_layout(title='Training loss of the Model',\n","#                    xaxis_title='Epoch',\n","#                    yaxis_title='Loss')\n","# fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Baq1OzNrjSTv"},"source":["Check for model's behaviour on ads that are not tarined for\n","\n","i.e check if the model is able to classify an ad as ad related although these ads were not present in the training data"]},{"cell_type":"code","metadata":{"id":"1hc8vijNhI7k"},"source":["# binary_test_sentences = removed_Data.tweet_text.values\n","# removed_Data['ad_related'] = removed_Data['ad_manual_adjusted'].apply(lambda ad: 0 if ad == 'none' else 1)\n","# binary_test_labels = removed_Data.ad_related.values"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iLC4BThggAk7"},"source":["Run the model on tweets that are not manually annotated\n","Use variable 'labelsPresent' to toggle between test data predictions and non manually annotated data predictions"]},{"cell_type":"code","metadata":{"id":"cFZYdFqBf_Eg","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1600541590621,"user_tz":300,"elapsed":865,"user":{"displayName":"Roopana Chenchu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhSh75QN1jzZ_cyXlnOrn2AdzeDgk7tzwAvqiW=s64","userId":"09352419684951286614"}},"outputId":"36199ffd-c6fe-44cb-ff3e-ad9a3e590009"},"source":["#Uncomment this snippet when running multi class on binary classifier's output\n","# if classType == 'binary':\n","#   binary_test_sentences = input.tweet_text.values\n","# else:\n","#   binary_output =  pd.read_csv('./app_1_bin_class_no_man_ann.csv')\n","#   binary_output = binary_output[binary_output['ad_related']==1]\n","#   test_sentences = binary_output.tweet_text.values\n","\n","# labels_present = False # for non manually annotated data\n","# print('labels not present')\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["32\n","labels not present\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8iAuFnenfmKx"},"source":["labels_present = True\n","\n","print('labels present')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UhBwyp9su8oE"},"source":["import pandas as pd\n","\n","# Get the lists of sentences and their labels.\n","\n","if classType == 'binary':\n","  sentences = binary_test_sentences\n","  labels = binary_test_labels\n","elif classType == 'sent_exploded':\n","  sentences = test_sentences_exploded\n","  labels = test_labels_exploded\n","else:\n","  print('multi class')\n","  sentences = test_sentences\n","  labels = test_labels\n","\n","# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","# For every sentence...\n","for sent in sentences:\n","    # `encode` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    encoded_sent = tokenizer.encode(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                   )\n","    input_ids.append(encoded_sent)\n","# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n","                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n","# Create attention masks\n","attention_masks = []\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask) \n","# Convert to tensors.\n","prediction_inputs = torch.tensor(input_ids)\n","prediction_masks = torch.tensor(attention_masks)\n","if labels_present:\n","  prediction_labels = torch.tensor(labels)\n","# Set the batch size.  \n","batch_size = 32  \n","# Create the DataLoader.\n","if labels_present:\n","  prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n","else:\n","  prediction_data = TensorDataset(prediction_inputs, prediction_masks)\n","\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"76v-kdGl2Tfk"},"source":["# model_archive = model\n","# model = loaded_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rEFkmEIkvOBg"},"source":["# Prediction on test set\n","print('Predicting labels for {:,} test sentences.'.format(len(prediction_inputs)))\n","# Put model in evaluation mode\n","model.eval()\n","# Tracking variables \n","predictions , true_labels = [], []\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  # Unpack the inputs from our dataloader\n","  if labels_present:\n","    b_input_ids, b_input_mask, b_labels = batch\n","  else:\n","    b_input_ids, b_input_mask = batch\n","  \n","  # Telling the model not to compute or store gradients, saving memory and \n","  # speeding up prediction\n","  with torch.no_grad():\n","      # Forward pass, calculate logit predictions\n","      outputs = model(b_input_ids, token_type_ids=None, \n","                      attention_mask = b_input_mask)\n","  logits = outputs[0]\n","  # Move logits and labels to CPU\n","  predictions.append(torch.nn.functional.softmax(logits, dim = 1).detach().cpu().numpy())\n","  \n","  #logits = logits.detach().cpu().numpy()\n","  # Store predictions and true labels\n","  # predictions.append(logits)\n","  \n","  if labels_present:\n","    label_ids = b_labels.to('cpu').numpy()\n","    true_labels.append(label_ids)\n","    \n","print('DONE.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"60BKaYQBva09"},"source":["def flatten_labels(true_labels):\n","  flat_labels =[]\n","  for i in range(len(true_labels)):\n","    flat_labels.extend(true_labels[i].flatten())\n","  return flat_labels\n","\n","def calculate_pred_labels(predictions):\n","  tp_count =0\n","  pred_labels = []\n","  pred_scores = []\n","  # For each input batch...\n","  print(\" len of predictions: \" + str(len(predictions)))\n","  for i in range(len(predictions)):\n","    # The predictions for this batch are a 2-column ndarray (one column for \"0\" and one column for \"1\"). \n","    # Pick the label with the highest value and turn this in to a list of 0s and 1s.\n","    #print(predictions[i])\n","    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n","    pred_scores_i = np.max(predictions[i], axis=1).flatten()\n","    pred_labels.extend(pred_labels_i)\n","    pred_scores.extend(pred_scores_i)\n","  return pred_labels, pred_scores\n","\n","print(len(true_labels))\n","print(len(predictions))\n","\n","pred_labels, pred_scores = calculate_pred_labels(predictions)\n","print(len(pred_labels))\n","\n","if labels_present:\n","  true_labels_flat = flatten_labels(true_labels)\n","  print(len(true_labels_flat))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DRNSZzBCj4Bw"},"source":["# input = pd.read_csv('./remaining_tweets_non_man.csv')\n","if labels_present:\n","  print('labels are present, do nothing')\n","else:\n","  if classType == 'binary':\n","    input['ad_related'] = pred_labels\n","    input.to_csv('./app_1_bin_class_no_man_ann.csv')\n","  elif classType == 'multi-class':\n","    binary_output['ad_id_predicted'] = pred_labels\n","    binary_output['ad_predicted'] = binary_output['ad_id_predicted'].apply(lambda id: list(ad_id_dict.keys())[id])\n","    binary_output.to_csv('./app_1_multi_class_no_man_ann.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qgXwPoAFBdzL"},"source":["from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, classification_report, confusion_matrix\n","\n","def getMetrics(true_labels_flat, pred_labels, averageType):\n","  print(\"Evaluating metrics as per '\"+averageType+\"' average type\")\n","  f1score = f1_score(true_labels_flat, pred_labels, average= averageType) \n","  print('f1_score:'+ str(f1score))\n","\n","  prec = precision_score(true_labels_flat, pred_labels, average=averageType)\n","  print('Precision:'+ str(prec))\n","\n","  acc = accuracy_score(true_labels_flat, pred_labels)\n","  print(\"Accuracy: \"+ str(acc))\n","\n","  recall = recall_score(true_labels_flat, pred_labels, average=averageType)\n","  print(\"recall: \"+ str(recall))\n","\n","  # classification_report(true_labels_flat, pred_labels)\n","  confusionmatrix = confusion_matrix(true_labels_flat, pred_labels)\n","  print(confusionmatrix)\n","  if averageType == 'binary':\n","    tn, fp, fn, tp = confusionmatrix.ravel()\n","    print(\"tp: \"+ str(tp)+\" tn: \"+ str(tn)+\" fp: \"+ str(fp)+\" fn: \"+ str(fn))\n","\n","if classType == 'binary':\n","  averageType = 'binary'\n","elif classType == 'sent_exploded':\n","  averageType = 'weighted' # no weighting for imbalance\n","else:\n","  averageType = 'weighted'\n","  # micro: Calculate metrics globally by counting the total true positives, false negatives and false positives.\n","  # macro: Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n","  # weighted : Calculate metrics for each label, and find their average weighted by support\n","  #  (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; \n","  # it can result in an F-score that is not between precision and recall.\n","\n","getMetrics(true_labels_flat, pred_labels, averageType)\n","\n","# //Accuracy of multi class by taking inputs of bin\n","# Evaluating metrics as per 'weighted' average type\n","# f1_score:0.6933475888363791\n","# Precision:0.6771610659170494\n","# Accuracy: 0.7354694485842027\n","# recall: 0.7354694485842027"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_aATLekRM8Aw"},"source":["sent_exploded_results_df = pd.DataFrame()\n","sent_exploded_results_df['sentence_raw'] = test_sentences_exploded\n","sent_exploded_results_df['label_pred'] = pred_labels\n","if labels_present:\n","  sent_exploded_results_df['label_true'] = true_labels_flat\n","sent_exploded_results_df['pred_scores'] = pred_scores\n","sent_exploded_results_df[['sentence','keywords_appended']] = sent_exploded_results_df.sentence_raw.str.split(\".\",expand=True) \n","print(sent_exploded_results_df.head())\n","\n","sent_exploded_results_df['keywords_appended_name'] = sent_exploded_results_df['keywords_appended'].apply(lambda x: 'none' if x is None else ad_keywords_name_dict[x.strip()])\n","\n","sent_exploded_results_df.to_csv(\"./sent_exploded_results_df.csv\")\n","print(sent_exploded_results_df.head())\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BpzwiZCsUdXR"},"source":["def get_pred_label_name_score(df):\n","  sentence = df['sentence'].iloc[0:1] # since grouped by sentence there will be only one unique sentence\n","  max_pred_score = 0\n","  pred_label = 0\n","  pred_label_name = \"none\"\n","\n","  true_label = 0\n","  true_label_name = \"none\"\n","\n","  for i, row in df.iterrows():\n","    if row['label_pred'] == 1 and row['pred_scores']>max_pred_score:\n","      max_pred_score = row['pred_scores']\n","      pred_label = 1\n","      pred_label_name = row['keywords_appended_name']\n","    # if all 0  => not related to any ad => none\n","    # if all 2 => none\n","\n","    # check for the true label as well \n","    if row['label_true'] == 1 :\n","        true_label = 1\n","        true_label_name = row['keywords_appended_name']\n","    elif row['label_true'] == 2 :\n","        true_label = 2\n","        true_label_name = row['keywords_appended_name']      \n","\n","  result = pd.DataFrame()\n","  result['sentence'] = sentence\n","  result['pred_label_name'] = pred_label_name\n","  result['max_pred_score'] = max_pred_score\n","  result['true_label'] = true_label\n","  result['true_label_name'] = true_label_name\n","  return result\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gFkiZxxwZ8Sd"},"source":["def get_pred_2_label_name_score(df):\n","  sentence = df['sentence'].iloc[0:1] # since grouped by sentence there will be only one unique sentence\n","  max_pred_score = 0\n","  pred_label = 0\n","  pred_label_name = \"none\"\n","\n","  true_label = 0\n","  true_label_name = \"none\"\n","\n","  for i, row in df.iterrows():\n","    if row['label_pred'] == 1 and row['pred_scores']>max_pred_score:\n","      max_pred_score = row['pred_scores']\n","      pred_label = 1\n","      pred_label_name = row['keywords_appended_name']\n","    # if all 0  => not related to any ad => none\n","    # if all 2 => none\n","\n","    # check for the true label as well \n","    if row['label_true'] == 1 :\n","        true_label = 1\n","        true_label_name = row['keywords_appended_name']\n","   # default 0 which will be none     \n","\n","  result = pd.DataFrame()\n","  result['sentence'] = sentence\n","  result['pred_label_name'] = pred_label_name\n","  result['max_pred_score'] = max_pred_score\n","  result['true_label'] = true_label\n","  result['true_label_name'] = true_label_name\n","  return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"86LSKXIUmjst"},"source":["sentence_results_df = pd.DataFrame()\n","sentence_results_df[['sentence_1', 'pred_label_name', 'max_pred_score', 'true_label', 'true_label_name']] = sent_exploded_results_df.groupby('sentence').apply(get_pred_2_label_name_score)\n","\n","print(sentence_results_df.shape)\n","print(sentence_results_df.head())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_brlXaN_k6FJ"},"source":["# groups = sent_exploded_results_df.groupby('sentence')\n","# result = pd.DataFrame()\n","# for group in groups:\n","#   # print(group[1])\n","#   result = result.append(get_pred_label_name_score(group[1]))\n","\n","# print(result.head())\n","# print(result.shape)\n","# result.to_csv('./sent_exp_grp_result_no_dupe.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YaqxxN2Xcp42"},"source":["sentence_results_df.to_csv('./sent_exp_group_results.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yAzIOvOR3fhH"},"source":["# import pandas as pd\n","# sentence_results_df = pd.read_csv('./sent_exp_group_results_smax_0_1.csv')\n","# sentence_results_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ScjlemUYo-IB"},"source":["# use for sent_exploded classifier\n","from sklearn.metrics import classification_report\n","true_names = sentence_results_df['true_label_name']\n","pred_names = sentence_results_df['pred_label_name']\n","classification_report = classification_report(true_names, pred_names,output_dict=True)\n","\n","classification_report_df = pd.DataFrame(classification_report).transpose()\n","print(classification_report_df.head())\n","if classType == 'sent_exploded':\n","  classification_report_df.to_csv('./classification_report_sent_explode.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NPOQZB6tWG1m"},"source":["# use only for multi class classification\n","from sklearn.metrics import classification_report\n","print(ad_id_dict)\n","\n","def convert_ad_ids_to_names(pred_ad_ids):\n","  if classification_ad_product:\n","    if classType == 'sent_exploded':\n","      ad_names_list_ordered = []\n","      ad_names_list_ordered.append(0)\n","      ad_names_list_ordered.append(1)\n","      ad_names_list_ordered.append(2)\n","    else:\n","      ad_names_list_ordered = list(product_id_dict.keys())\n","  else:\n","    ad_names_list_ordered = list(ad_id_dict.keys())\n","  # print(ad_names_list_ordered)\n","  pred_labels_names = len(pred_ad_ids)*[None]\n","  new_ad_id_dict = dict()\n","  pred_tweet_count = dict()\n","  for i in range(0, len(pred_ad_ids)):\n","    # print(pred_ad_ids[i])\n","    pred_labels_names[i] = ad_names_list_ordered[pred_ad_ids[i]]\n","    new_ad_id_dict[pred_ad_ids[i]] = ad_names_list_ordered[pred_ad_ids[i]]\n","    if ad_names_list_ordered[pred_ad_ids[i]] in pred_tweet_count:\n","      pred_tweet_count[ad_names_list_ordered[pred_ad_ids[i]]] = pred_tweet_count[ad_names_list_ordered[pred_ad_ids[i]]]+1\n","    else:\n","      pred_tweet_count[ad_names_list_ordered[pred_ad_ids[i]]] = 1\n","    # print(ad_names_list_ordered[pred_ad_ids[i]])\n","  new_ad_id_dict = sorted(new_ad_id_dict.items(), key=lambda x: x[1])\n","  print(new_ad_id_dict)\n","  print(pred_tweet_count)\n","  return new_ad_id_dict, pred_labels_names\n","\n","new_ad_id_dict , true_labels_flat_names = convert_ad_ids_to_names(true_labels_flat)\n","_, pred_labels_names = convert_ad_ids_to_names(pred_labels)\n","print(\" size of true label names:\"+ str(len(set(true_labels_flat_names))))\n","classification_report = classification_report(true_labels_flat_names, pred_labels_names,target_names=new_ad_id_dict,output_dict=True)\n","# print(classification_report)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fU1wl2t5a4pF"},"source":["classification_report_df = pd.DataFrame(classification_report).transpose()\n","print(classification_report_df.head())\n","if classType is not 'sent_exploded':\n","  classification_report_df.to_csv('./classification_report_sent_explode.csv')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CPMWn2qErDpH"},"source":["#plot confuson matrix\n","import matplotlib.pyplot as plt\n","import itertools\n","from sklearn.metrics import confusion_matrix\n","import numpy as np\n","\n","def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","\n","    # print(cm)\n","\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=90, fontsize = 16, )\n","    plt.yticks(tick_marks, classes,fontsize = 16)\n","\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, format(cm[i, j], fmt),\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    plt.savefig('sent_expl_conf_matrix.png')\n","\n","\n","true_names = sentence_results_df['true_label_name']\n","pred_names = sentence_results_df['pred_label_name']\n","sorted_true_names = (list(set(true_names)))\n","print(sorted_true_names)\n","\n","sorted_true_names = ['tide  bud knight', 'facebook  ready to rock?', 'hulu  tom brady s big announcement', 'donald j. trump for president  criminal justice reform', 'walmart  famous visitors',  'bud light seltzer  posty store  inside post s brain',  'planters  baby funeral', 'rocket mortgage  home','pop-tarts  pop-tarts fixed the pretzel commercial', 'olay  make space for women', 'doritos  the cool ranch']\n","cnf_matrix = confusion_matrix(true_names, pred_names, labels = sorted_true_names)\n","\n","# Plot normalized confusion matrix\n","fig = plt.figure()\n","fig.set_size_inches(20, 20, forward=True)\n","#fig.align_labels()\n","plot_confusion_matrix(cnf_matrix, classes=np.asarray(sorted_true_names), normalize=True,\n","                      title='Normalized confusion matrix')\n","\n","# ['hulu  tom brady s big announcement', 'genesis  going away party', 'tide  bud knight', \n","#  'nfl  inspire change  anquan boldin', 'marvel  black widow trailer', 'budweiser  typical american', \n","#  'hard rock hotels & casinos  bling cup', 'new york life  love takes action', 'none', 'planters  baby funeral', \n","#  'mountain dew', 'disney+  it s time', 'bud light seltzer  posty store  inside post s brain', \n","#  'turbotax  turbotax  all people are tax people remix', \n","#  'avocados from mexico  the avocados from mexico shopping network', 'hyundai  smaht pahk', \n","#  'little caesars pizza  best thing since sliced bread', 'cheetos  can t touch this', \n","#  'nfl  building a better game', 'fox nation  breaking news', 'reese s  rock',\n","#  'michelob  jimmy works it out', 'michelob  6 for 6-pack', 'amazon prime video  hunters', \n","#  'porsche  the heist', 'fox  halftime show  teaser_1', 'weathertech  lucky dog', 'jeep  groundhog day [t1]', \n","#  'rocket mortgage  home', 'squarespace  winona in winona', 'audi  let it go [t1]', 'olay  make space for women', \n","#  'doritos  the cool ranch', 'quibi  bank heist', 'pepsi zero sugar  zero sugar. done right.',\n","#  'hummer  gmc  quiet revolution', 'tide  finally later', 'donald j. trump for president  criminal justice reform',\n","#  'kia  tough never quits', 'pop-tarts  pop-tarts fixed the pretzel commercial', 'fox  not just another race', \n","#  'discover card  yes we’re accepted', 'procter & gamble  when we come together', 'coca-cola energy  show up', \n","#  'microsoft surface  be the one', 'no time to die  trailer', 'sabra  how do you  mmus?', 'facebook  ready to rock?', \n","#  'amazon echo  before alexa', 'pringles  the infinite dimensions of rick and morty', 'fox  super monday', \n","#  'google assistant  loretta', 't-mobile  mama tests 5g', 'walmart  famous visitors', \n","#  'verizon  the amazing things 5g won t do', 'snickers  fix the world', 'fox  toads  the masked singer']\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tvms49RRlo2J"},"source":["from sklearn.metrics import plot_confusion_matrix\n","import matplotlib.pyplot as plt\n","import sklearn.metrics\n","# rows: true labels\n","# columns : pred labels\n","cm= pd.DataFrame(\n","    confusion_matrix(true_names,pred_names, labels = (list(set(true_names)))), \n","                     index=(list(set(true_names))), columns=(list(set(true_names)))) \n","print(cm)\n","cm.to_csv('./conf_matrix_df.csv')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0v8qY9qJpHGb"},"source":["end here"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uaxi-JXVQbTw"},"source":["result = pd.DataFrame();\n","result['text_clean'] = test_sentences\n","result['true_label'] = true_labels_flat\n","result['true_label_name'] = result['true_label'].apply(lambda x: list(ad_id_dict.keys())[x])\n","result['predicted_label'] = pred_labels\n","result['pred_label_name'] = result['predicted_label'].apply(lambda x: list(ad_id_dict.keys())[x])\n","result.head()\n","result.to_csv('./result_multi_class.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7AwG8VIKxhFO"},"source":["from collections import Counter\n","counter = dict(Counter(true_names))\n","print(counter)\n","test_ad_true_count = pd.DataFrame()\n","test_ad_true_count['ad'] = counter.keys()\n","test_ad_true_count['count'] = counter.values()\n","test_ad_true_count.to_csv('./test_true_ad_count.csv')\n","\n","pred_counter = dict(Counter(pred_names))\n","print(pred_counter)\n","test_ad_pred_count = pd.DataFrame()\n","test_ad_pred_count['ad'] = pred_counter.keys()\n","test_ad_pred_count['count'] = pred_counter.values()\n","test_ad_pred_count.to_csv('./test_ad_pred_count.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"41-53pwVUd8m"},"source":["# def get_correct_samples(test_sentences, true_labels_flat, pred_labels, test_labels):\n","#   new_test_sentences = []\n","#   new_test_labels = []\n","#   for i in range(len(pred_labels)):\n","#     if true_labels_flat[i] == pred_labels[i]:\n","#       new_test_sentences.append(test_sentences[i])\n","#       new_test_labels.append (test_labels[i])\n","#   return new_test_sentences, new_test_labels\n","\n","def get_ad_related_samples(test_sentences, true_labels_flat, pred_labels, multi_class_labels):\n","  new_test_sentences = []\n","  new_test_labels = []\n","  for i in range(len(pred_labels)):\n","    if pred_labels[i] == 1:\n","      new_test_sentences.append(test_sentences[i])\n","      new_test_labels.append(multi_class_labels[i])\n","  return new_test_sentences, new_test_labels\n","print(len(binary_test_sentences))\n","print(len(true_labels_flat))\n","print(len(pred_labels))\n","print(len(test_labels))\n","new_test_sentences, new_test_labels =get_ad_related_samples(binary_test_sentences, true_labels_flat, pred_labels, test_labels)\n","bin_predicted_ad_related = pd.DataFrame()\n","bin_predicted_ad_related['sentences'] = new_test_sentences\n","bin_predicted_ad_related['labels'] = new_test_labels\n","bin_predicted_ad_related.to_csv('./bin_ad_predicted.csv')\n","print(bin_predicted_ad_related['labels'] )\n","# print(len(new_test_sentences))\n","# print(len(new_test_labels))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LjeQcWS7Sgzr"},"source":["bin_predicted_ad_related.head(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CZxmJKqvefPy"},"source":["# count =0\n","# for i in range(len(new_test_sentences)):\n","#   if new_test_labels[i] != 69:\n","#     count = count+1\n","#     # print(new_test_sentences[i])\n","#     # print(str(new_test_labels[i])+\":\"+ list(ad_id_dict.keys())[new_test_labels[i]])\n","# print( count)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SbfZ-97iUfYp"},"source":["Save the model"]},{"cell_type":"code","metadata":{"id":"68UvOrlLMY84"},"source":[" # save the model\n","\n","import os\n","\n","# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n","\n","output_dir = './model_save/'\n","\n","# Create output directory if needed\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","print(\"Saving model to %s\" % output_dir)\n","\n","# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","# They can then be reloaded using `from_pretrained()`\n","model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","model_to_save.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IdUSMdX9Uh83"},"source":["Sentence similarity between Tweets and ads using Fine tuned BERT embeddings"]},{"cell_type":"code","metadata":{"id":"f1qjC3fh52rv"},"source":["\n","def get_word_indeces(tokenizer, text, word):\n","    '''\n","    Determines the index or indeces of the tokens corresponding to `word`\n","    within `text`. `word` can consist of multiple words, e.g., \"cell biology\".\n","    \n","    Determining the indeces is tricky because words can be broken into multiple\n","    tokens. I've solved this with a rather roundabout approach--I replace `word`\n","    with the correct number of `[MASK]` tokens, and then find these in the \n","    tokenized result. \n","    '''\n","    # Tokenize the 'word'--it may be broken into multiple tokens or subwords.\n","    word_tokens = tokenizer.tokenize(word)\n","\n","    # Create a sequence of `[MASK]` tokens to put in place of `word`.\n","    masks_str = ' '.join(['[MASK]']*len(word_tokens))\n","\n","    # Replace the word with mask tokens.\n","    text_masked = text.replace(word, masks_str)\n","\n","    # `encode` performs multiple functions:\n","    #   1. Tokenizes the text\n","    #   2. Maps the tokens to their IDs\n","    #   3. Adds the special [CLS] and [SEP] tokens.\n","    input_ids = tokenizer.encode(text_masked)\n","\n","    # Use numpy's `where` function to find all indeces of the [MASK] token.\n","    mask_token_indeces = np.where(np.array(input_ids) == tokenizer.mask_token_id)[0]\n","\n","    return mask_token_indeces"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RtdgktBl58M7"},"source":["# Check Sentence embeddings\n","\n","def get_embedding(b_model, b_tokenizer, text, MAX_LEN = 64, word=''):\n","    '''\n","    Uses the provided model and tokenizer to produce an embedding for the\n","    provided `text`, and a \"contextualized\" embedding for `word`, if provided.\n","    '''\n","    # If a word is provided, figure out which tokens correspond to it.\n","    if not word == '':\n","        word_indeces = get_word_indeces(b_tokenizer, text, word)\n","\n","    # Encode the text, adding the (required!) special tokens, and converting toPyTorch tensors.\n","    encoded_dict = b_tokenizer.encode_plus(\n","                        text,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                )\n","    # print(encoded_dict)\n","    input_ids = encoded_dict['input_ids']\n","    # print(input_ids)\n","    b_model.eval()\n","    # bert_outputs = b_model(input_ids)     # Run the text through the model and get the hidden states.\n","    \n","    with torch.no_grad():     # Run the text through BERT, and collect all of the hidden states produced from all 12 layers. \n","\n","        outputs = b_model(input_ids)\n","\n","        # Evaluating the model will return a different number of objects based on how it's  configured in the `from_pretrained` call earlier. \n","        #In this case, becase we set `output_hidden_states = True`, the third item will be the hidden states from all layers. \n","        #See the documentation for more details:https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n","        hidden_states = outputs[2]     # `hidden_states` has shape [13 x 1 x <sentence length> x 768]\n","    # Select the embeddings from the second to last layer.\n","    # `token_vecs` is a tensor with shape [<sent length> x 768]\n","    # token_vecs = hidden_states[-2][0]\n","    \n","    token_vecs = hidden_states[-2][0]\n","    print(token_vecs.shape)\n","    # sentence_embedding = torch.mean(token_vecs, dim=0)    # Calculate the average of all token vectors.\n","    sentence_embedding = token_vecs[0]    # Calculate the average of all token vectors.\n","    sentence_embedding = sentence_embedding.detach().numpy()     # Convert to numpy array.\n","\n","    if not word == '':     # If `word` was provided, compute an embedding for those tokens.\n","        word_embedding = torch.mean(token_vecs[word_indeces], dim=0)         # Take the average of the embeddings for the tokens in `word`\n","        word_embedding = word_embedding.detach().numpy()         # Convert to numpy array.\n","    \n","        return (sentence_embedding, word_embedding)\n","    else:\n","        return sentence_embedding"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TgRt5-ZDsE4K"},"source":["\n","def get_sentence_embeddings(model, sentences, labels):\n","# Tokenize all of the sentences and map the tokens to thier word IDs.\n","  input_ids = []\n","  # For every sentence...\n","  for sent in sentences:\n","      encoded_sent = tokenizer.encode(\n","                          sent,                      # Sentence to encode.\n","                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                    )\n","      \n","      input_ids.append(encoded_sent)\n","  # Pad our input tokens\n","  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n","                            dtype=\"long\", truncating=\"post\", padding=\"post\")\n","  # Create attention masks\n","  attention_masks = []\n","  # Create a mask of 1s for each token followed by 0s for padding\n","  for seq in input_ids:\n","    seq_mask = [float(i>0) for i in seq]\n","    attention_masks.append(seq_mask) \n","  # Convert to tensors.\n","  prediction_inputs = torch.tensor(input_ids)\n","  prediction_masks = torch.tensor(attention_masks)\n","  prediction_labels = torch.tensor(labels)\n","  # Set the batch size.  \n","  batch_size = 32  \n","  # Create the DataLoader.\n","  prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n","  prediction_sampler = SequentialSampler(prediction_data)\n","  prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n","  # Prediction on test set\n","  print('Predicting labels for {:,} test sentences.'.format(len(prediction_inputs)))\n","  # Put model in evaluation mode\n","  model.eval()\n","  # Tracking variables \n","  predictions , true_labels = [], []\n","  sentence_embeddings = []\n","  # Predict \n","  for batch in prediction_dataloader:\n","    # Add batch to GPU\n","    batch = tuple(t.to(device) for t in batch)\n","    # Unpack the inputs from our dataloader\n","    b_input_ids, b_input_mask, b_labels = batch\n","    \n","    # Telling the model not to compute or store gradients, saving memory and \n","    # speeding up prediction\n","    with torch.no_grad():\n","        # Forward pass, calculate logit predictions\n","        outputs = model(b_input_ids, token_type_ids=None, \n","                        attention_mask=b_input_mask)\n","    logits = outputs[0]\n","    hidden_states = outputs[1]\n","\n","    # Move logits and labels to CPU\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","    # print(\"# of hidden states \"+ str(len(hidden_states)) )\n","    # print(\"# of input samples \"+ str(len(hidden_states[-2])))\n","    sentence_token_vecs = hidden_states[0] + hidden_states[1] + hidden_states[2] + hidden_states[3]\\\n","                        + hidden_states[4] + hidden_states[5] + hidden_states[6] + hidden_states[7]\\\n","                        + hidden_states[8] + hidden_states[9] + hidden_states[10] + hidden_states[11]\n","    # sentence_token_vecs = hidden_states[-2]\n","    # print(\"token vecs' shape:\" + str(sentence_token_vecs.shape)) # shape: [nSentences in a batch (32), sent length(64), 768] \n","    for i in range(sentence_token_vecs.shape[0]):\n","      sentence_embedding = sentence_token_vecs[i][0]   # Get the embedding of Frst token ie CLS\n","      # sentence_embedding = torch.mean(sentence_token_vecs[i], dim=0)    # Calculate the average of all token vectors.\n","      sentence_embeddings.append(sentence_embedding)\n","    # Store predictions and true labels\n","    predictions.append(logits)\n","    true_labels.append(label_ids)\n","  print('DONE.')\n","  print(\" length of sentence embeddings\" + str(len(sentence_embeddings)))\n","\n","  return sentence_embeddings"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HGc0hPd96MIY"},"source":["Get max similar sentence from model based on sentence embeddings"]},{"cell_type":"code","metadata":{"id":"fwLoTLVv6Kqu"},"source":["# bert_model = BertModel.from_pretrained('bert-base-uncased',\n","#                                   output_hidden_states = True)\n","# bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","# ad_keywords['ad_embeddings'] = ad_keywords['keywords_clean'].apply(lambda ad: \n","#                                           get_embedding(bert_model, bert_tokenizer, ad))\n","# ad_related_twts['tweet_embeddings'] = ad_related_twts['text_clean'].apply(lambda x: \n","#                                         get_embedding(bert_model, bert_tokenizer, x))\n","\n","# sentences = ad_related_twts.text_clean.values\n","# labels = ad_related_twts.ad_manual_adjusted_id.values\n","# test_df = ad_related_twts\n","\n","sentences = bin_predicted_ad_related['sentences']\n","labels = bin_predicted_ad_related['labels']\n","test_df = bin_predicted_ad_related\n","\n","test_df['tweet_embeddings'] = get_sentence_embeddings(model, sentences, labels)\n","\n","ad_sentences = ad_keywords['keywords_clean']\n","ad_labels = np.zeros(ad_sentences.shape[0])\n","ad_keywords['ad_embeddings'] = get_sentence_embeddings(model, ad_sentences, ad_labels)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rmyu5MuAND2-"},"source":["ad_keywords['ad_embeddings'] "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yTZj-oynH9l_"},"source":["# bin_predicted_ad_related['labels']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FIAOF1zhV87a"},"source":["ad_1 = ad_keywords[ad_keywords['ad_id'] == 1]\n","print(ad_1)\n","keywords_1 = ad_1['keywords_clean']\n","print(keywords_1)\n","embedding_id_1 = ad_1['ad_embeddings']\n","print(embedding_id_1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mtOxNUMo_c1W"},"source":["import numpy as np\n","\n","def get_topn(arr, n):\n","  top_indices_unsorted = (np.argpartition(arr, -n)[-n:]) # gives indices of topn but unsorted\n","  arr = np.array(arr) # first convert input list to array\n","  indices = np.argsort(-arr[top_indices_unsorted])\n","  top_indices_sorted = top_indices_unsorted[indices]\n","  topn = arr[top_indices_sorted]\n","\n","  return top_indices_sorted, topn\n","\n","# arr = [3,2,4,5,6,9,8,7 ]\n","# topn_indices, topn_vals = get_topn(arr, 5)\n","# print(arr)\n","# print(topn_vals)\n","# print(topn_indices)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2k8F_soj8iz7"},"source":["from scipy.spatial.distance import cosine\n","sent_sims = []\n","n = 5\n","max_sim_ad_id_1 = []\n","max_sim_ad_id_2 = []\n","max_sim_ad_id_3 = []\n","max_sim_ad_id_4 = []\n","max_sim_ad_id_5 = []\n","\n","max_sim_ad_1 = []\n","max_sim_ad_2 = []\n","max_sim_ad_3 = []\n","max_sim_ad_4 = []\n","max_sim_ad_5 = []\n","\n","ad_id_dict_keys = list(ad_id_dict.keys())\n","\n","data = test_df\n","\n","for sent_embedding in data['tweet_embeddings']:\n","  ad_sims = [None] * (len(ad_id_dict)-1) # -1 to exclude none \n","  # for ad_embedding in ad_keywords['ad_embeddings']:\n","  for i,row in ad_keywords.iterrows():\n","    ad_embedding = row.ad_embeddings\n","    ad_id = row['ad_id']\n","    if ad_id < (len(ad_id_dict)-1): # if equals len(ad_id_dict) it is none i.e it does not have any keywords to find embeddings\n","      sim = 1-cosine(sent_embedding.to('cpu').numpy(), ad_embedding.to('cpu').numpy())\n","      ad_sims[ad_id] = sim\n","  sent_sims.append(ad_sims)\n","  topn_ads, topn_scores = get_topn(ad_sims, n)\n","\n","  max_sim_ad_id_1.append(topn_ads[0])\n","  max_sim_ad_id_2.append(topn_ads[1])\n","  max_sim_ad_id_3.append(topn_ads[2])\n","  max_sim_ad_id_4.append(topn_ads[3])\n","  max_sim_ad_id_5.append(topn_ads[4])\n","\n","  max_sim_ad_1.append(ad_id_dict_keys[topn_ads[0]])\n","  max_sim_ad_2.append(ad_id_dict_keys[topn_ads[1]])\n","  max_sim_ad_3.append(ad_id_dict_keys[topn_ads[2]])\n","  max_sim_ad_4.append(ad_id_dict_keys[topn_ads[3]])\n","  max_sim_ad_5.append(ad_id_dict_keys[topn_ads[4]])\n","\n","data['max_sim_ad_1'] = max_sim_ad_1\n","data['max_sim_ad_2'] = max_sim_ad_2\n","data['max_sim_ad_3'] = max_sim_ad_3\n","data['max_sim_ad_4'] = max_sim_ad_4\n","data['max_sim_ad_5'] = max_sim_ad_5\n","data['max_sim_ad_id_1'] = max_sim_ad_id_1\n","data['max_sim_ad_id_2'] = max_sim_ad_id_2\n","data['max_sim_ad_id_3'] = max_sim_ad_id_3\n","data['max_sim_ad_id_4'] = max_sim_ad_id_4\n","data['max_sim_ad_id_5'] = max_sim_ad_id_5\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W394xle5ijp7"},"source":["# data.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pukY95bZ8VP8"},"source":["# get max similar ad based on embeddings from model\n","count = 0\n","print(len(data))\n","for i, ad in data.iterrows():\n","  # if ad['ad_manual_adjusted_id'] == ad['max_sim_ad_id_1'] or ad['ad_manual_adjusted_id'] == ad['max_sim_ad_id_2'] or ad['ad_manual_adjusted_id'] == ad['max_sim_ad_id_3']:\n","  if ad['labels'] == ad['max_sim_ad_id_1'] or ad['labels'] == ad['max_sim_ad_id_2']\\\n","  or ad['labels'] == ad['max_sim_ad_id_3']:\n","  # or ad['labels'] == ad['max_sim_ad_id_4']\\\n","  # or ad['labels'] == ad['max_sim_ad_id_5'] :\n","    count = count + 1\n","\n","accuracy = count/len(data)\n","print(count)\n","print(accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XL3_JhopFzQf"},"source":["# By taking CLS token vector i.e vecs[0], calculate cosine similarity values and accuracy values\n","# -1: 0.032\n","# -2: 0.05\n","# -3: 0.012\n","# -4: 0.029\n","# -5: 0.061\n","# -6: 0.039\n","# -7: 0.014\n","# -8: 0.013\n","# -9: 0.006\n","# -10: 0.007\n","# -11: 0.019\n","\n","\n","\n","# By taking average, calculate cosine similarity values and accuracy values\n","# -1: 0.016\n","# -2: 0.02\n","# -3: 0.007\n","# -4: 0.016\n","# -5: 0.023\n","# -6: 0.019\n","# -7: 0.010\n","# -8: 0.015\n","# -9: 0.023\n","# -10: 0.037\n","# -11: 0.046\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-A_-i1MB5ed1"},"source":["# import inflect\n","# p = inflect.engine()\n","\n","# def get_singular_words(plural_words):\n","#   sing_words = []\n","#   for word in plural_words:\n","#     sing_words.append(p.singular_noun(word))\n","#   return sing_words\n","\n","# words = [\"apples\", \"sheep\", \"oranges\", \"cats\", \"people\", \"dice\", \"pence\", \"trump\"]\n","# sing_words = get_singular_words(words)\n","# print(sing_words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HLi75PdoJErZ"},"source":["# from nltk.stem import PorterStemmer\n","\n","# porter=PorterStemmer()\n","# def get_stem_words(plural_words):\n","#   sing_words = []\n","#   for word in plural_words:\n","#     sing_words.append(porter.stem(word))\n","#   return sing_words\n","\n","# words = [\"apples\", \"sheep\", \"oranges\", \"cats\", \"people\", \"dice\", \"pence\", \"trump\"]\n","# stem_words = get_stem_words(words)\n","# print(stem_words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nvOjXGKFJIq2"},"source":["# https://huggingface.co/transformers/main_classes/tokenizer.html\n","# add words that are not in pre trained vocab as tokens\n","# TODO cross check if the embeddings actually got updated\n","\n","# added_tokens = []\n","# for key in ad_keys_clean:\n","#   if key not in vocab_tokens:\n","#     n = tokenizer.add_tokens(key)\n","#     if n==1:\n","#       added_tokens.append(key)\n","# print('added '+ str(len(added_tokens)) +' to the vocab')\n","# print(\"added words:\"+ str(added_tokens))"],"execution_count":null,"outputs":[]}]}